{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": "# Block Architecture Ablation Study\n## DWBlock vs STMBlock vs FastSTMBlock\n\nComparing:\n- **DWBlock** (JeongWonNet77_Rep256Basis8S24Drop): Full channel processing + PRCM\n- **STMBlock** (JeongWonNet_STMShuffle_NoStem): Split-Transform-Merge + AffinePRCM + Channel Shuffle\n- **FastSTMBlock** (JeongWonNet_STMNoConcatShuffle_NoStemMS): Slicing + SimplePRCM + 1x1 Mixing"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T03:50:48.607277Z",
     "iopub.status.busy": "2026-02-19T03:50:48.607174Z",
     "iopub.status.idle": "2026-02-19T03:50:49.166305Z",
     "shell.execute_reply": "2026-02-19T03:50:49.166036Z",
     "shell.execute_reply.started": "2026-02-19T03:50:48.607268Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import numpy as np\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## 1. Module Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T03:50:49.166808Z",
     "iopub.status.busy": "2026-02-19T03:50:49.166654Z",
     "iopub.status.idle": "2026-02-19T03:50:49.173259Z",
     "shell.execute_reply": "2026-02-19T03:50:49.173018Z",
     "shell.execute_reply.started": "2026-02-19T03:50:49.166799Z"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# RepConv (Re-parameterizable Conv)\n",
    "# ============================================\n",
    "\n",
    "class RepConv(nn.Module):\n",
    "    \"\"\"Re-parameterizable Convolution Block\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3,\n",
    "                 stride=1, padding=1, groups=1, use_activation=True):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.groups = groups\n",
    "        self.use_identity = (stride == 1) and (in_channels == out_channels)\n",
    "        \n",
    "        self.conv_kxk = nn.Conv2d(in_channels, out_channels, kernel_size,\n",
    "                                  stride, padding, groups=groups, bias=False)\n",
    "        self.bn_kxk = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        if kernel_size > 1:\n",
    "            self.conv_1x1 = nn.Conv2d(in_channels, out_channels, 1,\n",
    "                                      stride, 0, groups=groups, bias=False)\n",
    "            self.bn_1x1 = nn.BatchNorm2d(out_channels)\n",
    "        else:\n",
    "            self.conv_1x1 = None\n",
    "        \n",
    "        if self.use_identity:\n",
    "            self.bn_identity = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        self.activation = nn.ReLU(inplace=True) if use_activation else nn.Identity()\n",
    "           \n",
    "    def forward(self, x):\n",
    "        if hasattr(self, 'fused_conv'):\n",
    "            return self.activation(self.fused_conv(x))\n",
    "        \n",
    "        out = self.bn_kxk(self.conv_kxk(x))\n",
    "        if self.conv_1x1 is not None:\n",
    "            out += self.bn_1x1(self.conv_1x1(x))\n",
    "        if self.use_identity:\n",
    "            out += self.bn_identity(x)\n",
    "        return self.activation(out)\n",
    "    \n",
    "    def switch_to_deploy(self):\n",
    "        if hasattr(self, 'fused_conv'):\n",
    "            return\n",
    "        \n",
    "        kernel, bias = self._fuse_bn_tensor(self.conv_kxk, self.bn_kxk)\n",
    "        \n",
    "        if self.conv_1x1 is not None:\n",
    "            kernel_1x1, bias_1x1 = self._fuse_bn_tensor(self.conv_1x1, self.bn_1x1)\n",
    "            kernel += self._pad_1x1_to_kxk(kernel_1x1)\n",
    "            bias += bias_1x1\n",
    "        \n",
    "        if self.use_identity:\n",
    "            kernel_identity, bias_identity = self._fuse_bn_tensor(None, self.bn_identity)\n",
    "            kernel += kernel_identity\n",
    "            bias += bias_identity\n",
    "        \n",
    "        self.fused_conv = nn.Conv2d(\n",
    "            self.in_channels, self.out_channels, self.kernel_size,\n",
    "            self.stride, self.padding, groups=self.groups, bias=True\n",
    "        )\n",
    "        self.fused_conv.weight.data = kernel\n",
    "        self.fused_conv.bias.data = bias\n",
    "        \n",
    "        self.__delattr__('conv_kxk')\n",
    "        self.__delattr__('bn_kxk')\n",
    "        if self.conv_1x1 is not None:\n",
    "            self.__delattr__('conv_1x1')\n",
    "            self.__delattr__('bn_1x1')\n",
    "        if hasattr(self, 'bn_identity'):\n",
    "            self.__delattr__('bn_identity')\n",
    "   \n",
    "    def _fuse_bn_tensor(self, conv, bn):\n",
    "        if conv is None:\n",
    "            input_dim = self.in_channels // self.groups\n",
    "            kernel_value = torch.zeros((self.in_channels, input_dim,\n",
    "                                        self.kernel_size, self.kernel_size),\n",
    "                                       dtype=bn.weight.dtype, device=bn.weight.device)\n",
    "            for i in range(self.in_channels):\n",
    "                kernel_value[i, i % input_dim,\n",
    "                             self.kernel_size // 2, self.kernel_size // 2] = 1\n",
    "            kernel = kernel_value\n",
    "        else:\n",
    "            kernel = conv.weight\n",
    "        \n",
    "        std = torch.sqrt(bn.running_var + bn.eps)\n",
    "        t = (bn.weight / std).reshape(-1, 1, 1, 1)\n",
    "        return kernel * t, bn.bias - bn.running_mean * bn.weight / std\n",
    "   \n",
    "    def _pad_1x1_to_kxk(self, kernel_1x1):\n",
    "        if self.kernel_size == 1:\n",
    "            return kernel_1x1\n",
    "        pad = self.kernel_size // 2\n",
    "        return F.pad(kernel_1x1, [pad, pad, pad, pad])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T03:50:49.174038Z",
     "iopub.status.busy": "2026-02-19T03:50:49.173928Z",
     "iopub.status.idle": "2026-02-19T03:50:49.183805Z",
     "shell.execute_reply": "2026-02-19T03:50:49.183589Z",
     "shell.execute_reply.started": "2026-02-19T03:50:49.174027Z"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# PRCM (Scale Only)\n",
    "# ============================================\n",
    "\n",
    "class PRCM(nn.Module):\n",
    "    \"\"\"Pattern Recalibration Module - Scale Only\"\"\"\n",
    "    def __init__(self, channels, num_basis=8, dropout_rate=0.5):\n",
    "        super().__init__()\n",
    "        self.basis = nn.Parameter(torch.randn(num_basis, channels))\n",
    "        self.fuser = nn.Linear(num_basis, channels, bias=False)\n",
    "        self.coeff_dropout = nn.Dropout(dropout_rate) if dropout_rate > 0 else nn.Identity()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        ctx = x.mean(dim=[2, 3])\n",
    "        coeff = ctx @ self.basis.t()\n",
    "        coeff = self.coeff_dropout(coeff)\n",
    "        w = self.fuser(coeff).sigmoid().unsqueeze(-1).unsqueeze(-1)\n",
    "        return x * w\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# AffinePRCM (Scale + Shift)\n",
    "# ============================================\n",
    "\n",
    "class AffinePRCM(nn.Module):\n",
    "    \"\"\"Affine Modulation PRCM - Scale + Shift\"\"\"\n",
    "    def __init__(self, channels, num_basis=8, dropout_rate=0.5):\n",
    "        super().__init__()\n",
    "        self.basis = nn.Parameter(torch.randn(num_basis, channels))\n",
    "        self.scale_proj = nn.Linear(num_basis, channels, bias=False)\n",
    "        self.shift_proj = nn.Linear(num_basis, channels, bias=False)\n",
    "        self.coeff_dropout = nn.Dropout(dropout_rate) if dropout_rate > 0 else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        ctx = x.mean(dim=[2, 3])\n",
    "        coeff = ctx @ self.basis.t()\n",
    "        coeff = self.coeff_dropout(coeff)\n",
    "        alpha = self.scale_proj(coeff).sigmoid().unsqueeze(-1).unsqueeze(-1)\n",
    "        beta = self.shift_proj(coeff).unsqueeze(-1).unsqueeze(-1)\n",
    "        return x * alpha + beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T03:50:49.184224Z",
     "iopub.status.busy": "2026-02-19T03:50:49.184105Z",
     "iopub.status.idle": "2026-02-19T03:50:49.189193Z",
     "shell.execute_reply": "2026-02-19T03:50:49.188930Z",
     "shell.execute_reply.started": "2026-02-19T03:50:49.184216Z"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# DWBlock (from JeongWonNet77_Rep256Basis8S24Drop)\n",
    "# ============================================\n",
    "\n",
    "class DWBlock(nn.Module):\n",
    "    \"\"\"DWBlock: 1x1 Conv (optional) + RepConv DW + PRCM\n",
    "    \n",
    "    Structure:\n",
    "        [1x1 Conv] -> RepConv 7x7 DW -> PRCM\n",
    "        \n",
    "    - Full channel processing (no split)\n",
    "    - PRCM: scale only (multiplicative)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch, out_ch, kernel_size=7, num_basis=8, dropout_rate=0.5):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        if in_ch != out_ch:\n",
    "            layers.append(nn.Conv2d(in_ch, out_ch, kernel_size=1, bias=False))\n",
    "        layers.append(RepConv(out_ch, out_ch, kernel_size=kernel_size, \n",
    "                              padding=kernel_size//2, groups=out_ch))\n",
    "        self.conv = nn.Sequential(*layers)\n",
    "        self.prcm = PRCM(out_ch, num_basis=num_basis, dropout_rate=dropout_rate)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.prcm(self.conv(x))\n",
    "    \n",
    "    def switch_to_deploy(self):\n",
    "        for m in self.conv.modules():\n",
    "            if isinstance(m, RepConv):\n",
    "                m.switch_to_deploy()\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# SplitTransformMergeBlock (from JeongWonNet_STMShuffle_NoStem)\n",
    "# ============================================\n",
    "\n",
    "class STMBlock(nn.Module):\n",
    "    \"\"\"Split-Transform-Merge Block (ShuffleNet V2 inspired)\n",
    "    \n",
    "    Structure:\n",
    "        Input -> Split into [Passive, Active]\n",
    "        Passive: 1x1 Conv (channel adjust only)\n",
    "        Active:  1x1 Conv -> RepConv 7x7 DW -> AffinePRCM\n",
    "        Output:  Concat -> Channel Shuffle\n",
    "        \n",
    "    - Only half channels go through heavy computation\n",
    "    - AffinePRCM: scale + shift (affine)\n",
    "    - Channel shuffle for information mixing\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=7, num_basis=8, dropout_rate=0.5):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert in_channels % 2 == 0, f\"in_channels must be even, got {in_channels}\"\n",
    "        assert out_channels % 2 == 0, f\"out_channels must be even, got {out_channels}\"\n",
    "        \n",
    "        self.split_channels = in_channels // 2\n",
    "        self.out_split_channels = out_channels // 2\n",
    "        \n",
    "        # Passive branch: minimal processing\n",
    "        if self.split_channels != self.out_split_channels:\n",
    "            self.passive_adjust = nn.Conv2d(\n",
    "                self.split_channels, self.out_split_channels, kernel_size=1, bias=False\n",
    "            )\n",
    "        else:\n",
    "            self.passive_adjust = nn.Identity()\n",
    "        \n",
    "        # Active branch: full processing\n",
    "        if self.split_channels != self.out_split_channels:\n",
    "            self.pw_conv = nn.Conv2d(\n",
    "                self.split_channels, self.out_split_channels, kernel_size=1, bias=False\n",
    "            )\n",
    "        else:\n",
    "            self.pw_conv = nn.Identity()\n",
    "        \n",
    "        self.dw_repconv = RepConv(\n",
    "            self.out_split_channels, self.out_split_channels, \n",
    "            kernel_size=kernel_size, padding=kernel_size // 2,\n",
    "            groups=self.out_split_channels\n",
    "        )\n",
    "        \n",
    "        self.affine_prcm = AffinePRCM(\n",
    "            self.out_split_channels, num_basis=num_basis, dropout_rate=dropout_rate\n",
    "        )\n",
    "    \n",
    "    def channel_shuffle(self, x, groups):\n",
    "        B, C, H, W = x.shape\n",
    "        channels_per_group = C // groups\n",
    "        x = x.view(B, groups, channels_per_group, H, W)\n",
    "        x = torch.transpose(x, 1, 2).contiguous()\n",
    "        x = x.view(B, -1, H, W)\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Split\n",
    "        x_passive, x_active = torch.chunk(x, 2, dim=1)\n",
    "        \n",
    "        # Transform\n",
    "        x_passive = self.passive_adjust(x_passive)\n",
    "        \n",
    "        x_active = self.pw_conv(x_active)\n",
    "        x_active = self.dw_repconv(x_active)\n",
    "        x_active = self.affine_prcm(x_active)\n",
    "        \n",
    "        # Merge\n",
    "        out = torch.cat([x_passive, x_active], dim=1)\n",
    "        out = self.channel_shuffle(out, 2)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def switch_to_deploy(self):\n",
    "        if isinstance(self.dw_repconv, RepConv):\n",
    "            self.dw_repconv.switch_to_deploy()"
   ]
  },
  {
   "cell_type": "code",
   "id": "zw242tww3a",
   "source": "# ============================================\n# SimplePRCM (Scale Only, Optimized)\n# ============================================\n\nclass SimplePRCM(nn.Module):\n    \"\"\"SimplePRCM - Optimized version without shift\n    \n    Same as PRCM but explicitly named for FastSTMBlock\n    \"\"\"\n    def __init__(self, channels, num_basis=8, dropout_rate=0.5):\n        super().__init__()\n        self.basis = nn.Parameter(torch.randn(num_basis, channels))\n        self.scale_proj = nn.Linear(num_basis, channels, bias=False)\n        self.coeff_dropout = nn.Dropout(dropout_rate) if dropout_rate > 0 else nn.Identity()\n\n    def forward(self, x):\n        ctx = x.mean(dim=[2, 3])\n        coeff = ctx @ self.basis.t()\n        coeff = self.coeff_dropout(coeff)\n        alpha = self.scale_proj(coeff).sigmoid().unsqueeze(-1).unsqueeze(-1)\n        return x * alpha\n\n\n# ============================================\n# FastSTMBlock (from JeongWonNet_STMNoConcatShuffle_NoStemMS)\n# ============================================\n\nclass FastSTMBlock(nn.Module):\n    \"\"\"Fast Split-Transform-Merge Block (FasterNet Style)\n    \n    Optimizations vs STMBlock:\n        1. Slicing instead of torch.chunk (zero-copy view)\n        2. SimplePRCM (no shift) instead of AffinePRCM\n        3. 1x1 Conv Mixing instead of Channel Shuffle\n    \n    Structure:\n        Input -> [1x1 Align] -> Slicing\n        Active (C/2):  RepConv 7x7 DW -> SimplePRCM\n        Passive (C/2): Identity\n        Concat -> 1x1 Conv Mixing -> BN -> ReLU\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size=7, num_basis=8, dropout_rate=0.5):\n        super().__init__()\n        \n        # Channel Alignment\n        if in_channels != out_channels:\n            self.align_conv = nn.Conv2d(in_channels, out_channels, 1, bias=False)\n            self.align_bn = nn.BatchNorm2d(out_channels)\n        else:\n            self.align_conv = None\n        \n        self.out_channels = out_channels\n        self.dim_conv = out_channels // 2\n        \n        # Active Branch (half channels only)\n        self.partial_conv = RepConv(\n            self.dim_conv, self.dim_conv,\n            kernel_size=kernel_size, padding=kernel_size // 2,\n            groups=self.dim_conv\n        )\n        self.partial_prcm = SimplePRCM(\n            self.dim_conv, num_basis=num_basis, dropout_rate=dropout_rate\n        )\n        \n        # Mixing (instead of Channel Shuffle)\n        self.mix_conv = nn.Conv2d(out_channels, out_channels, 1, bias=False)\n        self.mix_bn = nn.BatchNorm2d(out_channels)\n        self.act = nn.ReLU(inplace=True)\n    \n    def forward(self, x):\n        # 1. Channel Alignment\n        if self.align_conv is not None:\n            x = self.act(self.align_bn(self.align_conv(x)))\n        \n        # 2. Slicing (zero-copy view, faster than chunk)\n        x_active = x[:, :self.dim_conv, :, :]\n        x_passive = x[:, self.dim_conv:, :, :]\n        \n        # 3. Transform (Active only)\n        x_active = self.partial_conv(x_active)\n        x_active = self.partial_prcm(x_active)\n        \n        # 4. Merge\n        x_out = torch.cat([x_active, x_passive], dim=1)\n        \n        # 5. Mix (1x1 Conv instead of Shuffle)\n        x_out = self.act(self.mix_bn(self.mix_conv(x_out)))\n        \n        return x_out\n    \n    def switch_to_deploy(self):\n        if isinstance(self.partial_conv, RepConv):\n            self.partial_conv.switch_to_deploy()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## 2. Latency Measurement Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T03:50:49.189518Z",
     "iopub.status.busy": "2026-02-19T03:50:49.189441Z",
     "iopub.status.idle": "2026-02-19T03:50:49.192673Z",
     "shell.execute_reply": "2026-02-19T03:50:49.192476Z",
     "shell.execute_reply.started": "2026-02-19T03:50:49.189510Z"
    }
   },
   "outputs": [],
   "source": [
    "def count_params(model):\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "def measure_latency_gpu(model, input_tensor, warmup=100, repeat=500):\n",
    "    \"\"\"GPU latency measurement with CUDA synchronization\"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"CUDA not available\")\n",
    "        return None, None\n",
    "    \n",
    "    model.eval()\n",
    "    model.cuda()\n",
    "    input_tensor = input_tensor.cuda()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(warmup):\n",
    "            _ = model(input_tensor)\n",
    "            torch.cuda.synchronize()\n",
    "    \n",
    "    times = []\n",
    "    with torch.no_grad():\n",
    "        for _ in range(repeat):\n",
    "            torch.cuda.synchronize()\n",
    "            start = time.perf_counter()\n",
    "            _ = model(input_tensor)\n",
    "            torch.cuda.synchronize()\n",
    "            end = time.perf_counter()\n",
    "            times.append((end - start) * 1000)\n",
    "    \n",
    "    return np.mean(times), np.std(times)\n",
    "\n",
    "def measure_latency_cpu(model, input_tensor, warmup=20, repeat=100):\n",
    "    \"\"\"CPU latency measurement\"\"\"\n",
    "    model.eval()\n",
    "    model.cpu()\n",
    "    input_tensor = input_tensor.cpu()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(warmup):\n",
    "            _ = model(input_tensor)\n",
    "    \n",
    "    times = []\n",
    "    with torch.no_grad():\n",
    "        for _ in range(repeat):\n",
    "            start = time.perf_counter()\n",
    "            _ = model(input_tensor)\n",
    "            end = time.perf_counter()\n",
    "            times.append((end - start) * 1000)\n",
    "    \n",
    "    return np.mean(times), np.std(times)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## 3. Architecture Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T03:50:49.193013Z",
     "iopub.status.busy": "2026-02-19T03:50:49.192922Z",
     "iopub.status.idle": "2026-02-19T03:50:49.195395Z",
     "shell.execute_reply": "2026-02-19T03:50:49.195207Z",
     "shell.execute_reply.started": "2026-02-19T03:50:49.193005Z"
    }
   },
   "outputs": [],
   "source": "print(\"=\" * 75)\nprint(\"Block Architecture Comparison\")\nprint(\"=\" * 75)\n\nprint(\"\"\"\n┌─────────────────────────────────────────────────────────────────────────┐\n│ DWBlock (JeongWonNet77_Rep256Basis8S24Drop)                             │\n├─────────────────────────────────────────────────────────────────────────┤\n│   Input (C channels)                                                    │\n│      │                                                                  │\n│      ▼                                                                  │\n│   [1x1 Conv] (if in_ch != out_ch)                                       │\n│      │                                                                  │\n│      ▼                                                                  │\n│   RepConv 7x7 DW (full C channels)                                      │\n│      │                                                                  │\n│      ▼                                                                  │\n│   PRCM (scale only: x * α)                                              │\n│      │                                                                  │\n│      ▼                                                                  │\n│   Output                                                                │\n└─────────────────────────────────────────────────────────────────────────┘\n\n┌─────────────────────────────────────────────────────────────────────────┐\n│ STMBlock (JeongWonNet_STMShuffle_NoStem)                                │\n├─────────────────────────────────────────────────────────────────────────┤\n│   Input (C channels)                                                    │\n│      │                                                                  │\n│      ▼                                                                  │\n│   torch.chunk (Split C/2 + C/2)                                         │\n│      │                                                                  │\n│   ┌──┴──┐                                                               │\n│   │     │                                                               │\n│   ▼     ▼                                                               │\n│ Passive Active                                                          │\n│ (C/2)   (C/2)                                                           │\n│   │       │                                                             │\n│   ▼       ▼                                                             │\n│ 1x1     1x1 Conv -> RepConv 7x7 DW -> AffinePRCM (x*α+β)                │\n│ Conv      │                                                             │\n│   │       │                                                             │\n│   └───┬───┘                                                             │\n│       │                                                                 │\n│       ▼                                                                 │\n│   torch.cat -> Channel Shuffle                                          │\n│       │                                                                 │\n│       ▼                                                                 │\n│   Output                                                                │\n└─────────────────────────────────────────────────────────────────────────┘\n\n┌─────────────────────────────────────────────────────────────────────────┐\n│ FastSTMBlock (JeongWonNet_STMNoConcatShuffle_NoStemMS)                  │\n├─────────────────────────────────────────────────────────────────────────┤\n│   Input (C channels)                                                    │\n│      │                                                                  │\n│      ▼                                                                  │\n│   [1x1 Conv + BN + ReLU] (channel alignment)                            │\n│      │                                                                  │\n│      ▼                                                                  │\n│   Slicing (zero-copy view, faster than chunk)                           │\n│      │                                                                  │\n│   ┌──┴──┐                                                               │\n│   │     │                                                               │\n│   ▼     ▼                                                               │\n│ Active Passive                                                          │\n│ (C/2)   (C/2)                                                           │\n│   │       │                                                             │\n│   ▼       │                                                             │\n│ RepConv   │ (Identity)                                                  │\n│ 7x7 DW    │                                                             │\n│   │       │                                                             │\n│   ▼       │                                                             │\n│ SimplePRCM│ (scale only: x*α, no shift)                                 │\n│   │       │                                                             │\n│   └───┬───┘                                                             │\n│       │                                                                 │\n│       ▼                                                                 │\n│   torch.cat -> 1x1 Conv + BN + ReLU (Mixing)                            │\n│       │                                                                 │\n│       ▼                                                                 │\n│   Output                                                                │\n└─────────────────────────────────────────────────────────────────────────┘\n\nKey Differences:\n  ┌────────────────┬──────────────────┬──────────────────┬──────────────────┐\n  │                │ DWBlock          │ STMBlock         │ FastSTMBlock     │\n  ├────────────────┼──────────────────┼──────────────────┼──────────────────┤\n  │ Split          │ None (full C)    │ torch.chunk      │ Slicing (view)   │\n  │ Active Ch.     │ C                │ C/2              │ C/2              │\n  │ Passive        │ None             │ 1x1 Conv         │ Identity         │\n  │ PRCM Type      │ PRCM (scale)     │ AffinePRCM (+β)  │ SimplePRCM       │\n  │ Mixing         │ None             │ Channel Shuffle  │ 1x1 Conv + BN    │\n  └────────────────┴──────────────────┴──────────────────┴──────────────────┘\n\"\"\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## 4. Parameter Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T03:50:49.195678Z",
     "iopub.status.busy": "2026-02-19T03:50:49.195610Z",
     "iopub.status.idle": "2026-02-19T03:50:49.200279Z",
     "shell.execute_reply": "2026-02-19T03:50:49.200051Z",
     "shell.execute_reply.started": "2026-02-19T03:50:49.195670Z"
    }
   },
   "outputs": [],
   "source": "channels = 64\nnum_basis = 8\n\nprint(\"=\" * 75)\nprint(f\"Parameter Comparison (in_ch={channels}, out_ch={channels})\")\nprint(\"=\" * 75)\n\ndw_block = DWBlock(channels, channels, kernel_size=7, num_basis=num_basis)\nstm_block = STMBlock(channels, channels, kernel_size=7, num_basis=num_basis)\nfast_block = FastSTMBlock(channels, channels, kernel_size=7, num_basis=num_basis)\n\ndw_params = count_params(dw_block)\nstm_params = count_params(stm_block)\nfast_params = count_params(fast_block)\n\nprint(f\"\\n{'Block':<30} {'Parameters':>15} {'vs DWBlock':>15}\")\nprint(\"-\" * 62)\nprint(f\"{'DWBlock':<30} {dw_params:>15,} {'baseline':>15}\")\nprint(f\"{'STMBlock':<30} {stm_params:>15,} {(stm_params/dw_params)*100-100:>+14.1f}%\")\nprint(f\"{'FastSTMBlock':<30} {fast_params:>15,} {(fast_params/dw_params)*100-100:>+14.1f}%\")\n\nhalf_ch = channels // 2\nprint(f\"\\n[FastSTMBlock Parameter Breakdown]\")\nprint(f\"  mix_conv (1x1): {channels}*{channels} = {channels*channels:,}\")\nprint(f\"  mix_bn: {channels}*2 = {channels*2:,}\")\nprint(f\"  RepConv 7x7 DW ({half_ch}ch): ~{half_ch*49 + half_ch*6:,}\")\nprint(f\"  SimplePRCM ({half_ch}ch): {num_basis}*{half_ch}*2 = {num_basis*half_ch*2:,}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## 5. GPU Latency Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T03:50:49.200622Z",
     "iopub.status.busy": "2026-02-19T03:50:49.200533Z",
     "iopub.status.idle": "2026-02-19T03:50:49.505878Z",
     "shell.execute_reply": "2026-02-19T03:50:49.505578Z",
     "shell.execute_reply.started": "2026-02-19T03:50:49.200613Z"
    }
   },
   "outputs": [],
   "source": "channels = 64\nresolution = 64\nbatch_size = 1\n\nx = torch.randn(batch_size, channels, resolution, resolution)\n\nprint(\"=\" * 80)\nprint(f\"GPU Latency Comparison\")\nprint(f\"Input: ({batch_size}, {channels}, {resolution}, {resolution})\")\nprint(\"=\" * 80)\n\nblocks = OrderedDict([\n    (\"DWBlock (train)\", DWBlock(channels, channels)),\n    (\"STMBlock (train)\", STMBlock(channels, channels)),\n    (\"FastSTMBlock (train)\", FastSTMBlock(channels, channels)),\n])\n\n# Deploy mode\ndw_deploy = DWBlock(channels, channels)\ndw_deploy.switch_to_deploy()\nblocks[\"DWBlock (deploy)\"] = dw_deploy\n\nstm_deploy = STMBlock(channels, channels)\nstm_deploy.switch_to_deploy()\nblocks[\"STMBlock (deploy)\"] = stm_deploy\n\nfast_deploy = FastSTMBlock(channels, channels)\nfast_deploy.switch_to_deploy()\nblocks[\"FastSTMBlock (deploy)\"] = fast_deploy\n\nprint(f\"\\n{'Block':<25} {'Mean (ms)':>12} {'Std (ms)':>12} {'vs DWBlock':>15}\")\nprint(\"-\" * 68)\n\ndw_latency = None\nfor name, block in blocks.items():\n    mean, std = measure_latency_gpu(block, x)\n    if mean is None:\n        continue\n    if \"DWBlock (train)\" in name:\n        dw_latency = mean\n        diff_str = \"baseline\"\n    else:\n        diff = ((mean - dw_latency) / dw_latency) * 100\n        diff_str = f\"{diff:+.1f}%\"\n    print(f\"{name:<25} {mean:>12.4f} {std:>12.4f} {diff_str:>15}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## 6. CPU Latency Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T03:50:49.506286Z",
     "iopub.status.busy": "2026-02-19T03:50:49.506188Z",
     "iopub.status.idle": "2026-02-19T03:50:49.665821Z",
     "shell.execute_reply": "2026-02-19T03:50:49.665473Z",
     "shell.execute_reply.started": "2026-02-19T03:50:49.506277Z"
    }
   },
   "outputs": [],
   "source": "channels = 64\nresolution = 64\nbatch_size = 1\n\nx = torch.randn(batch_size, channels, resolution, resolution)\n\nprint(\"=\" * 80)\nprint(f\"CPU Latency Comparison\")\nprint(f\"Input: ({batch_size}, {channels}, {resolution}, {resolution})\")\nprint(\"=\" * 80)\n\nblocks = OrderedDict([\n    (\"DWBlock (train)\", DWBlock(channels, channels)),\n    (\"STMBlock (train)\", STMBlock(channels, channels)),\n    (\"FastSTMBlock (train)\", FastSTMBlock(channels, channels)),\n])\n\n# Deploy mode\ndw_deploy = DWBlock(channels, channels)\ndw_deploy.switch_to_deploy()\nblocks[\"DWBlock (deploy)\"] = dw_deploy\n\nstm_deploy = STMBlock(channels, channels)\nstm_deploy.switch_to_deploy()\nblocks[\"STMBlock (deploy)\"] = stm_deploy\n\nfast_deploy = FastSTMBlock(channels, channels)\nfast_deploy.switch_to_deploy()\nblocks[\"FastSTMBlock (deploy)\"] = fast_deploy\n\nprint(f\"\\n{'Block':<25} {'Mean (ms)':>12} {'Std (ms)':>12} {'vs DWBlock':>15}\")\nprint(\"-\" * 68)\n\ndw_latency = None\nfor name, block in blocks.items():\n    mean, std = measure_latency_cpu(block, x)\n    if \"DWBlock (train)\" in name:\n        dw_latency = mean\n        diff_str = \"baseline\"\n    else:\n        diff = ((mean - dw_latency) / dw_latency) * 100\n        diff_str = f\"{diff:+.1f}%\"\n    print(f\"{name:<25} {mean:>12.4f} {std:>12.4f} {diff_str:>15}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## 7. Multi-Channel Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T03:50:49.666387Z",
     "iopub.status.busy": "2026-02-19T03:50:49.666226Z",
     "iopub.status.idle": "2026-02-19T03:50:50.226806Z",
     "shell.execute_reply": "2026-02-19T03:50:50.226525Z",
     "shell.execute_reply.started": "2026-02-19T03:50:49.666373Z"
    }
   },
   "outputs": [],
   "source": "channel_list = [24, 48, 64, 96, 128, 192]\nresolution = 64\n\nprint(\"=\" * 95)\nprint(\"GPU Latency vs Channel Size (deploy mode, resolution=64)\")\nprint(\"=\" * 95)\n\nresults_gpu = {\"DWBlock\": [], \"STMBlock\": [], \"FastSTMBlock\": []}\nparams = {\"DWBlock\": [], \"STMBlock\": [], \"FastSTMBlock\": []}\n\nfor ch in channel_list:\n    x = torch.randn(1, ch, resolution, resolution)\n    \n    # DWBlock\n    block = DWBlock(ch, ch)\n    block.switch_to_deploy()\n    params[\"DWBlock\"].append(count_params(block))\n    mean, _ = measure_latency_gpu(block, x)\n    results_gpu[\"DWBlock\"].append(mean)\n    \n    # STMBlock\n    block = STMBlock(ch, ch)\n    block.switch_to_deploy()\n    params[\"STMBlock\"].append(count_params(block))\n    mean, _ = measure_latency_gpu(block, x)\n    results_gpu[\"STMBlock\"].append(mean)\n    \n    # FastSTMBlock\n    block = FastSTMBlock(ch, ch)\n    block.switch_to_deploy()\n    params[\"FastSTMBlock\"].append(count_params(block))\n    mean, _ = measure_latency_gpu(block, x)\n    results_gpu[\"FastSTMBlock\"].append(mean)\n\nprint(f\"\\n{'Ch':<6} {'DWBlock':<12} {'STMBlock':<12} {'FastSTM':<12} {'DW Params':<10} {'STM Params':<10} {'Fast Params':<10}\")\nprint(\"-\" * 78)\n\nfor i, ch in enumerate(channel_list):\n    print(f\"{ch:<6} {results_gpu['DWBlock'][i]:.4f} ms   {results_gpu['STMBlock'][i]:.4f} ms   {results_gpu['FastSTMBlock'][i]:.4f} ms   {params['DWBlock'][i]:<10,} {params['STMBlock'][i]:<10,} {params['FastSTMBlock'][i]:<10,}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T03:50:50.227193Z",
     "iopub.status.busy": "2026-02-19T03:50:50.227109Z",
     "iopub.status.idle": "2026-02-19T03:50:50.656369Z",
     "shell.execute_reply": "2026-02-19T03:50:50.656100Z",
     "shell.execute_reply.started": "2026-02-19T03:50:50.227185Z"
    }
   },
   "outputs": [],
   "source": "print(\"\\n\" + \"=\" * 80)\nprint(\"CPU Latency vs Channel Size (deploy mode, resolution=64)\")\nprint(\"=\" * 80)\n\nresults_cpu = {\"DWBlock\": [], \"STMBlock\": [], \"FastSTMBlock\": []}\n\nfor ch in channel_list:\n    x = torch.randn(1, ch, resolution, resolution)\n    \n    block = DWBlock(ch, ch)\n    block.switch_to_deploy()\n    mean, _ = measure_latency_cpu(block, x)\n    results_cpu[\"DWBlock\"].append(mean)\n    \n    block = STMBlock(ch, ch)\n    block.switch_to_deploy()\n    mean, _ = measure_latency_cpu(block, x)\n    results_cpu[\"STMBlock\"].append(mean)\n    \n    block = FastSTMBlock(ch, ch)\n    block.switch_to_deploy()\n    mean, _ = measure_latency_cpu(block, x)\n    results_cpu[\"FastSTMBlock\"].append(mean)\n\nprint(f\"\\n{'Channels':<10} {'DWBlock':<15} {'STMBlock':<15} {'FastSTMBlock':<15}\")\nprint(\"-\" * 58)\n\nfor i, ch in enumerate(channel_list):\n    print(f\"{ch:<10} {results_cpu['DWBlock'][i]:.4f} ms{'':<6} {results_cpu['STMBlock'][i]:.4f} ms{'':<6} {results_cpu['FastSTMBlock'][i]:.4f} ms\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## 8. Multi-Resolution Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T03:50:50.656759Z",
     "iopub.status.busy": "2026-02-19T03:50:50.656671Z",
     "iopub.status.idle": "2026-02-19T03:50:51.529037Z",
     "shell.execute_reply": "2026-02-19T03:50:51.528708Z",
     "shell.execute_reply.started": "2026-02-19T03:50:50.656750Z"
    }
   },
   "outputs": [],
   "source": "resolutions = [16, 32, 64, 128, 256]\nchannels = 64\n\nprint(\"=\" * 80)\nprint(\"GPU Latency vs Resolution (deploy mode, channels=64)\")\nprint(\"=\" * 80)\n\nresults_gpu = {\"DWBlock\": [], \"STMBlock\": [], \"FastSTMBlock\": []}\n\nfor res in resolutions:\n    x = torch.randn(1, channels, res, res)\n    \n    block = DWBlock(channels, channels)\n    block.switch_to_deploy()\n    mean, _ = measure_latency_gpu(block, x)\n    results_gpu[\"DWBlock\"].append(mean)\n    \n    block = STMBlock(channels, channels)\n    block.switch_to_deploy()\n    mean, _ = measure_latency_gpu(block, x)\n    results_gpu[\"STMBlock\"].append(mean)\n    \n    block = FastSTMBlock(channels, channels)\n    block.switch_to_deploy()\n    mean, _ = measure_latency_gpu(block, x)\n    results_gpu[\"FastSTMBlock\"].append(mean)\n\nprint(f\"\\n{'Resolution':<12} {'DWBlock':<15} {'STMBlock':<15} {'FastSTMBlock':<15}\")\nprint(\"-\" * 60)\n\nfor i, res in enumerate(resolutions):\n    print(f\"{res}x{res:<10} {results_gpu['DWBlock'][i]:.4f} ms{'':<6} {results_gpu['STMBlock'][i]:.4f} ms{'':<6} {results_gpu['FastSTMBlock'][i]:.4f} ms\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T03:50:51.529490Z",
     "iopub.status.busy": "2026-02-19T03:50:51.529353Z",
     "iopub.status.idle": "2026-02-19T03:50:52.738991Z",
     "shell.execute_reply": "2026-02-19T03:50:52.738698Z",
     "shell.execute_reply.started": "2026-02-19T03:50:51.529481Z"
    }
   },
   "outputs": [],
   "source": "print(\"\\n\" + \"=\" * 80)\nprint(\"CPU Latency vs Resolution (deploy mode, channels=64)\")\nprint(\"=\" * 80)\n\nresults_cpu = {\"DWBlock\": [], \"STMBlock\": [], \"FastSTMBlock\": []}\n\nfor res in resolutions:\n    x = torch.randn(1, channels, res, res)\n    \n    block = DWBlock(channels, channels)\n    block.switch_to_deploy()\n    mean, _ = measure_latency_cpu(block, x)\n    results_cpu[\"DWBlock\"].append(mean)\n    \n    block = STMBlock(channels, channels)\n    block.switch_to_deploy()\n    mean, _ = measure_latency_cpu(block, x)\n    results_cpu[\"STMBlock\"].append(mean)\n    \n    block = FastSTMBlock(channels, channels)\n    block.switch_to_deploy()\n    mean, _ = measure_latency_cpu(block, x)\n    results_cpu[\"FastSTMBlock\"].append(mean)\n\nprint(f\"\\n{'Resolution':<12} {'DWBlock':<15} {'STMBlock':<15} {'FastSTMBlock':<15}\")\nprint(\"-\" * 60)\n\nfor i, res in enumerate(resolutions):\n    print(f\"{res}x{res:<10} {results_cpu['DWBlock'][i]:.4f} ms{'':<6} {results_cpu['STMBlock'][i]:.4f} ms{'':<6} {results_cpu['FastSTMBlock'][i]:.4f} ms\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "## 9. Channel Expansion Comparison (in_ch != out_ch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T03:50:52.740284Z",
     "iopub.status.busy": "2026-02-19T03:50:52.740142Z",
     "iopub.status.idle": "2026-02-19T03:50:53.349522Z",
     "shell.execute_reply": "2026-02-19T03:50:53.349144Z",
     "shell.execute_reply.started": "2026-02-19T03:50:52.740274Z"
    }
   },
   "outputs": [],
   "source": "channel_pairs = [(24, 48), (48, 64), (64, 96), (96, 128), (128, 192)]\nresolution = 64\n\nprint(\"=\" * 85)\nprint(\"GPU Latency with Channel Expansion (deploy mode)\")\nprint(\"=\" * 85)\n\nresults = {\"DWBlock\": [], \"STMBlock\": [], \"FastSTMBlock\": []}\n\nfor in_ch, out_ch in channel_pairs:\n    x = torch.randn(1, in_ch, resolution, resolution)\n    \n    block = DWBlock(in_ch, out_ch)\n    block.switch_to_deploy()\n    mean, _ = measure_latency_gpu(block, x)\n    results[\"DWBlock\"].append(mean)\n    \n    block = STMBlock(in_ch, out_ch)\n    block.switch_to_deploy()\n    mean, _ = measure_latency_gpu(block, x)\n    results[\"STMBlock\"].append(mean)\n    \n    block = FastSTMBlock(in_ch, out_ch)\n    block.switch_to_deploy()\n    mean, _ = measure_latency_gpu(block, x)\n    results[\"FastSTMBlock\"].append(mean)\n\nprint(f\"\\n{'in->out':<15} {'DWBlock':<15} {'STMBlock':<15} {'FastSTMBlock':<15}\")\nprint(\"-\" * 62)\n\nfor i, (in_ch, out_ch) in enumerate(channel_pairs):\n    print(f\"{in_ch}->{out_ch:<10} {results['DWBlock'][i]:.4f} ms{'':<6} {results['STMBlock'][i]:.4f} ms{'':<6} {results['FastSTMBlock'][i]:.4f} ms\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "## 10. Component-wise Latency Breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T03:50:53.349868Z",
     "iopub.status.busy": "2026-02-19T03:50:53.349786Z",
     "iopub.status.idle": "2026-02-19T03:50:53.400863Z",
     "shell.execute_reply": "2026-02-19T03:50:53.400531Z",
     "shell.execute_reply.started": "2026-02-19T03:50:53.349859Z"
    }
   },
   "outputs": [],
   "source": "def measure_component(module, x, warmup=50, repeat=200):\n    \"\"\"Measure single component latency\"\"\"\n    if torch.cuda.is_available():\n        module = module.cuda().eval()\n        x = x.cuda()\n        \n        with torch.no_grad():\n            for _ in range(warmup):\n                _ = module(x)\n                torch.cuda.synchronize()\n        \n        times = []\n        with torch.no_grad():\n            for _ in range(repeat):\n                torch.cuda.synchronize()\n                start = time.perf_counter()\n                _ = module(x)\n                torch.cuda.synchronize()\n                end = time.perf_counter()\n                times.append((end - start) * 1000)\n        return np.mean(times)\n    return None\n\nchannels = 64\nhalf_ch = channels // 2\nresolution = 64\n\nx_full = torch.randn(1, channels, resolution, resolution)\nx_half = torch.randn(1, half_ch, resolution, resolution)\n\nprint(\"=\" * 75)\nprint(f\"Component-wise GPU Latency (channels={channels}, resolution={resolution})\")\nprint(\"=\" * 75)\n\nprint(\"\\n--- DWBlock Components ---\")\nrepconv_full = RepConv(channels, channels, kernel_size=7, padding=3, groups=channels)\nrepconv_full.switch_to_deploy()\nt = measure_component(repconv_full, x_full)\nprint(f\"  RepConv 7x7 DW ({channels}ch, deploy): {t:.4f} ms\")\n\nprcm = PRCM(channels, num_basis=8, dropout_rate=0)\nt = measure_component(prcm, x_full)\nprint(f\"  PRCM ({channels}ch):                   {t:.4f} ms\")\n\nprint(\"\\n--- STMBlock Components ---\")\n\nclass ChunkOp(nn.Module):\n    def forward(self, x):\n        return torch.chunk(x, 2, dim=1)\nt = measure_component(ChunkOp(), x_full)\nprint(f\"  torch.chunk (split):                 {t:.4f} ms\")\n\nrepconv_half = RepConv(half_ch, half_ch, kernel_size=7, padding=3, groups=half_ch)\nrepconv_half.switch_to_deploy()\nt = measure_component(repconv_half, x_half)\nprint(f\"  RepConv 7x7 DW ({half_ch}ch, deploy):  {t:.4f} ms\")\n\naffine_prcm = AffinePRCM(half_ch, num_basis=8, dropout_rate=0)\nt = measure_component(affine_prcm, x_half)\nprint(f\"  AffinePRCM ({half_ch}ch):              {t:.4f} ms\")\n\nclass ConcatOp(nn.Module):\n    def forward(self, x):\n        a, b = torch.chunk(x, 2, dim=1)\n        return torch.cat([a, b], dim=1)\nt = measure_component(ConcatOp(), x_full)\nprint(f\"  torch.cat (concat):                  {t:.4f} ms\")\n\nclass ShuffleOp(nn.Module):\n    def forward(self, x):\n        B, C, H, W = x.shape\n        x = x.view(B, 2, C//2, H, W)\n        x = torch.transpose(x, 1, 2).contiguous()\n        return x.view(B, C, H, W)\nt = measure_component(ShuffleOp(), x_full)\nprint(f\"  Channel Shuffle:                     {t:.4f} ms\")\n\nprint(\"\\n--- FastSTMBlock Components ---\")\n\n# Slicing (zero-copy view)\nclass SliceOp(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.dim = dim\n    def forward(self, x):\n        return x[:, :self.dim, :, :], x[:, self.dim:, :, :]\nt = measure_component(SliceOp(half_ch), x_full)\nprint(f\"  Slicing (zero-copy view):            {t:.4f} ms\")\n\nsimple_prcm = SimplePRCM(half_ch, num_basis=8, dropout_rate=0)\nt = measure_component(simple_prcm, x_half)\nprint(f\"  SimplePRCM ({half_ch}ch):              {t:.4f} ms\")\n\n# 1x1 Conv Mixing\nmix_block = nn.Sequential(\n    nn.Conv2d(channels, channels, 1, bias=False),\n    nn.BatchNorm2d(channels),\n    nn.ReLU(inplace=True)\n)\nt = measure_component(mix_block, x_full)\nprint(f\"  1x1 Conv + BN + ReLU (mixing):       {t:.4f} ms\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "## 11. Batch Size Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T03:50:53.401248Z",
     "iopub.status.busy": "2026-02-19T03:50:53.401143Z",
     "iopub.status.idle": "2026-02-19T03:50:54.571792Z",
     "shell.execute_reply": "2026-02-19T03:50:54.571368Z",
     "shell.execute_reply.started": "2026-02-19T03:50:53.401239Z"
    }
   },
   "outputs": [],
   "source": "batch_sizes = [1, 2, 4, 8, 16]\nchannels = 64\nresolution = 64\n\nprint(\"=\" * 80)\nprint(\"GPU Latency vs Batch Size (deploy mode)\")\nprint(\"=\" * 80)\n\nresults = {\"DWBlock\": [], \"STMBlock\": [], \"FastSTMBlock\": []}\n\nfor bs in batch_sizes:\n    x = torch.randn(bs, channels, resolution, resolution)\n    \n    block = DWBlock(channels, channels)\n    block.switch_to_deploy()\n    mean, _ = measure_latency_gpu(block, x)\n    results[\"DWBlock\"].append(mean)\n    \n    block = STMBlock(channels, channels)\n    block.switch_to_deploy()\n    mean, _ = measure_latency_gpu(block, x)\n    results[\"STMBlock\"].append(mean)\n    \n    block = FastSTMBlock(channels, channels)\n    block.switch_to_deploy()\n    mean, _ = measure_latency_gpu(block, x)\n    results[\"FastSTMBlock\"].append(mean)\n\nprint(f\"\\n{'Batch':<10} {'DWBlock':<15} {'STMBlock':<15} {'FastSTMBlock':<15}\")\nprint(\"-\" * 58)\n\nfor i, bs in enumerate(batch_sizes):\n    print(f\"{bs:<10} {results['DWBlock'][i]:.4f} ms{'':<6} {results['STMBlock'][i]:.4f} ms{'':<6} {results['FastSTMBlock'][i]:.4f} ms\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-28",
   "metadata": {},
   "source": [
    "## 12. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-29",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T03:50:54.572208Z",
     "iopub.status.busy": "2026-02-19T03:50:54.572122Z",
     "iopub.status.idle": "2026-02-19T03:50:54.574295Z",
     "shell.execute_reply": "2026-02-19T03:50:54.574091Z",
     "shell.execute_reply.started": "2026-02-19T03:50:54.572199Z"
    }
   },
   "outputs": [],
   "source": "print(\"=\" * 75)\nprint(\"ABLATION SUMMARY\")\nprint(\"=\" * 75)\nprint(\"\"\"\n┌─────────────────────────────────────────────────────────────────────────┐\n│ DWBlock (JeongWonNet77_Rep256Basis8S24Drop)                             │\n├─────────────────────────────────────────────────────────────────────────┤\n│ Structure: [1x1 Conv] -> RepConv 7x7 DW (full C) -> PRCM                │\n│ - All channels go through depthwise convolution                         │\n│ - PRCM: scale only (x * α)                                              │\n│ - Most parameters (larger RepConv, full-channel PRCM)                   │\n└─────────────────────────────────────────────────────────────────────────┘\n\n┌─────────────────────────────────────────────────────────────────────────┐\n│ STMBlock (JeongWonNet_STMShuffle_NoStem)                                │\n├─────────────────────────────────────────────────────────────────────────┤\n│ Structure: chunk -> [Passive | Active] -> cat -> Channel Shuffle        │\n│ - Only C/2 channels go through heavy computation                        │\n│ - AffinePRCM: scale + shift (x * α + β)                                 │\n│ - Channel shuffle for information mixing                                │\n│ - Fewer parameters than DWBlock                                         │\n└─────────────────────────────────────────────────────────────────────────┘\n\n┌─────────────────────────────────────────────────────────────────────────┐\n│ FastSTMBlock (JeongWonNet_STMNoConcatShuffle_NoStemMS)                  │\n├─────────────────────────────────────────────────────────────────────────┤\n│ Structure: slice -> [Active | Passive] -> cat -> 1x1 Conv Mix           │\n│ - Slicing instead of chunk (zero-copy view)                             │\n│ - SimplePRCM: scale only (no shift overhead)                            │\n│ - 1x1 Conv + BN + ReLU for mixing (more learnable than shuffle)         │\n│ - More parameters due to 1x1 mixing conv (C*C params)                   │\n└─────────────────────────────────────────────────────────────────────────┘\n\nKey Findings:\n  1. DWBlock: Highest latency (full channel processing)\n  2. STMBlock: Lower latency, fewer params (half channel + shuffle)\n  3. FastSTMBlock: More params (1x1 mixing), but potentially faster on GPU\n     - 1x1 Conv is highly optimized on GPU (cuBLAS GEMM)\n     - Shuffle requires memory reordering\n\nTrade-offs:\n  ┌────────────────┬──────────────┬──────────────┬──────────────┐\n  │ Metric         │ DWBlock      │ STMBlock     │ FastSTMBlock │\n  ├────────────────┼──────────────┼──────────────┼──────────────┤\n  │ Parameters     │ Highest      │ Lowest       │ Medium       │\n  │ GPU Latency    │ Medium       │ Medium       │ Depends      │\n  │ CPU Latency    │ Lowest       │ Medium       │ Highest      │\n  │ Expressiveness │ Full channel │ Half + shift │ Half + learn │\n  │ GPU-friendly   │ Yes          │ Yes          │ Best (GEMM)  │\n  └────────────────┴──────────────┴──────────────┴──────────────┘\n\nRecommendations:\n  - GPU deployment: FastSTMBlock (1x1 conv well-optimized)\n  - CPU deployment: DWBlock (simpler, no extra mixing layer)\n  - Low-param budget: STMBlock (shuffle is parameter-free)\n  - Best accuracy: Needs experimental validation\n\"\"\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Skin_Lesion_Seg (Python 3.8)",
   "language": "python",
   "name": "skin_lesion_seg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}