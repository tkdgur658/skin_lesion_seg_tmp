{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Block Latency Comparison\n",
    "## JeongWonNet Block vs ConvNeXt Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T03:27:53.548019Z",
     "iopub.status.busy": "2026-02-19T03:27:53.547948Z",
     "iopub.status.idle": "2026-02-19T03:27:54.101873Z",
     "shell.execute_reply": "2026-02-19T03:27:54.101526Z",
     "shell.execute_reply.started": "2026-02-19T03:27:53.548011Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Block Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T03:27:54.102378Z",
     "iopub.status.busy": "2026-02-19T03:27:54.102274Z",
     "iopub.status.idle": "2026-02-19T03:27:54.110303Z",
     "shell.execute_reply": "2026-02-19T03:27:54.110107Z",
     "shell.execute_reply.started": "2026-02-19T03:27:54.102369Z"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# JeongWonNet Block (RepConv + PRCM)\n",
    "# ============================================\n",
    "\n",
    "class RepConv(nn.Module):\n",
    "    \"\"\"Re-parameterizable Convolution Block\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3,\n",
    "                 stride=1, padding=1, groups=1, use_activation=True):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.groups = groups\n",
    "        self.use_identity = (stride == 1) and (in_channels == out_channels)\n",
    "        \n",
    "        self.conv_kxk = nn.Conv2d(in_channels, out_channels, kernel_size,\n",
    "                                  stride, padding, groups=groups, bias=False)\n",
    "        self.bn_kxk = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        if kernel_size > 1:\n",
    "            self.conv_1x1 = nn.Conv2d(in_channels, out_channels, 1,\n",
    "                                      stride, 0, groups=groups, bias=False)\n",
    "            self.bn_1x1 = nn.BatchNorm2d(out_channels)\n",
    "        else:\n",
    "            self.conv_1x1 = None\n",
    "        \n",
    "        if self.use_identity:\n",
    "            self.bn_identity = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        self.activation = nn.ReLU(inplace=True) if use_activation else nn.Identity()\n",
    "           \n",
    "    def forward(self, x):\n",
    "        if hasattr(self, 'fused_conv'):\n",
    "            return self.activation(self.fused_conv(x))\n",
    "        \n",
    "        out = self.bn_kxk(self.conv_kxk(x))\n",
    "        if self.conv_1x1 is not None:\n",
    "            out += self.bn_1x1(self.conv_1x1(x))\n",
    "        if self.use_identity:\n",
    "            out += self.bn_identity(x)\n",
    "        return self.activation(out)\n",
    "    \n",
    "    def switch_to_deploy(self):\n",
    "        if hasattr(self, 'fused_conv'):\n",
    "            return\n",
    "        \n",
    "        kernel, bias = self._fuse_bn_tensor(self.conv_kxk, self.bn_kxk)\n",
    "        \n",
    "        if self.conv_1x1 is not None:\n",
    "            kernel_1x1, bias_1x1 = self._fuse_bn_tensor(self.conv_1x1, self.bn_1x1)\n",
    "            kernel += self._pad_1x1_to_kxk(kernel_1x1)\n",
    "            bias += bias_1x1\n",
    "        \n",
    "        if self.use_identity:\n",
    "            kernel_identity, bias_identity = self._fuse_bn_tensor(None, self.bn_identity)\n",
    "            kernel += kernel_identity\n",
    "            bias += bias_identity\n",
    "        \n",
    "        self.fused_conv = nn.Conv2d(\n",
    "            self.in_channels, self.out_channels, self.kernel_size,\n",
    "            self.stride, self.padding, groups=self.groups, bias=True\n",
    "        )\n",
    "        self.fused_conv.weight.data = kernel\n",
    "        self.fused_conv.bias.data = bias\n",
    "        \n",
    "        self.__delattr__('conv_kxk')\n",
    "        self.__delattr__('bn_kxk')\n",
    "        if self.conv_1x1 is not None:\n",
    "            self.__delattr__('conv_1x1')\n",
    "            self.__delattr__('bn_1x1')\n",
    "        if hasattr(self, 'bn_identity'):\n",
    "            self.__delattr__('bn_identity')\n",
    "   \n",
    "    def _fuse_bn_tensor(self, conv, bn):\n",
    "        if conv is None:\n",
    "            input_dim = self.in_channels // self.groups\n",
    "            kernel_value = torch.zeros((self.in_channels, input_dim,\n",
    "                                        self.kernel_size, self.kernel_size),\n",
    "                                       dtype=bn.weight.dtype, device=bn.weight.device)\n",
    "            for i in range(self.in_channels):\n",
    "                kernel_value[i, i % input_dim,\n",
    "                             self.kernel_size // 2, self.kernel_size // 2] = 1\n",
    "            kernel = kernel_value\n",
    "        else:\n",
    "            kernel = conv.weight\n",
    "        \n",
    "        std = torch.sqrt(bn.running_var + bn.eps)\n",
    "        t = (bn.weight / std).reshape(-1, 1, 1, 1)\n",
    "        return kernel * t, bn.bias - bn.running_mean * bn.weight / std\n",
    "   \n",
    "    def _pad_1x1_to_kxk(self, kernel_1x1):\n",
    "        if self.kernel_size == 1:\n",
    "            return kernel_1x1\n",
    "        pad = self.kernel_size // 2\n",
    "        return F.pad(kernel_1x1, [pad, pad, pad, pad])\n",
    "\n",
    "\n",
    "class PRCM(nn.Module):\n",
    "    \"\"\"Pattern Recalibration Module\"\"\"\n",
    "    def __init__(self, channels, num_basis=8):\n",
    "        super().__init__()\n",
    "        self.basis = nn.Parameter(torch.randn(num_basis, channels))\n",
    "        self.fuser = nn.Linear(num_basis, channels, bias=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        ctx = x.mean(dim=[2, 3])\n",
    "        coeff = ctx @ self.basis.t()\n",
    "        w = self.fuser(coeff).sigmoid().unsqueeze(-1).unsqueeze(-1)\n",
    "        return x * w\n",
    "\n",
    "\n",
    "class JeongWonBlock(nn.Module):\n",
    "    \"\"\"JeongWonNet Block: 1x1 Conv + RepConv DW + PRCM\"\"\"\n",
    "    def __init__(self, in_ch, out_ch, kernel_size=7, num_basis=8):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        if in_ch != out_ch:\n",
    "            layers.append(nn.Conv2d(in_ch, out_ch, kernel_size=1, bias=False))\n",
    "        layers.append(RepConv(out_ch, out_ch, kernel_size=kernel_size, \n",
    "                              padding=kernel_size//2, groups=out_ch))\n",
    "        self.conv = nn.Sequential(*layers)\n",
    "        self.prcm = PRCM(out_ch, num_basis=num_basis)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.prcm(self.conv(x))\n",
    "    \n",
    "    def switch_to_deploy(self):\n",
    "        # self.modules()는 자기 자신을 포함하므로 children() 사용\n",
    "        for m in self.conv.modules():\n",
    "            if isinstance(m, RepConv):\n",
    "                m.switch_to_deploy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T03:27:54.110685Z",
     "iopub.status.busy": "2026-02-19T03:27:54.110534Z",
     "iopub.status.idle": "2026-02-19T03:27:54.120558Z",
     "shell.execute_reply": "2026-02-19T03:27:54.120370Z",
     "shell.execute_reply.started": "2026-02-19T03:27:54.110678Z"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ConvNeXt Block\n",
    "# ============================================\n",
    "\n",
    "class LayerNorm2d(nn.Module):\n",
    "    \"\"\"LayerNorm for channels-first tensors (B, C, H, W)\"\"\"\n",
    "    def __init__(self, channels, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(channels))\n",
    "        self.bias = nn.Parameter(torch.zeros(channels))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        u = x.mean(1, keepdim=True)\n",
    "        s = (x - u).pow(2).mean(1, keepdim=True)\n",
    "        x = (x - u) / torch.sqrt(s + self.eps)\n",
    "        x = self.weight[:, None, None] * x + self.bias[:, None, None]\n",
    "        return x\n",
    "\n",
    "\n",
    "class ConvNeXtBlock(nn.Module):\n",
    "    \"\"\"ConvNeXt Block: DW Conv 7x7 -> LN -> 1x1 -> GELU -> 1x1\"\"\"\n",
    "    def __init__(self, channels, expansion=4, kernel_size=7):\n",
    "        super().__init__()\n",
    "        hidden = channels * expansion\n",
    "        \n",
    "        self.dwconv = nn.Conv2d(channels, channels, kernel_size, \n",
    "                                padding=kernel_size//2, groups=channels)\n",
    "        self.norm = LayerNorm2d(channels)\n",
    "        self.pwconv1 = nn.Conv2d(channels, hidden, 1)\n",
    "        self.act = nn.GELU()\n",
    "        self.pwconv2 = nn.Conv2d(hidden, channels, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.dwconv(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.pwconv1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.pwconv2(x)\n",
    "        return x + residual\n",
    "\n",
    "\n",
    "class ConvNeXtBlockV2(nn.Module):\n",
    "    \"\"\"ConvNeXt V2 Block with GRN\"\"\"\n",
    "    def __init__(self, channels, expansion=4, kernel_size=7):\n",
    "        super().__init__()\n",
    "        hidden = channels * expansion\n",
    "        \n",
    "        self.dwconv = nn.Conv2d(channels, channels, kernel_size, \n",
    "                                padding=kernel_size//2, groups=channels)\n",
    "        self.norm = LayerNorm2d(channels)\n",
    "        self.pwconv1 = nn.Conv2d(channels, hidden, 1)\n",
    "        self.act = nn.GELU()\n",
    "        # GRN (Global Response Normalization)\n",
    "        self.grn_gamma = nn.Parameter(torch.zeros(1, hidden, 1, 1))\n",
    "        self.grn_beta = nn.Parameter(torch.zeros(1, hidden, 1, 1))\n",
    "        self.pwconv2 = nn.Conv2d(hidden, channels, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.dwconv(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.pwconv1(x)\n",
    "        x = self.act(x)\n",
    "        # GRN\n",
    "        gx = torch.norm(x, p=2, dim=(2, 3), keepdim=True)\n",
    "        nx = gx / (gx.mean(dim=1, keepdim=True) + 1e-6)\n",
    "        x = x + self.grn_gamma * (x * nx) + self.grn_beta\n",
    "        x = self.pwconv2(x)\n",
    "        return x + residual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Latency Measurement Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T03:27:54.120915Z",
     "iopub.status.busy": "2026-02-19T03:27:54.120781Z",
     "iopub.status.idle": "2026-02-19T03:27:54.124707Z",
     "shell.execute_reply": "2026-02-19T03:27:54.124513Z",
     "shell.execute_reply.started": "2026-02-19T03:27:54.120908Z"
    }
   },
   "outputs": [],
   "source": [
    "def count_params(model):\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "def measure_latency_cpu(model, input_tensor, warmup=10, repeat=100):\n",
    "    \"\"\"CPU latency measurement\"\"\"\n",
    "    model.eval()\n",
    "    model.cpu()\n",
    "    input_tensor = input_tensor.cpu()\n",
    "    \n",
    "    # Warmup\n",
    "    with torch.no_grad():\n",
    "        for _ in range(warmup):\n",
    "            _ = model(input_tensor)\n",
    "    \n",
    "    # Measure\n",
    "    times = []\n",
    "    with torch.no_grad():\n",
    "        for _ in range(repeat):\n",
    "            start = time.perf_counter()\n",
    "            _ = model(input_tensor)\n",
    "            end = time.perf_counter()\n",
    "            times.append((end - start) * 1000)  # ms\n",
    "    \n",
    "    return np.mean(times), np.std(times)\n",
    "\n",
    "def measure_latency_gpu(model, input_tensor, warmup=50, repeat=200):\n",
    "    \"\"\"GPU latency measurement with CUDA synchronization\"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"CUDA not available, skipping GPU test\")\n",
    "        return None, None\n",
    "    \n",
    "    model.eval()\n",
    "    model.cuda()\n",
    "    input_tensor = input_tensor.cuda()\n",
    "    \n",
    "    # Warmup\n",
    "    with torch.no_grad():\n",
    "        for _ in range(warmup):\n",
    "            _ = model(input_tensor)\n",
    "            torch.cuda.synchronize()\n",
    "    \n",
    "    # Measure\n",
    "    times = []\n",
    "    with torch.no_grad():\n",
    "        for _ in range(repeat):\n",
    "            torch.cuda.synchronize()\n",
    "            start = time.perf_counter()\n",
    "            _ = model(input_tensor)\n",
    "            torch.cuda.synchronize()\n",
    "            end = time.perf_counter()\n",
    "            times.append((end - start) * 1000)  # ms\n",
    "    \n",
    "    return np.mean(times), np.std(times)\n",
    "\n",
    "def measure_layer_latency_gpu(layer, input_tensor, warmup=50, repeat=200):\n",
    "    \"\"\"Single layer GPU latency\"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        return None\n",
    "    \n",
    "    layer.eval()\n",
    "    layer.cuda()\n",
    "    input_tensor = input_tensor.cuda()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(warmup):\n",
    "            _ = layer(input_tensor)\n",
    "            torch.cuda.synchronize()\n",
    "    \n",
    "    times = []\n",
    "    with torch.no_grad():\n",
    "        for _ in range(repeat):\n",
    "            torch.cuda.synchronize()\n",
    "            start = time.perf_counter()\n",
    "            _ = layer(input_tensor)\n",
    "            torch.cuda.synchronize()\n",
    "            end = time.perf_counter()\n",
    "            times.append((end - start) * 1000)\n",
    "    \n",
    "    return np.mean(times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Single Block Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T03:27:54.125474Z",
     "iopub.status.busy": "2026-02-19T03:27:54.125302Z",
     "iopub.status.idle": "2026-02-19T03:27:54.129761Z",
     "shell.execute_reply": "2026-02-19T03:27:54.129570Z",
     "shell.execute_reply.started": "2026-02-19T03:27:54.125466Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Single Block Comparison\n",
      "Input: (1, 64, 64, 64)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Test configurations\n",
    "channels = 64\n",
    "batch_size = 1\n",
    "resolution = 64  # Feature map size\n",
    "\n",
    "# Create blocks\n",
    "jeongwon_block = JeongWonBlock(channels, channels, kernel_size=7, num_basis=8)\n",
    "convnext_block = ConvNeXtBlock(channels, expansion=4, kernel_size=7)\n",
    "convnext_v2_block = ConvNeXtBlockV2(channels, expansion=4, kernel_size=7)\n",
    "\n",
    "# Input tensor\n",
    "x = torch.randn(batch_size, channels, resolution, resolution)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Single Block Comparison\")\n",
    "print(f\"Input: ({batch_size}, {channels}, {resolution}, {resolution})\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T03:27:54.130100Z",
     "iopub.status.busy": "2026-02-19T03:27:54.129970Z",
     "iopub.status.idle": "2026-02-19T03:27:54.131590Z",
     "shell.execute_reply": "2026-02-19T03:27:54.131411Z",
     "shell.execute_reply.started": "2026-02-19T03:27:54.130093Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Parameters]\n",
      "JeongWonBlock:    4,608\n",
      "ConvNeXtBlock:    36,416\n",
      "ConvNeXtBlockV2:  36,928\n"
     ]
    }
   ],
   "source": [
    "# Parameter count\n",
    "print(\"\\n[Parameters]\")\n",
    "print(f\"JeongWonBlock:    {count_params(jeongwon_block):,}\")\n",
    "print(f\"ConvNeXtBlock:    {count_params(convnext_block):,}\")\n",
    "print(f\"ConvNeXtBlockV2:  {count_params(convnext_v2_block):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T03:27:54.131932Z",
     "iopub.status.busy": "2026-02-19T03:27:54.131824Z",
     "iopub.status.idle": "2026-02-19T03:27:54.562249Z",
     "shell.execute_reply": "2026-02-19T03:27:54.562001Z",
     "shell.execute_reply.started": "2026-02-19T03:27:54.131924Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Layer-wise GPU Latency Breakdown]\n",
      "============================================================\n",
      "\n",
      "--- JeongWonBlock (train mode) ---\n",
      "  conv_kxk + bn_kxk (7x7 DW):  0.0406 ms\n",
      "  conv_1x1 + bn_1x1:           0.0181 ms\n",
      "  bn_identity:                 0.0124 ms\n",
      "  ReLU:                        0.0068 ms\n",
      "  PRCM:                        0.0259 ms\n",
      "  --------------------------\n",
      "  Total:                       0.0850 ms\n",
      "\n",
      "--- JeongWonBlock (deploy mode) ---\n",
      "  fused_conv + ReLU (7x7 DW): 0.0371 ms\n",
      "  PRCM:                        0.0248 ms\n",
      "  --------------------------\n",
      "  Total:                       0.0597 ms\n",
      "\n",
      "--- ConvNeXtBlock ---\n",
      "  dwconv (7x7 DW):             0.0341 ms\n",
      "  LayerNorm2d:                 0.0389 ms\n",
      "  pwconv1 (1x1, expand 4x):    0.0271 ms\n",
      "  GELU:                        0.0145 ms\n",
      "  pwconv2 (1x1, project):      0.0214 ms\n",
      "  --------------------------\n",
      "  Total:                       0.1180 ms\n",
      "\n",
      "--- ConvNeXtBlockV2 ---\n",
      "  dwconv (7x7 DW):             0.0341 ms\n",
      "  LayerNorm2d:                 0.0390 ms\n",
      "  pwconv1 (1x1, expand 4x):    0.0275 ms\n",
      "  GELU:                        0.0146 ms\n",
      "  GRN:                         0.0783 ms\n",
      "  pwconv2 (1x1, project):      0.0214 ms\n",
      "  --------------------------\n",
      "  Total:                       0.1905 ms\n"
     ]
    }
   ],
   "source": [
    "# Layer-wise Latency Breakdown\n",
    "print(\"\\n[Layer-wise GPU Latency Breakdown]\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "channels = 64\n",
    "resolution = 64\n",
    "x = torch.randn(1, channels, resolution, resolution).cuda()\n",
    "\n",
    "# =====================\n",
    "# JeongWonBlock Layers\n",
    "# =====================\n",
    "print(\"\\n--- JeongWonBlock (train mode) ---\")\n",
    "block = JeongWonBlock(channels, channels, kernel_size=7, num_basis=8).cuda().eval()\n",
    "\n",
    "# RepConv layers (inside self.conv)\n",
    "repconv = None\n",
    "for m in block.conv.modules():\n",
    "    if isinstance(m, RepConv):\n",
    "        repconv = m\n",
    "        break\n",
    "\n",
    "if repconv:\n",
    "    # conv_kxk + bn\n",
    "    t = measure_layer_latency_gpu(nn.Sequential(repconv.conv_kxk, repconv.bn_kxk), x)\n",
    "    print(f\"  conv_kxk + bn_kxk (7x7 DW):  {t:.4f} ms\")\n",
    "    \n",
    "    # conv_1x1 + bn\n",
    "    if repconv.conv_1x1 is not None:\n",
    "        t = measure_layer_latency_gpu(nn.Sequential(repconv.conv_1x1, repconv.bn_1x1), x)\n",
    "        print(f\"  conv_1x1 + bn_1x1:           {t:.4f} ms\")\n",
    "    \n",
    "    # bn_identity\n",
    "    if repconv.use_identity:\n",
    "        t = measure_layer_latency_gpu(repconv.bn_identity, x)\n",
    "        print(f\"  bn_identity:                 {t:.4f} ms\")\n",
    "    \n",
    "    # activation\n",
    "    t = measure_layer_latency_gpu(repconv.activation, x)\n",
    "    print(f\"  ReLU:                        {t:.4f} ms\")\n",
    "\n",
    "# PRCM\n",
    "t = measure_layer_latency_gpu(block.prcm, x)\n",
    "print(f\"  PRCM:                        {t:.4f} ms\")\n",
    "\n",
    "# Total\n",
    "total, _ = measure_latency_gpu(block, x)\n",
    "print(f\"  --------------------------\")\n",
    "print(f\"  Total:                       {total:.4f} ms\")\n",
    "\n",
    "# =====================\n",
    "# JeongWonBlock Deploy\n",
    "# =====================\n",
    "print(\"\\n--- JeongWonBlock (deploy mode) ---\")\n",
    "block_deploy = JeongWonBlock(channels, channels, kernel_size=7, num_basis=8)\n",
    "block_deploy.switch_to_deploy()\n",
    "block_deploy = block_deploy.cuda().eval()\n",
    "\n",
    "# Find fused conv\n",
    "for m in block_deploy.conv.modules():\n",
    "    if isinstance(m, RepConv) and hasattr(m, 'fused_conv'):\n",
    "        t = measure_layer_latency_gpu(nn.Sequential(m.fused_conv, m.activation), x)\n",
    "        print(f\"  fused_conv + ReLU (7x7 DW): {t:.4f} ms\")\n",
    "        break\n",
    "\n",
    "t = measure_layer_latency_gpu(block_deploy.prcm, x)\n",
    "print(f\"  PRCM:                        {t:.4f} ms\")\n",
    "\n",
    "total, _ = measure_latency_gpu(block_deploy, x)\n",
    "print(f\"  --------------------------\")\n",
    "print(f\"  Total:                       {total:.4f} ms\")\n",
    "\n",
    "# =====================\n",
    "# ConvNeXtBlock Layers\n",
    "# =====================\n",
    "print(\"\\n--- ConvNeXtBlock ---\")\n",
    "block = ConvNeXtBlock(channels, expansion=4, kernel_size=7).cuda().eval()\n",
    "\n",
    "t = measure_layer_latency_gpu(block.dwconv, x)\n",
    "print(f\"  dwconv (7x7 DW):             {t:.4f} ms\")\n",
    "\n",
    "t = measure_layer_latency_gpu(block.norm, x)\n",
    "print(f\"  LayerNorm2d:                 {t:.4f} ms\")\n",
    "\n",
    "t = measure_layer_latency_gpu(block.pwconv1, x)\n",
    "print(f\"  pwconv1 (1x1, expand 4x):    {t:.4f} ms\")\n",
    "\n",
    "x_expanded = torch.randn(1, channels * 4, resolution, resolution).cuda()\n",
    "t = measure_layer_latency_gpu(block.act, x_expanded)\n",
    "print(f\"  GELU:                        {t:.4f} ms\")\n",
    "\n",
    "t = measure_layer_latency_gpu(block.pwconv2, x_expanded)\n",
    "print(f\"  pwconv2 (1x1, project):      {t:.4f} ms\")\n",
    "\n",
    "total, _ = measure_latency_gpu(block, x)\n",
    "print(f\"  --------------------------\")\n",
    "print(f\"  Total:                       {total:.4f} ms\")\n",
    "\n",
    "# =====================\n",
    "# ConvNeXtBlockV2 Layers\n",
    "# =====================\n",
    "print(\"\\n--- ConvNeXtBlockV2 ---\")\n",
    "block = ConvNeXtBlockV2(channels, expansion=4, kernel_size=7).cuda().eval()\n",
    "\n",
    "t = measure_layer_latency_gpu(block.dwconv, x)\n",
    "print(f\"  dwconv (7x7 DW):             {t:.4f} ms\")\n",
    "\n",
    "t = measure_layer_latency_gpu(block.norm, x)\n",
    "print(f\"  LayerNorm2d:                 {t:.4f} ms\")\n",
    "\n",
    "t = measure_layer_latency_gpu(block.pwconv1, x)\n",
    "print(f\"  pwconv1 (1x1, expand 4x):    {t:.4f} ms\")\n",
    "\n",
    "t = measure_layer_latency_gpu(block.act, x_expanded)\n",
    "print(f\"  GELU:                        {t:.4f} ms\")\n",
    "\n",
    "# GRN timing\n",
    "class GRN(nn.Module):\n",
    "    def __init__(self, gamma, beta):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        self.beta = beta\n",
    "    def forward(self, x):\n",
    "        gx = torch.norm(x, p=2, dim=(2, 3), keepdim=True)\n",
    "        nx = gx / (gx.mean(dim=1, keepdim=True) + 1e-6)\n",
    "        return x + self.gamma * (x * nx) + self.beta\n",
    "\n",
    "grn = GRN(block.grn_gamma, block.grn_beta).cuda().eval()\n",
    "t = measure_layer_latency_gpu(grn, x_expanded)\n",
    "print(f\"  GRN:                         {t:.4f} ms\")\n",
    "\n",
    "t = measure_layer_latency_gpu(block.pwconv2, x_expanded)\n",
    "print(f\"  pwconv2 (1x1, project):      {t:.4f} ms\")\n",
    "\n",
    "total, _ = measure_latency_gpu(block, x)\n",
    "print(f\"  --------------------------\")\n",
    "print(f\"  Total:                       {total:.4f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T03:27:54.562762Z",
     "iopub.status.busy": "2026-02-19T03:27:54.562561Z",
     "iopub.status.idle": "2026-02-19T03:27:54.784813Z",
     "shell.execute_reply": "2026-02-19T03:27:54.784550Z",
     "shell.execute_reply.started": "2026-02-19T03:27:54.562753Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CPU Latency]\n",
      "JeongWonBlock (train):    0.268 ± 0.011 ms\n",
      "JeongWonBlock (deploy):   0.147 ± 0.009 ms\n",
      "ConvNeXtBlock:            0.686 ± 0.026 ms\n",
      "ConvNeXtBlockV2:          0.816 ± 0.049 ms\n"
     ]
    }
   ],
   "source": [
    "# CPU Latency\n",
    "print(\"\\n[CPU Latency]\")\n",
    "\n",
    "mean, std = measure_latency_cpu(jeongwon_block, x)\n",
    "print(f\"JeongWonBlock (train):    {mean:.3f} ± {std:.3f} ms\")\n",
    "\n",
    "# Deploy mode (fused)\n",
    "jeongwon_block_deploy = JeongWonBlock(channels, channels, kernel_size=7, num_basis=8)\n",
    "jeongwon_block_deploy.load_state_dict(jeongwon_block.state_dict())\n",
    "jeongwon_block_deploy.switch_to_deploy()\n",
    "mean, std = measure_latency_cpu(jeongwon_block_deploy, x)\n",
    "print(f\"JeongWonBlock (deploy):   {mean:.3f} ± {std:.3f} ms\")\n",
    "\n",
    "mean, std = measure_latency_cpu(convnext_block, x)\n",
    "print(f\"ConvNeXtBlock:            {mean:.3f} ± {std:.3f} ms\")\n",
    "\n",
    "mean, std = measure_latency_cpu(convnext_v2_block, x)\n",
    "print(f\"ConvNeXtBlockV2:          {mean:.3f} ± {std:.3f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T03:27:54.785208Z",
     "iopub.status.busy": "2026-02-19T03:27:54.785109Z",
     "iopub.status.idle": "2026-02-19T03:27:54.908228Z",
     "shell.execute_reply": "2026-02-19T03:27:54.907883Z",
     "shell.execute_reply.started": "2026-02-19T03:27:54.785201Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[GPU Latency]\n",
      "JeongWonBlock (train):    0.088 ± 0.028 ms\n",
      "JeongWonBlock (deploy):   0.060 ± 0.001 ms\n",
      "ConvNeXtBlock:            0.118 ± 0.002 ms\n",
      "ConvNeXtBlockV2:          0.187 ± 0.002 ms\n"
     ]
    }
   ],
   "source": [
    "# GPU Latency\n",
    "print(\"\\n[GPU Latency]\")\n",
    "\n",
    "jeongwon_block = JeongWonBlock(channels, channels, kernel_size=7, num_basis=8)\n",
    "mean, std = measure_latency_gpu(jeongwon_block, x)\n",
    "if mean: print(f\"JeongWonBlock (train):    {mean:.3f} ± {std:.3f} ms\")\n",
    "\n",
    "jeongwon_block_deploy = JeongWonBlock(channels, channels, kernel_size=7, num_basis=8)\n",
    "jeongwon_block_deploy.switch_to_deploy()\n",
    "mean, std = measure_latency_gpu(jeongwon_block_deploy, x)\n",
    "if mean: print(f\"JeongWonBlock (deploy):   {mean:.3f} ± {std:.3f} ms\")\n",
    "\n",
    "mean, std = measure_latency_gpu(convnext_block, x)\n",
    "if mean: print(f\"ConvNeXtBlock:            {mean:.3f} ± {std:.3f} ms\")\n",
    "\n",
    "mean, std = measure_latency_gpu(convnext_v2_block, x)\n",
    "if mean: print(f\"ConvNeXtBlockV2:          {mean:.3f} ± {std:.3f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Multi-Scale Comparison (Different Resolutions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T03:27:54.908578Z",
     "iopub.status.busy": "2026-02-19T03:27:54.908504Z",
     "iopub.status.idle": "2026-02-19T03:27:56.346231Z",
     "shell.execute_reply": "2026-02-19T03:27:56.345891Z",
     "shell.execute_reply.started": "2026-02-19T03:27:54.908570Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Multi-Resolution GPU Latency Comparison\n",
      "============================================================\n",
      "\n",
      "Resolution  JeongWon (deploy)   ConvNeXt            ConvNeXtV2          \n",
      "------------------------------------------------------------------------\n",
      "32x32        0.036 ms            0.077 ms            0.104 ms            \n",
      "64x64        0.062 ms            0.122 ms            0.188 ms            \n",
      "128x128       0.130 ms            0.361 ms            0.587 ms            \n",
      "256x256       0.451 ms            1.313 ms            2.098 ms            \n"
     ]
    }
   ],
   "source": [
    "resolutions = [32, 64, 128, 256]\n",
    "channels = 64\n",
    "batch_size = 1\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Multi-Resolution GPU Latency Comparison\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results = {\"JeongWon (deploy)\": [], \"ConvNeXt\": [], \"ConvNeXtV2\": []}\n",
    "\n",
    "for res in resolutions:\n",
    "    x = torch.randn(batch_size, channels, res, res)\n",
    "    \n",
    "    # JeongWonBlock (deploy)\n",
    "    block = JeongWonBlock(channels, channels, kernel_size=7, num_basis=8)\n",
    "    block.switch_to_deploy()\n",
    "    mean, _ = measure_latency_gpu(block, x)\n",
    "    results[\"JeongWon (deploy)\"].append(mean)\n",
    "    \n",
    "    # ConvNeXt\n",
    "    block = ConvNeXtBlock(channels, expansion=4, kernel_size=7)\n",
    "    mean, _ = measure_latency_gpu(block, x)\n",
    "    results[\"ConvNeXt\"].append(mean)\n",
    "    \n",
    "    # ConvNeXtV2\n",
    "    block = ConvNeXtBlockV2(channels, expansion=4, kernel_size=7)\n",
    "    mean, _ = measure_latency_gpu(block, x)\n",
    "    results[\"ConvNeXtV2\"].append(mean)\n",
    "\n",
    "# Print results\n",
    "print(f\"\\n{'Resolution':<12}\", end=\"\")\n",
    "for name in results.keys():\n",
    "    print(f\"{name:<20}\", end=\"\")\n",
    "print()\n",
    "print(\"-\" * 72)\n",
    "\n",
    "for i, res in enumerate(resolutions):\n",
    "    print(f\"{res}x{res:<10}\", end=\"\")\n",
    "    for name in results.keys():\n",
    "        print(f\"{results[name][i]:.3f} ms{'':<12}\", end=\"\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Multi-Channel Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T03:27:56.346675Z",
     "iopub.status.busy": "2026-02-19T03:27:56.346569Z",
     "iopub.status.idle": "2026-02-19T03:27:57.141125Z",
     "shell.execute_reply": "2026-02-19T03:27:57.140874Z",
     "shell.execute_reply.started": "2026-02-19T03:27:56.346667Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Multi-Channel GPU Latency Comparison (64x64 resolution)\n",
      "============================================================\n",
      "\n",
      "Channels    JeongWon (deploy)   ConvNeXt            ConvNeXtV2          \n",
      "------------------------------------------------------------------------\n",
      "24          0.042 ms            0.072 ms            0.104 ms            \n",
      "48          0.047 ms            0.098 ms            0.154 ms            \n",
      "64          0.060 ms            0.116 ms            0.185 ms            \n",
      "96          0.067 ms            0.168 ms            0.269 ms            \n",
      "128         0.083 ms            0.215 ms            0.339 ms            \n",
      "192         0.107 ms            0.328 ms            0.501 ms            \n"
     ]
    }
   ],
   "source": [
    "channel_list = [24, 48, 64, 96, 128, 192]\n",
    "resolution = 64\n",
    "batch_size = 1\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Multi-Channel GPU Latency Comparison (64x64 resolution)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results = {\"JeongWon (deploy)\": [], \"ConvNeXt\": [], \"ConvNeXtV2\": []}\n",
    "\n",
    "for ch in channel_list:\n",
    "    x = torch.randn(batch_size, ch, resolution, resolution)\n",
    "    \n",
    "    # JeongWonBlock (deploy)\n",
    "    block = JeongWonBlock(ch, ch, kernel_size=7, num_basis=8)\n",
    "    block.switch_to_deploy()\n",
    "    mean, _ = measure_latency_gpu(block, x)\n",
    "    results[\"JeongWon (deploy)\"].append(mean)\n",
    "    \n",
    "    # ConvNeXt\n",
    "    block = ConvNeXtBlock(ch, expansion=4, kernel_size=7)\n",
    "    mean, _ = measure_latency_gpu(block, x)\n",
    "    results[\"ConvNeXt\"].append(mean)\n",
    "    \n",
    "    # ConvNeXtV2\n",
    "    block = ConvNeXtBlockV2(ch, expansion=4, kernel_size=7)\n",
    "    mean, _ = measure_latency_gpu(block, x)\n",
    "    results[\"ConvNeXtV2\"].append(mean)\n",
    "\n",
    "# Print results\n",
    "print(f\"\\n{'Channels':<12}\", end=\"\")\n",
    "for name in results.keys():\n",
    "    print(f\"{name:<20}\", end=\"\")\n",
    "print()\n",
    "print(\"-\" * 72)\n",
    "\n",
    "for i, ch in enumerate(channel_list):\n",
    "    print(f\"{ch:<12}\", end=\"\")\n",
    "    for name in results.keys():\n",
    "        print(f\"{results[name][i]:.3f} ms{'':<12}\", end=\"\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Full Model Comparison (Stacked Blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T03:27:57.141510Z",
     "iopub.status.busy": "2026-02-19T03:27:57.141418Z",
     "iopub.status.idle": "2026-02-19T03:27:57.869030Z",
     "shell.execute_reply": "2026-02-19T03:27:57.868656Z",
     "shell.execute_reply.started": "2026-02-19T03:27:57.141502Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Full Encoder Comparison (6 stages)\n",
      "============================================================\n",
      "\n",
      "JeongWon Encoder Params: 87,048\n",
      "  GPU Latency (train):  0.889 ± 0.011 ms\n",
      "  GPU Latency (deploy): 0.498 ± 0.003 ms\n",
      "\n",
      "ConvNeXt Encoder Params: 777,112\n",
      "  GPU Latency: 1.267 ± 0.009 ms\n"
     ]
    }
   ],
   "source": [
    "class StackedJeongWonBlocks(nn.Module):\n",
    "    def __init__(self, c_list=[24, 48, 64, 96, 128, 192]):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.ModuleList()\n",
    "        in_ch = 3\n",
    "        for out_ch in c_list:\n",
    "            self.blocks.append(JeongWonBlock(in_ch, out_ch, kernel_size=7, num_basis=8))\n",
    "            in_ch = out_ch\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for block in self.blocks:\n",
    "            x = F.max_pool2d(block(x), 2)\n",
    "        return x\n",
    "    \n",
    "    def switch_to_deploy(self):\n",
    "        for block in self.blocks:\n",
    "            block.switch_to_deploy()\n",
    "\n",
    "\n",
    "class StackedConvNeXtBlocks(nn.Module):\n",
    "    def __init__(self, c_list=[24, 48, 64, 96, 128, 192]):\n",
    "        super().__init__()\n",
    "        self.stem = nn.Conv2d(3, c_list[0], 3, padding=1)\n",
    "        self.blocks = nn.ModuleList()\n",
    "        for i, ch in enumerate(c_list):\n",
    "            self.blocks.append(ConvNeXtBlock(ch, expansion=4, kernel_size=7))\n",
    "            if i < len(c_list) - 1:\n",
    "                self.blocks.append(nn.Conv2d(ch, c_list[i+1], 2, stride=2))  # Downsample\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Compare\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Full Encoder Comparison (6 stages)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "x = torch.randn(1, 3, 256, 256)\n",
    "\n",
    "# JeongWon Encoder\n",
    "encoder_jw = StackedJeongWonBlocks()\n",
    "print(f\"\\nJeongWon Encoder Params: {count_params(encoder_jw):,}\")\n",
    "mean, std = measure_latency_gpu(encoder_jw, x)\n",
    "if mean: print(f\"  GPU Latency (train):  {mean:.3f} ± {std:.3f} ms\")\n",
    "\n",
    "encoder_jw_deploy = StackedJeongWonBlocks()\n",
    "encoder_jw_deploy.switch_to_deploy()\n",
    "mean, std = measure_latency_gpu(encoder_jw_deploy, x)\n",
    "if mean: print(f\"  GPU Latency (deploy): {mean:.3f} ± {std:.3f} ms\")\n",
    "\n",
    "# ConvNeXt Encoder\n",
    "encoder_cn = StackedConvNeXtBlocks()\n",
    "print(f\"\\nConvNeXt Encoder Params: {count_params(encoder_cn):,}\")\n",
    "mean, std = measure_latency_gpu(encoder_cn, x)\n",
    "if mean: print(f\"  GPU Latency: {mean:.3f} ± {std:.3f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T03:27:57.869647Z",
     "iopub.status.busy": "2026-02-19T03:27:57.869426Z",
     "iopub.status.idle": "2026-02-19T03:27:57.872170Z",
     "shell.execute_reply": "2026-02-19T03:27:57.871924Z",
     "shell.execute_reply.started": "2026-02-19T03:27:57.869630Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "SUMMARY\n",
      "============================================================\n",
      "\n",
      "JeongWonBlock:\n",
      "  - Structure: 1x1 Conv + RepConv 7x7 DW + PRCM\n",
      "  - Pros: Lightweight, Re-parameterizable (deploy mode faster)\n",
      "  - PRCM: Global context with minimal overhead\n",
      "\n",
      "ConvNeXtBlock:\n",
      "  - Structure: DW 7x7 -> LN -> 1x1 (expand) -> GELU -> 1x1 (project)\n",
      "  - Pros: Strong representational power\n",
      "  - Cons: More parameters (4x expansion), LayerNorm overhead\n",
      "\n",
      "ConvNeXtBlockV2:\n",
      "  - Adds GRN (Global Response Normalization)\n",
      "  - Additional overhead from GRN computation\n",
      "\n",
      "Key Insights:\n",
      "  1. JeongWon deploy mode fuses RepConv branches -> faster inference\n",
      "  2. ConvNeXt has 4x channel expansion -> more FLOPs\n",
      "  3. PRCM is lightweight compared to full expansion+projection\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\"\"\n",
    "JeongWonBlock:\n",
    "  - Structure: 1x1 Conv + RepConv 7x7 DW + PRCM\n",
    "  - Pros: Lightweight, Re-parameterizable (deploy mode faster)\n",
    "  - PRCM: Global context with minimal overhead\n",
    "\n",
    "ConvNeXtBlock:\n",
    "  - Structure: DW 7x7 -> LN -> 1x1 (expand) -> GELU -> 1x1 (project)\n",
    "  - Pros: Strong representational power\n",
    "  - Cons: More parameters (4x expansion), LayerNorm overhead\n",
    "\n",
    "ConvNeXtBlockV2:\n",
    "  - Adds GRN (Global Response Normalization)\n",
    "  - Additional overhead from GRN computation\n",
    "\n",
    "Key Insights:\n",
    "  1. JeongWon deploy mode fuses RepConv branches -> faster inference\n",
    "  2. ConvNeXt has 4x channel expansion -> more FLOPs\n",
    "  3. PRCM is lightweight compared to full expansion+projection\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Skin_Lesion_Seg (Python 3.8)",
   "language": "python",
   "name": "skin_lesion_seg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
