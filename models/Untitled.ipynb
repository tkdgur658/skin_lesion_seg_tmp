{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfcf0c17-59e9-4151-8f47-23fd109e198d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-18T14:15:15.931401Z",
     "iopub.status.busy": "2026-02-18T14:15:15.930874Z",
     "iopub.status.idle": "2026-02-18T14:15:16.063038Z",
     "shell.execute_reply": "2026-02-18T14:15:16.062706Z",
     "shell.execute_reply.started": "2026-02-18T14:15:15.931359Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 416\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    415\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[0;32m--> 416\u001b[0m \u001b[43mJeongWonNet_MS_SP\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrand\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m224\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m224\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "# The code will release soon!!\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from timm.layers import trunc_normal_\n",
    "class RepConv(nn.Module):\n",
    "    \"\"\"\n",
    "    Re-parameterizable Convolution Block\n",
    "    훈련: Conv + BN + Identity(or 1x1) branchF.interpolate\n",
    "    추론: 단일 3x3 Conv로 융합\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3,\n",
    "                 stride=1, padding=1, groups=1, use_identity=True, use_activation=True):\n",
    "        super(RepConv, self).__init__()\n",
    "       \n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.groups = groups\n",
    "       \n",
    "        # use_identity는 stride=1이고 in/out 채널이 같을 때만 활성화\n",
    "        self.use_identity = use_identity and (stride == 1) and (in_channels == out_channels)\n",
    "       \n",
    "        # 주 Branch: kernel_size Conv + BN\n",
    "        self.conv_kxk = nn.Conv2d(in_channels, out_channels, kernel_size,\n",
    "                                  stride, padding, groups=groups, bias=False)\n",
    "        self.bn_kxk = nn.BatchNorm2d(out_channels)\n",
    "       \n",
    "        # 1x1 Branch (더 많은 표현력)\n",
    "        if kernel_size > 1:\n",
    "            self.conv_1x1 = nn.Conv2d(in_channels, out_channels, 1,\n",
    "                                      stride, 0, groups=groups, bias=False)\n",
    "            self.bn_1x1 = nn.BatchNorm2d(out_channels)\n",
    "        else:\n",
    "            self.conv_1x1 = None\n",
    "       \n",
    "        # Identity Branch (residual connection)\n",
    "        if self.use_identity:\n",
    "            self.bn_identity = nn.BatchNorm2d(out_channels)\n",
    "       \n",
    "        # 활성화 함수 선택적 적용\n",
    "        if use_activation:\n",
    "            self.activation = nn.ReLU(inplace=True)\n",
    "        else:\n",
    "            self.activation = nn.Identity()\n",
    "           \n",
    "    def forward(self, x):\n",
    "        if hasattr(self, 'fused_conv'):\n",
    "            # 추론 모드: 융합된 단일 Conv만 사용\n",
    "            return self.activation(self.fused_conv(x))\n",
    "       \n",
    "        # 훈련 모드: 모든 branch 합산\n",
    "        out = self.bn_kxk(self.conv_kxk(x))\n",
    "       \n",
    "        if self.conv_1x1 is not None:\n",
    "            out += self.bn_1x1(self.conv_1x1(x))\n",
    "       \n",
    "        if self.use_identity:\n",
    "            out += self.bn_identity(x)\n",
    "       \n",
    "        return self.activation(out)\n",
    "   \n",
    "    def switch_to_deploy(self):\n",
    "        \"\"\"추론 모드로 전환: 모든 branch를 단일 Conv로 융합\"\"\"\n",
    "        if hasattr(self, 'fused_conv'):\n",
    "            return\n",
    "       \n",
    "        # 각 branch의 weight와 bias를 추출하여 합산\n",
    "        kernel, bias = self._fuse_bn_tensor(self.conv_kxk, self.bn_kxk)\n",
    "       \n",
    "        if self.conv_1x1 is not None:\n",
    "            kernel_1x1, bias_1x1 = self._fuse_bn_tensor(self.conv_1x1, self.bn_1x1)\n",
    "            # 1x1을 kxk로 패딩\n",
    "            kernel += self._pad_1x1_to_kxk(kernel_1x1)\n",
    "            bias += bias_1x1\n",
    "       \n",
    "        if self.use_identity:\n",
    "            kernel_identity, bias_identity = self._fuse_bn_tensor(None, self.bn_identity)\n",
    "            kernel += kernel_identity\n",
    "            bias += bias_identity\n",
    "       \n",
    "        # 융합된 Conv 생성\n",
    "        self.fused_conv = nn.Conv2d(\n",
    "            self.in_channels, self.out_channels, self.kernel_size,\n",
    "            self.stride, self.padding, groups=self.groups, bias=True\n",
    "        )\n",
    "        self.fused_conv.weight.data = kernel\n",
    "        self.fused_conv.bias.data = bias\n",
    "       \n",
    "        # 훈련용 레이어 제거 (메모리 절약)\n",
    "        self.__delattr__('conv_kxk')\n",
    "        self.__delattr__('bn_kxk')\n",
    "        if self.conv_1x1 is not None:\n",
    "            self.__delattr__('conv_1x1')\n",
    "            self.__delattr__('bn_1x1')\n",
    "        if hasattr(self, 'bn_identity'):\n",
    "            self.__delattr__('bn_identity')\n",
    "   \n",
    "    def _fuse_bn_tensor(self, conv, bn):\n",
    "        \"\"\"Conv + BN을 융합하여 weight, bias 반환\"\"\"\n",
    "        if conv is None:\n",
    "            # Identity branch\n",
    "            input_dim = self.in_channels // self.groups\n",
    "            kernel_value = torch.zeros((self.in_channels, input_dim,\n",
    "                                        self.kernel_size, self.kernel_size),\n",
    "                                       dtype=bn.weight.dtype, device=bn.weight.device)\n",
    "            for i in range(self.in_channels):\n",
    "                kernel_value[i, i % input_dim,\n",
    "                             self.kernel_size // 2, self.kernel_size // 2] = 1\n",
    "            kernel = kernel_value\n",
    "            running_mean = bn.running_mean\n",
    "            running_var = bn.running_var\n",
    "            gamma = bn.weight\n",
    "            beta = bn.bias\n",
    "            eps = bn.eps\n",
    "        else:\n",
    "            kernel = conv.weight\n",
    "            running_mean = bn.running_mean\n",
    "            running_var = bn.running_var\n",
    "            gamma = bn.weight\n",
    "            beta = bn.bias\n",
    "            eps = bn.eps\n",
    "       \n",
    "        std = torch.sqrt(running_var + eps)\n",
    "        t = (gamma / std).reshape(-1, 1, 1, 1)\n",
    "        return kernel * t, beta - running_mean * gamma / std\n",
    "   \n",
    "    def _pad_1x1_to_kxk(self, kernel_1x1):\n",
    "        \"\"\"1x1 kernel을 kxk로 패딩\"\"\"\n",
    "        if self.kernel_size == 1:\n",
    "            return kernel_1x1\n",
    "        else:\n",
    "            pad = self.kernel_size // 2\n",
    "            return F.pad(kernel_1x1, [pad, pad, pad, pad])\n",
    "class MSRepConv(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Scale Re-parameterizable Conv\n",
    "    훈련: 3x3 + 5x5 + 1x1 + Identity\n",
    "    추론: 단일 5x5 Conv로 융합\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_sizes=[1, 3, 5],\n",
    "                 stride=1, groups=1, use_identity=True, use_activation=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.max_kernel = max(kernel_sizes)\n",
    "        self.stride = stride\n",
    "        self.padding = self.max_kernel // 2\n",
    "        self.groups = groups\n",
    "        self.kernel_sizes = sorted(kernel_sizes, reverse=True)\n",
    "        \n",
    "        self.use_identity = use_identity and (stride == 1) and (in_channels == out_channels)\n",
    "        \n",
    "        # 각 스케일별 Conv + BN\n",
    "        self.branches = nn.ModuleList()\n",
    "        for ks in self.kernel_sizes:\n",
    "            pad = ks // 2\n",
    "            conv = nn.Conv2d(in_channels, out_channels, ks, \n",
    "                           stride, pad, groups=groups, bias=False)\n",
    "            bn = nn.BatchNorm2d(out_channels)\n",
    "            self.branches.append(nn.ModuleDict({'conv': conv, 'bn': bn}))\n",
    "        \n",
    "        # Identity branch\n",
    "        if self.use_identity:\n",
    "            self.bn_identity = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        self.activation = nn.ReLU(inplace=True) if use_activation else nn.Identity()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if hasattr(self, 'fused_conv'):\n",
    "            return self.activation(self.fused_conv(x))\n",
    "        \n",
    "        # 모든 스케일 합산\n",
    "        out = 0\n",
    "        for branch in self.branches:\n",
    "            out = out + branch['bn'](branch['conv'](x))\n",
    "        \n",
    "        if self.use_identity:\n",
    "            out = out + self.bn_identity(x)\n",
    "        \n",
    "        return self.activation(out)\n",
    "    \n",
    "    def switch_to_deploy(self):\n",
    "        if hasattr(self, 'fused_conv'):\n",
    "            return\n",
    "        \n",
    "        # 모든 branch를 최대 커널 사이즈로 융합\n",
    "        kernel = 0\n",
    "        bias = 0\n",
    "        \n",
    "        for branch in self.branches:\n",
    "            k, b = self._fuse_bn_tensor(branch['conv'], branch['bn'])\n",
    "            kernel = kernel + self._pad_to_max_kernel(k, branch['conv'].kernel_size[0])\n",
    "            bias = bias + b\n",
    "        \n",
    "        if self.use_identity:\n",
    "            k_id, b_id = self._fuse_bn_tensor(None, self.bn_identity)\n",
    "            kernel = kernel + k_id\n",
    "            bias = bias + b_id\n",
    "        \n",
    "        self.fused_conv = nn.Conv2d(\n",
    "            self.in_channels, self.out_channels, self.max_kernel,\n",
    "            self.stride, self.padding, groups=self.groups, bias=True\n",
    "        )\n",
    "        self.fused_conv.weight.data = kernel\n",
    "        self.fused_conv.bias.data = bias\n",
    "        \n",
    "        # 메모리 정리\n",
    "        self.__delattr__('branches')\n",
    "        if hasattr(self, 'bn_identity'):\n",
    "            self.__delattr__('bn_identity')\n",
    "    \n",
    "    def _fuse_bn_tensor(self, conv, bn):\n",
    "        if conv is None:\n",
    "            # Identity\n",
    "            input_dim = self.in_channels // self.groups\n",
    "            kernel_value = torch.zeros((self.in_channels, input_dim,\n",
    "                                       self.max_kernel, self.max_kernel),\n",
    "                                      dtype=bn.weight.dtype, device=bn.weight.device)\n",
    "            for i in range(self.in_channels):\n",
    "                kernel_value[i, i % input_dim, \n",
    "                           self.max_kernel // 2, self.max_kernel // 2] = 1\n",
    "            kernel = kernel_value\n",
    "        else:\n",
    "            kernel = conv.weight\n",
    "        \n",
    "        running_mean = bn.running_mean\n",
    "        running_var = bn.running_var\n",
    "        gamma = bn.weight\n",
    "        beta = bn.bias\n",
    "        eps = bn.eps\n",
    "        \n",
    "        std = torch.sqrt(running_var + eps)\n",
    "        t = (gamma / std).reshape(-1, 1, 1, 1)\n",
    "        return kernel * t, beta - running_mean * gamma / std\n",
    "    \n",
    "    def _pad_to_max_kernel(self, kernel, current_size):\n",
    "        if current_size == self.max_kernel:\n",
    "            return kernel\n",
    "        pad = (self.max_kernel - current_size) // 2\n",
    "        return F.pad(kernel, [pad, pad, pad, pad])\n",
    "        \n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, in_channels, reduction=8, dropout_rate=0.2):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "        intermediate_channels = in_channels // reduction\n",
    "\n",
    "        self.conv1_rep = RepConv(in_channels, intermediate_channels, 7, 1, 3, \n",
    "                                use_identity=True, use_activation=True)\n",
    "        \n",
    "        self.dropout_1 = nn.Dropout2d(dropout_rate)\n",
    "        self.dropout_2 = nn.Dropout2d(dropout_rate)\n",
    "        \n",
    "        # Attention map generation\n",
    "        self.conv2 = nn.Conv2d(intermediate_channels, 1, 3, 1, 1, bias=False)\n",
    "        self.act2 = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x_att = self.conv1_rep(x)\n",
    "        x_mod = x_att \n",
    "        x_mod = self.dropout_1(x_mod)  \n",
    "        \n",
    "        attention_map = self.conv2(x_mod)\n",
    "        attention_map = self.act2(attention_map)\n",
    "        output = x * attention_map\n",
    "        output = self.dropout_2(output)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def switch_to_deploy(self):\n",
    "        \"\"\"GSA 내부의 RepConv를 융합\"\"\"\n",
    "        self.conv1_rep.switch_to_deploy()\n",
    "class PRCM(nn.Module):\n",
    "    \"\"\"\n",
    "    Parametric Resonance Channel Mixer (PRCM)\n",
    "    - 각 채널별 글로벌 컨텍스트를 학습 가능한 low-rank basis로 분석\n",
    "    - basis coefficient를 통해 채널 가중치를 생성해 중요 표현 강조\n",
    "    \"\"\"\n",
    "    def __init__(self, channels, num_basis=2):\n",
    "        super().__init__()\n",
    "        self.num_basis = num_basis\n",
    "        # 학습 가능한 basis 벡터: [num_basis, channels]\n",
    "        self.basis = nn.Parameter(torch.randn(num_basis, channels))\n",
    "        # low-rank coefficient → channel 가중치 맵핑\n",
    "        self.fuser = nn.Linear(num_basis, channels, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        # 채널별 전역 평균 컨텍스트: [B, C]\n",
    "        ctx = x.mean(dim=[2, 3])\n",
    "        # basis 투영: [B, num_basis]\n",
    "        coeff = ctx @ self.basis.t()\n",
    "        # 채널별 가중치 생성 및 sigmoid: [B, C, 1, 1]\n",
    "        w = self.fuser(coeff).sigmoid().unsqueeze(-1).unsqueeze(-1)\n",
    "        return x * w\n",
    "\n",
    "\n",
    "class JeongWonNet_MS_SP(nn.Module):\n",
    "    \"\"\"\n",
    "    Simplified Hybrid Depthwise Separable UNet with PRCM:\n",
    "    - UCMNet 스타일의 단순한 skip connection (decoder + encoder)\n",
    "    - Bridge connection 제거로 명확한 구조\n",
    "    - 1×1 Pointwise Conv (채널 조정)\n",
    "    - 3×3 Depthwise Conv (groups=out_ch)\n",
    "    - GroupNorm + GELU\n",
    "    - PRCM으로 채널 간 중요 패턴 강조\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=1, input_channels=3, c_list=[24, 48, 64, 96, 128, 192], gt_ds=True):\n",
    "        super().__init__()\n",
    "        self.gt_ds = gt_ds\n",
    "\n",
    "        def make_dw_block(in_ch, out_ch):\n",
    "            layers = []\n",
    "            # 채널 조정이 필요할 때만 1×1 pointwise conv\n",
    "            if in_ch != out_ch:\n",
    "                layers.append(nn.Conv2d(in_ch, out_ch, kernel_size=1, bias=False))\n",
    "            # Depthwise convolution\n",
    "            layers.append(MSRepConv(out_ch, out_ch, kernel_sizes=[1, 3, 5],  groups=out_ch))\n",
    "            layers.append(SpatialAttention(out_ch))\n",
    "            layers.append(PRCM(out_ch, num_basis=8))\n",
    "            return nn.Sequential(*layers)\n",
    "\n",
    "        # Encoder blocks\n",
    "        self.encoder1 = make_dw_block(input_channels, c_list[0])\n",
    "        self.encoder2 = make_dw_block(c_list[0], c_list[1])\n",
    "        self.encoder3 = make_dw_block(c_list[1], c_list[2])\n",
    "        self.encoder4 = make_dw_block(c_list[2], c_list[3])\n",
    "        self.encoder5 = make_dw_block(c_list[3], c_list[4])\n",
    "        self.encoder6 = make_dw_block(c_list[4], c_list[5])\n",
    "\n",
    "        # Deep Supervision heads\n",
    "        if gt_ds:\n",
    "            self.gt_conv1 = nn.Conv2d(c_list[4], num_classes, 1)\n",
    "            self.gt_conv2 = nn.Conv2d(c_list[3], num_classes, 1)\n",
    "            self.gt_conv3 = nn.Conv2d(c_list[2], num_classes, 1)\n",
    "            self.gt_conv4 = nn.Conv2d(c_list[1], num_classes, 1)\n",
    "            self.gt_conv5 = nn.Conv2d(c_list[0], num_classes, 1)\n",
    "\n",
    "        # Decoder blocks\n",
    "        self.decoder1 = make_dw_block(c_list[5], c_list[4])\n",
    "        self.decoder2 = make_dw_block(c_list[4], c_list[3])\n",
    "        self.decoder3 = make_dw_block(c_list[3], c_list[2])\n",
    "        self.decoder4 = make_dw_block(c_list[2], c_list[1])\n",
    "        self.decoder5 = make_dw_block(c_list[1], c_list[0])\n",
    "\n",
    "        # Final 1x1 conv to num_classes\n",
    "        self.final = nn.Conv2d(c_list[0], num_classes, kernel_size=1)\n",
    "\n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "            fan_out //= m.groups\n",
    "            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.zero_()\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        is_eval = not self.training\n",
    "        \n",
    "        # === Encoder forward ===\n",
    "        e1 = F.max_pool2d(self.encoder1(x), 2)      # [B, c0, H/2, W/2]\n",
    "        e2 = F.max_pool2d(self.encoder2(e1), 2)     # [B, c1, H/4, W/4]\n",
    "        e3 = F.max_pool2d(self.encoder3(e2), 2)     # [B, c2, H/8, W/8]\n",
    "        e4 = F.max_pool2d(self.encoder4(e3), 2)     # [B, c3, H/16, W/16]\n",
    "        e5 = F.max_pool2d(self.encoder5(e4), 2)     # [B, c4, H/32, W/32]\n",
    "        e6 = self.encoder6(e5)                      # [B, c5, H/32, W/32]\n",
    "\n",
    "        # === Decoder forward with simple skip connections ===\n",
    "        # d5: 가장 깊은 레벨, e5와 같은 해상도\n",
    "        d5 = self.decoder1(e6)                      # [B, c4, H/32, W/32]\n",
    "        d5 = d5 + e5                                # Simple skip connection\n",
    "\n",
    "        # d4: upsampling 후 e4와 더함\n",
    "        d4 = F.interpolate(self.decoder2(d5), scale_factor=2, mode='bilinear', align_corners=True)  # [B, c3, H/16, W/16]\n",
    "        d4 = d4 + e4                                # Simple skip connection\n",
    "\n",
    "        # d3: upsampling 후 e3와 더함\n",
    "        d3 = F.interpolate(self.decoder3(d4), scale_factor=2, mode='bilinear', align_corners=True)  # [B, c2, H/8, W/8]\n",
    "        d3 = d3 + e3                                # Simple skip connection\n",
    "\n",
    "        # d2: upsampling 후 e2와 더함\n",
    "        d2 = F.interpolate(self.decoder4(d3), scale_factor=2, mode='bilinear', align_corners=True)  # [B, c1, H/4, W/4]\n",
    "        d2 = d2 + e2                                # Simple skip connection\n",
    "\n",
    "        # d1: upsampling 후 e1과 더함\n",
    "        d1 = F.interpolate(self.decoder5(d2), scale_factor=2, mode='bilinear', align_corners=True)  # [B, c0, H/2, W/2]\n",
    "        d1 = d1 + e1                                # Simple skip connection\n",
    "\n",
    "        # Final segmentation map\n",
    "        out = F.interpolate(self.final(d1), scale_factor=2, mode='bilinear', align_corners=True)    # [B, num_classes, H, W]\n",
    "\n",
    "        # Return deep supervision outputs if training\n",
    "        if self.gt_ds and not is_eval:\n",
    "            h, w = x.shape[2], x.shape[3]\n",
    "            return (\n",
    "                F.interpolate(self.gt_conv1(d5), (h, w), mode='bilinear', align_corners=True),\n",
    "                F.interpolate(self.gt_conv2(d4), (h, w), mode='bilinear', align_corners=True),\n",
    "                F.interpolate(self.gt_conv3(d3), (h, w), mode='bilinear', align_corners=True),\n",
    "                F.interpolate(self.gt_conv4(d2), (h, w), mode='bilinear', align_corners=True),\n",
    "                F.interpolate(self.gt_conv5(d1), (h, w), mode='bilinear', align_corners=True)\n",
    "            ), out\n",
    "        else:\n",
    "            return out\n",
    "JeongWonNet_MS_SP()(torch.rand((5,3,224,224)))[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a504ea9d-a26c-4e39-86c1-520697dfa407",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb35261-2fd2-40f9-8699-6d092a34af08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Skin_Lesion_Seg (Python 3.8)",
   "language": "python",
   "name": "skin_lesion_seg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
