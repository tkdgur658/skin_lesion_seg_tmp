{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": "# PRCM Expansion Ablation Study\n## GAP 이후 Expansion 전략 비교\n\nComparing:\n\n### Baseline (Traditional Conv Block)\n0. **InvertedBottleneck** - Direct Transform (MobileNetV2 style)\n   - Structure: `7x7 DW Conv -> BN -> ReLU -> 1x1 Expand -> BN -> ReLU -> 1x1 Project -> BN`\n   - 특징: 공간 정보를 직접 처리하는 전통적인 convolution 방식\n\n### PRCM Variants (Channel Recalibration)\n1. **PRCM (baseline)** - Modulator\n   - Structure: `GAP -> basis projection -> fuser -> sigmoid -> x * w`\n   - 특징: Low-rank basis를 사용한 채널 재조정\n\n2. **PRCM_SE** - Modulator\n   - Structure: `GAP -> expand(C*r) -> ReLU -> shrink(C) -> sigmoid -> x * w`\n   - 특징: SE-style expansion, HW=1이라 expansion 늘려도 연산량 적음\n\n3. **PRCM_AdaptivePool** - **NOT a Modulator** (Direct Transform)\n   - Structure: `AdaptivePool(NxN) -> 1x1 Conv expand -> ReLU -> 1x1 Conv shrink -> Upsample -> out`\n   - 특징: \n     - 공간 정보 유지 (1x1, 2x2, 4x4, 8x8, 16x16, 32x32 등)\n     - 1x1 Conv 사용 → **pool_size와 무관하게 파라미터 동일**\n     - 직접 feature 변환 (NOT modulator)\n\n4. **PRCM_BasisExpand** - Modulator\n   - Structure: `GAP -> basis(C->B) -> expand(B*r) -> ReLU -> shrink(C) -> sigmoid -> x * w`\n   - 특징: Basis로 먼저 압축 후 expansion, 파라미터 효율적\n\n5. **PRCM_SE_Affine** - Modulator (scale + shift)\n   - Structure: `GAP -> expand -> ReLU -> [scale_proj, shift_proj] -> x * alpha + beta`\n   - 특징: SE + Affine transform (scale과 shift 모두 적용)"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T04:50:57.274679Z",
     "iopub.status.busy": "2026-02-19T04:50:57.274092Z",
     "iopub.status.idle": "2026-02-19T04:50:57.851327Z",
     "shell.execute_reply": "2026-02-19T04:50:57.851062Z",
     "shell.execute_reply.started": "2026-02-19T04:50:57.274636Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import numpy as np\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## 1. PRCM Variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T04:50:57.852011Z",
     "iopub.status.busy": "2026-02-19T04:50:57.851892Z",
     "iopub.status.idle": "2026-02-19T04:50:57.855050Z",
     "shell.execute_reply": "2026-02-19T04:50:57.854828Z",
     "shell.execute_reply.started": "2026-02-19T04:50:57.852002Z"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# PRCM (Baseline)\n",
    "# ============================================\n",
    "\n",
    "class PRCM(nn.Module):\n",
    "    \"\"\"Pattern Recalibration Module (Baseline)\n",
    "    \n",
    "    Structure: GAP -> basis projection -> fuser -> sigmoid\n",
    "    \"\"\"\n",
    "    def __init__(self, channels, num_basis=8, dropout_rate=0.5):\n",
    "        super().__init__()\n",
    "        self.basis = nn.Parameter(torch.randn(num_basis, channels))\n",
    "        self.fuser = nn.Linear(num_basis, channels, bias=False)\n",
    "        self.coeff_dropout = nn.Dropout(dropout_rate) if dropout_rate > 0 else nn.Identity()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        ctx = x.mean(dim=[2, 3])  # [B, C]\n",
    "        coeff = ctx @ self.basis.t()  # [B, num_basis]\n",
    "        coeff = self.coeff_dropout(coeff)\n",
    "        w = self.fuser(coeff).sigmoid().unsqueeze(-1).unsqueeze(-1)\n",
    "        return x * w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T04:50:57.855367Z",
     "iopub.status.busy": "2026-02-19T04:50:57.855297Z",
     "iopub.status.idle": "2026-02-19T04:50:57.865060Z",
     "shell.execute_reply": "2026-02-19T04:50:57.864866Z",
     "shell.execute_reply.started": "2026-02-19T04:50:57.855360Z"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# PRCM_SE (SE-style Expansion)\n",
    "# ============================================\n",
    "\n",
    "class PRCM_SE(nn.Module):\n",
    "    \"\"\"PRCM with SE-style expansion\n",
    "    \n",
    "    Structure: GAP -> expand(C*r) -> ReLU -> shrink(C) -> sigmoid\n",
    "    \n",
    "    GAP 이후 HW=1이므로 expansion ratio를 크게 해도 연산량 적음\n",
    "    \"\"\"\n",
    "    def __init__(self, channels, expansion=4, dropout_rate=0.5):\n",
    "        super().__init__()\n",
    "        hidden = channels * expansion\n",
    "        \n",
    "        self.expand = nn.Linear(channels, hidden, bias=False)\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "        self.shrink = nn.Linear(hidden, channels, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout_rate) if dropout_rate > 0 else nn.Identity()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        ctx = x.mean(dim=[2, 3])  # [B, C]\n",
    "        ctx = self.expand(ctx)    # [B, C*r]\n",
    "        ctx = self.act(ctx)\n",
    "        ctx = self.dropout(ctx)\n",
    "        w = self.shrink(ctx).sigmoid().unsqueeze(-1).unsqueeze(-1)  # [B, C, 1, 1]\n",
    "        return x * w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T04:50:57.865384Z",
     "iopub.status.busy": "2026-02-19T04:50:57.865285Z",
     "iopub.status.idle": "2026-02-19T04:50:57.868134Z",
     "shell.execute_reply": "2026-02-19T04:50:57.867935Z",
     "shell.execute_reply.started": "2026-02-19T04:50:57.865376Z"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# PRCM_AdaptivePool (Spatial Info Preserved - Direct Transform)\n",
    "# ============================================\n",
    "\n",
    "class PRCM_AdaptivePool(nn.Module):\n",
    "    \"\"\"PRCM with Adaptive Pooling (preserve spatial info)\n",
    "    \n",
    "    Structure: AdaptivePool(pool_size) -> 1x1 Conv expand -> ReLU -> 1x1 Conv shrink -> upsample\n",
    "    \n",
    "    GAP 대신 고정 크기 pooling으로 공간 정보 일부 유지\n",
    "    1x1 Conv 사용 (flatten 대신)\n",
    "    Modulator가 아님 - 직접 feature 변환\n",
    "    \"\"\"\n",
    "    def __init__(self, channels, pool_size=2, expansion=2, dropout_rate=0.5):\n",
    "        super().__init__()\n",
    "        self.pool_size = pool_size\n",
    "        hidden = channels * expansion\n",
    "        \n",
    "        # 1x1 Conv (flatten 대신)\n",
    "        self.conv1 = nn.Conv2d(channels, hidden, 1, bias=False)\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(hidden, channels, 1, bias=False)\n",
    "        self.dropout = nn.Dropout2d(dropout_rate) if dropout_rate > 0 else nn.Identity()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        \n",
    "        # Adaptive pool to fixed size\n",
    "        if H >= self.pool_size and W >= self.pool_size:\n",
    "            out = F.adaptive_avg_pool2d(x, (self.pool_size, self.pool_size))  # [B, C, ps, ps]\n",
    "        else:\n",
    "            # 입력이 pool_size보다 작으면 직접 사용\n",
    "            out = x\n",
    "        \n",
    "        out = self.conv1(out)   # [B, hidden, ps, ps]\n",
    "        out = self.act(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.conv2(out)   # [B, C, ps, ps]\n",
    "        \n",
    "        # Upsample back to original size\n",
    "        out = F.interpolate(out, size=(H, W), mode='bilinear', align_corners=True)\n",
    "        \n",
    "        # NOT a modulator - direct feature transformation\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T04:50:57.868540Z",
     "iopub.status.busy": "2026-02-19T04:50:57.868383Z",
     "iopub.status.idle": "2026-02-19T04:50:57.871019Z",
     "shell.execute_reply": "2026-02-19T04:50:57.870823Z",
     "shell.execute_reply.started": "2026-02-19T04:50:57.868532Z"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# PRCM_BasisExpand (Basis + Expansion)\n",
    "# ============================================\n",
    "\n",
    "class PRCM_BasisExpand(nn.Module):\n",
    "    \"\"\"PRCM with Basis Projection + Expansion\n",
    "    \n",
    "    Structure: GAP -> basis(C->B) -> expand(B->B*r) -> ReLU -> shrink(B*r->C) -> sigmoid\n",
    "    \n",
    "    Basis로 먼저 압축 후 expansion\n",
    "    \"\"\"\n",
    "    def __init__(self, channels, num_basis=8, expansion=4, dropout_rate=0.5):\n",
    "        super().__init__()\n",
    "        self.basis = nn.Parameter(torch.randn(num_basis, channels))\n",
    "        hidden = num_basis * expansion\n",
    "        \n",
    "        self.expand = nn.Linear(num_basis, hidden, bias=False)\n",
    "        self.act = nn.ReLU(inplace=True)\n",
    "        self.shrink = nn.Linear(hidden, channels, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout_rate) if dropout_rate > 0 else nn.Identity()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        ctx = x.mean(dim=[2, 3])  # [B, C]\n",
    "        coeff = ctx @ self.basis.t()  # [B, num_basis]\n",
    "        coeff = self.expand(coeff)  # [B, num_basis * expansion]\n",
    "        coeff = self.act(coeff)\n",
    "        coeff = self.dropout(coeff)\n",
    "        w = self.shrink(coeff).sigmoid().unsqueeze(-1).unsqueeze(-1)\n",
    "        return x * w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T04:50:57.871325Z",
     "iopub.status.busy": "2026-02-19T04:50:57.871248Z",
     "iopub.status.idle": "2026-02-19T04:50:57.873823Z",
     "shell.execute_reply": "2026-02-19T04:50:57.873622Z",
     "shell.execute_reply.started": "2026-02-19T04:50:57.871317Z"
    }
   },
   "outputs": [],
   "source": "# ============================================\n# PRCM_SE_Affine (SE + Shift)\n# ============================================\n\nclass PRCM_SE_Affine(nn.Module):\n    \"\"\"PRCM SE-style with Affine (scale + shift)\n    \n    Structure: GAP -> expand -> ReLU -> [scale_proj, shift_proj] -> affine\n    \"\"\"\n    def __init__(self, channels, expansion=4, dropout_rate=0.5):\n        super().__init__()\n        hidden = channels * expansion\n        \n        self.expand = nn.Linear(channels, hidden, bias=False)\n        self.act = nn.ReLU(inplace=True)\n        self.scale_proj = nn.Linear(hidden, channels, bias=False)\n        self.shift_proj = nn.Linear(hidden, channels, bias=False)\n        self.dropout = nn.Dropout(dropout_rate) if dropout_rate > 0 else nn.Identity()\n    \n    def forward(self, x):\n        ctx = x.mean(dim=[2, 3])\n        ctx = self.expand(ctx)\n        ctx = self.act(ctx)\n        ctx = self.dropout(ctx)\n        \n        alpha = self.scale_proj(ctx).sigmoid().unsqueeze(-1).unsqueeze(-1)\n        beta = self.shift_proj(ctx).unsqueeze(-1).unsqueeze(-1)\n        return x * alpha + beta\n\n\n# ============================================\n# InvertedBottleneck (Baseline Block)\n# ============================================\n\nclass InvertedBottleneck(nn.Module):\n    \"\"\"Inverted Bottleneck Block (MobileNetV2 style baseline)\n    \n    Structure: 7x7 DW Conv -> 1x1 Expand -> ReLU -> 1x1 Project\n    \n    PRCM variants와 비교하기 위한 베이스라인 블록\n    공간 정보를 직접 처리하는 전통적인 convolution 방식\n    \"\"\"\n    def __init__(self, channels, kernel_size=7, expansion=2, dropout_rate=0.5):\n        super().__init__()\n        hidden = channels * expansion\n        \n        # 7x7 Depthwise Conv\n        self.dw_conv = nn.Conv2d(\n            channels, channels, kernel_size, \n            padding=kernel_size // 2, groups=channels, bias=False\n        )\n        self.bn1 = nn.BatchNorm2d(channels)\n        \n        # 1x1 Expand\n        self.expand = nn.Conv2d(channels, hidden, 1, bias=False)\n        self.bn2 = nn.BatchNorm2d(hidden)\n        self.act = nn.ReLU(inplace=True)\n        \n        # 1x1 Project\n        self.project = nn.Conv2d(hidden, channels, 1, bias=False)\n        self.bn3 = nn.BatchNorm2d(channels)\n        \n        self.dropout = nn.Dropout2d(dropout_rate) if dropout_rate > 0 else nn.Identity()\n    \n    def forward(self, x):\n        # 7x7 DW\n        out = self.dw_conv(x)\n        out = self.bn1(out)\n        out = self.act(out)\n        \n        # 1x1 Expand\n        out = self.expand(out)\n        out = self.bn2(out)\n        out = self.act(out)\n        out = self.dropout(out)\n        \n        # 1x1 Project\n        out = self.project(out)\n        out = self.bn3(out)\n        \n        return out\n\n\nclass InvertedBottleneckRes(nn.Module):\n    \"\"\"Inverted Bottleneck with Residual Connection\n    \n    Structure: 7x7 DW Conv -> 1x1 Expand -> ReLU -> 1x1 Project + Residual\n    \"\"\"\n    def __init__(self, channels, kernel_size=7, expansion=2, dropout_rate=0.5):\n        super().__init__()\n        hidden = channels * expansion\n        \n        # 7x7 Depthwise Conv\n        self.dw_conv = nn.Conv2d(\n            channels, channels, kernel_size, \n            padding=kernel_size // 2, groups=channels, bias=False\n        )\n        self.bn1 = nn.BatchNorm2d(channels)\n        \n        # 1x1 Expand\n        self.expand = nn.Conv2d(channels, hidden, 1, bias=False)\n        self.bn2 = nn.BatchNorm2d(hidden)\n        self.act = nn.ReLU(inplace=True)\n        \n        # 1x1 Project\n        self.project = nn.Conv2d(hidden, channels, 1, bias=False)\n        self.bn3 = nn.BatchNorm2d(channels)\n        \n        self.dropout = nn.Dropout2d(dropout_rate) if dropout_rate > 0 else nn.Identity()\n    \n    def forward(self, x):\n        identity = x\n        \n        # 7x7 DW\n        out = self.dw_conv(x)\n        out = self.bn1(out)\n        out = self.act(out)\n        \n        # 1x1 Expand\n        out = self.expand(out)\n        out = self.bn2(out)\n        out = self.act(out)\n        out = self.dropout(out)\n        \n        # 1x1 Project + Residual\n        out = self.project(out)\n        out = self.bn3(out)\n        out = out + identity\n        \n        return out"
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## 2. Measurement Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T04:50:57.874208Z",
     "iopub.status.busy": "2026-02-19T04:50:57.874057Z",
     "iopub.status.idle": "2026-02-19T04:50:57.877062Z",
     "shell.execute_reply": "2026-02-19T04:50:57.876863Z",
     "shell.execute_reply.started": "2026-02-19T04:50:57.874200Z"
    }
   },
   "outputs": [],
   "source": [
    "def count_params(model):\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "def measure_latency_gpu(model, input_tensor, warmup=100, repeat=500):\n",
    "    if not torch.cuda.is_available():\n",
    "        return None, None\n",
    "    \n",
    "    model.eval().cuda()\n",
    "    input_tensor = input_tensor.cuda()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(warmup):\n",
    "            _ = model(input_tensor)\n",
    "            torch.cuda.synchronize()\n",
    "    \n",
    "    times = []\n",
    "    with torch.no_grad():\n",
    "        for _ in range(repeat):\n",
    "            torch.cuda.synchronize()\n",
    "            start = time.perf_counter()\n",
    "            _ = model(input_tensor)\n",
    "            torch.cuda.synchronize()\n",
    "            times.append((time.perf_counter() - start) * 1000)\n",
    "    \n",
    "    return np.mean(times), np.std(times)\n",
    "\n",
    "def measure_latency_cpu(model, input_tensor, warmup=20, repeat=100):\n",
    "    model.eval().cpu()\n",
    "    input_tensor = input_tensor.cpu()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(warmup):\n",
    "            _ = model(input_tensor)\n",
    "    \n",
    "    times = []\n",
    "    with torch.no_grad():\n",
    "        for _ in range(repeat):\n",
    "            start = time.perf_counter()\n",
    "            _ = model(input_tensor)\n",
    "            times.append((time.perf_counter() - start) * 1000)\n",
    "    \n",
    "    return np.mean(times), np.std(times)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## 3. Architecture Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T04:50:57.877444Z",
     "iopub.status.busy": "2026-02-19T04:50:57.877353Z",
     "iopub.status.idle": "2026-02-19T04:50:57.880390Z",
     "shell.execute_reply": "2026-02-19T04:50:57.880213Z",
     "shell.execute_reply.started": "2026-02-19T04:50:57.877437Z"
    }
   },
   "outputs": [],
   "source": "print(\"=\" * 80)\nprint(\"PRCM Variants Architecture\")\nprint(\"=\" * 80)\n\nprint(\"\"\"\n┌─────────────────────────────────────────────────────────────────────────────┐\n│ InvertedBottleneck (Baseline) - Direct Transform                            │\n├─────────────────────────────────────────────────────────────────────────────┤\n│   x [B,C,H,W] -> 7x7 DW Conv -> BN -> ReLU                                  │\n│   -> 1x1 Expand(C*r) -> BN -> ReLU                                          │\n│   -> 1x1 Project(C) -> BN -> out                                            │\n│                                                                             │\n│   Params: C*7*7 + C*C*r + C*r*C + BNs = C*49 + 2*C^2*r + BNs                │\n│   (공간 정보 직접 처리, 전통적인 convolution 방식)                           │\n└─────────────────────────────────────────────────────────────────────────────┘\n\n┌─────────────────────────────────────────────────────────────────────────────┐\n│ PRCM (Baseline) - Modulator                                                 │\n├─────────────────────────────────────────────────────────────────────────────┤\n│   x [B,C,H,W] -> GAP -> ctx [B,C]                                           │\n│   ctx @ basis.T -> coeff [B, num_basis]                                     │\n│   fuser(coeff) -> w [B, C] -> sigmoid -> x * w                              │\n│                                                                             │\n│   Params: num_basis * C + num_basis * C = 2 * num_basis * C                 │\n└─────────────────────────────────────────────────────────────────────────────┘\n\n┌─────────────────────────────────────────────────────────────────────────────┐\n│ PRCM_SE (SE-style Expansion) - Modulator                                    │\n├─────────────────────────────────────────────────────────────────────────────┤\n│   x [B,C,H,W] -> GAP -> ctx [B,C]                                           │\n│   expand(C -> C*r) -> ReLU -> shrink(C*r -> C) -> sigmoid -> x * w          │\n│                                                                             │\n│   Params: C * C*r + C*r * C = 2 * C^2 * r                                   │\n│   (expansion 크게 해도 HW=1이라 연산량 적음)                                  │\n└─────────────────────────────────────────────────────────────────────────────┘\n\n┌─────────────────────────────────────────────────────────────────────────────┐\n│ PRCM_AdaptivePool (Spatial Info) - **NOT a Modulator**                      │\n├─────────────────────────────────────────────────────────────────────────────┤\n│   x [B,C,H,W] -> AdaptivePool(ps) -> [B,C,ps,ps]                            │\n│   1x1 Conv expand -> ReLU -> 1x1 Conv shrink -> Upsample -> out             │\n│                                                                             │\n│   **NOT x * w, direct feature transformation**                              │\n│                                                                             │\n│   Params: C * hidden + hidden * C = 2 * C * C*exp                           │\n│   (공간 정보 유지, 1x1 Conv로 효율적)                                        │\n└─────────────────────────────────────────────────────────────────────────────┘\n\n┌─────────────────────────────────────────────────────────────────────────────┐\n│ PRCM_BasisExpand (Basis + Expansion) - Modulator                            │\n├─────────────────────────────────────────────────────────────────────────────┤\n│   x [B,C,H,W] -> GAP -> ctx [B,C]                                           │\n│   ctx @ basis.T -> coeff [B, num_basis]                                     │\n│   expand(B -> B*r) -> ReLU -> shrink(B*r -> C) -> sigmoid -> x * w          │\n│                                                                             │\n│   Params: num_basis * C + num_basis * B*r + B*r * C                         │\n│   (basis로 압축 후 expansion - 중간 크기)                                    │\n└─────────────────────────────────────────────────────────────────────────────┘\n\n┌─────────────────────────────────────────────────────────────────────────────┐\n│ PRCM_SE_Affine (SE + Shift) - Modulator (scale + shift)                     │\n├─────────────────────────────────────────────────────────────────────────────┤\n│   Same as PRCM_SE but with scale + shift output                             │\n│   -> scale_proj(C), shift_proj(C) -> x * alpha + beta                       │\n│                                                                             │\n│   Params: C * C*r + C*r * C * 2 (scale + shift)                             │\n└─────────────────────────────────────────────────────────────────────────────┘\n\"\"\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## 4. Parameter Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T04:50:57.881210Z",
     "iopub.status.busy": "2026-02-19T04:50:57.881079Z",
     "iopub.status.idle": "2026-02-19T04:50:57.886941Z",
     "shell.execute_reply": "2026-02-19T04:50:57.886725Z",
     "shell.execute_reply.started": "2026-02-19T04:50:57.881203Z"
    }
   },
   "outputs": [],
   "source": "channels = 64\nnum_basis = 8\n\nprint(\"=\" * 80)\nprint(f\"Parameter Comparison (channels={channels})\")\nprint(\"=\" * 80)\n\nvariants = OrderedDict([\n    # Baseline Conv Block\n    (\"InvertedBottleneck (7x7, exp=2)\", InvertedBottleneck(channels, kernel_size=7, expansion=2)),\n    (\"InvertedBottleneck (7x7, exp=4)\", InvertedBottleneck(channels, kernel_size=7, expansion=4)),\n    (\"InvertedBottleneckRes (7x7, exp=2)\", InvertedBottleneckRes(channels, kernel_size=7, expansion=2)),\n    # PRCM variants\n    (\"PRCM (baseline)\", PRCM(channels, num_basis=8)),\n    (\"PRCM_SE (exp=2)\", PRCM_SE(channels, expansion=2)),\n    (\"PRCM_SE (exp=4)\", PRCM_SE(channels, expansion=4)),\n    (\"PRCM_AdaptivePool (2x2, exp=2)\", PRCM_AdaptivePool(channels, pool_size=2, expansion=2)),\n    (\"PRCM_AdaptivePool (4x4, exp=2)\", PRCM_AdaptivePool(channels, pool_size=4, expansion=2)),\n    (\"PRCM_AdaptivePool (8x8, exp=2)\", PRCM_AdaptivePool(channels, pool_size=8, expansion=2)),\n    (\"PRCM_BasisExpand (B=8, exp=4)\", PRCM_BasisExpand(channels, num_basis=8, expansion=4)),\n    (\"PRCM_SE_Affine (exp=4)\", PRCM_SE_Affine(channels, expansion=4)),\n])\n\nbaseline_params = count_params(variants[\"PRCM (baseline)\"])\ninvbottleneck_params = count_params(variants[\"InvertedBottleneck (7x7, exp=2)\"])\n\nprint(f\"\\n{'Variant':<40} {'Params':>12} {'vs PRCM':>12} {'vs InvBN':>12}\")\nprint(\"-\" * 80)\n\nfor name, module in variants.items():\n    params = count_params(module)\n    ratio_prcm = params / baseline_params\n    ratio_inv = params / invbottleneck_params\n    print(f\"{name:<40} {params:>12,} {ratio_prcm:>11.1f}x {ratio_inv:>11.2f}x\")\n\nprint(\"\\n[Note]\")\nprint(\"- InvertedBottleneck: 7x7 DW + 1x1 expand + 1x1 project + BNs\")\nprint(\"- PRCM variants: GAP 이후 처리로 파라미터 효율적\")\nprint(\"- PRCM_AdaptivePool: 1x1 Conv 사용으로 pool_size와 무관하게 파라미터 동일\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## 5. GPU Latency Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T04:50:57.887390Z",
     "iopub.status.busy": "2026-02-19T04:50:57.887209Z",
     "iopub.status.idle": "2026-02-19T04:50:58.293294Z",
     "shell.execute_reply": "2026-02-19T04:50:58.292923Z",
     "shell.execute_reply.started": "2026-02-19T04:50:57.887382Z"
    }
   },
   "outputs": [],
   "source": "channels = 64\nresolution = 64\n\nx = torch.randn(1, channels, resolution, resolution)\n\nprint(\"=\" * 85)\nprint(f\"GPU Latency Comparison (input: 1x{channels}x{resolution}x{resolution})\")\nprint(\"=\" * 85)\n\nvariants = OrderedDict([\n    # Baseline Conv Block\n    (\"InvertedBottleneck (7x7, exp=2)\", InvertedBottleneck(channels, kernel_size=7, expansion=2, dropout_rate=0)),\n    (\"InvertedBottleneck (7x7, exp=4)\", InvertedBottleneck(channels, kernel_size=7, expansion=4, dropout_rate=0)),\n    (\"InvertedBottleneckRes (7x7, exp=2)\", InvertedBottleneckRes(channels, kernel_size=7, expansion=2, dropout_rate=0)),\n    # PRCM variants\n    (\"PRCM (baseline)\", PRCM(channels, num_basis=8, dropout_rate=0)),\n    (\"PRCM_SE (exp=4)\", PRCM_SE(channels, expansion=4, dropout_rate=0)),\n    (\"PRCM_AdaptivePool (2x2)\", PRCM_AdaptivePool(channels, pool_size=2, expansion=2, dropout_rate=0)),\n    (\"PRCM_AdaptivePool (4x4)\", PRCM_AdaptivePool(channels, pool_size=4, expansion=2, dropout_rate=0)),\n    (\"PRCM_AdaptivePool (8x8)\", PRCM_AdaptivePool(channels, pool_size=8, expansion=2, dropout_rate=0)),\n    (\"PRCM_BasisExpand (exp=4)\", PRCM_BasisExpand(channels, num_basis=8, expansion=4, dropout_rate=0)),\n    (\"PRCM_SE_Affine (exp=4)\", PRCM_SE_Affine(channels, expansion=4, dropout_rate=0)),\n])\n\nprint(f\"\\n{'Variant':<40} {'Mean (ms)':>12} {'Std (ms)':>12} {'Params':>12}\")\nprint(\"-\" * 80)\n\nfor name, module in variants.items():\n    mean, std = measure_latency_gpu(module, x)\n    params = count_params(module)\n    if mean:\n        print(f\"{name:<40} {mean:>12.4f} {std:>12.4f} {params:>12,}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## 6. CPU Latency Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T04:50:58.293673Z",
     "iopub.status.busy": "2026-02-19T04:50:58.293596Z",
     "iopub.status.idle": "2026-02-19T04:50:58.681908Z",
     "shell.execute_reply": "2026-02-19T04:50:58.681421Z",
     "shell.execute_reply.started": "2026-02-19T04:50:58.293664Z"
    }
   },
   "outputs": [],
   "source": "print(\"=\" * 85)\nprint(f\"CPU Latency Comparison (input: 1x{channels}x{resolution}x{resolution})\")\nprint(\"=\" * 85)\n\nprint(f\"\\n{'Variant':<40} {'Mean (ms)':>12} {'Std (ms)':>12}\")\nprint(\"-\" * 67)\n\nfor name, module in variants.items():\n    mean, std = measure_latency_cpu(module, x)\n    print(f\"{name:<40} {mean:>12.4f} {std:>12.4f}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## 7. Multi-Channel Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T04:50:58.682593Z",
     "iopub.status.busy": "2026-02-19T04:50:58.682427Z",
     "iopub.status.idle": "2026-02-19T04:50:59.793342Z",
     "shell.execute_reply": "2026-02-19T04:50:59.792808Z",
     "shell.execute_reply.started": "2026-02-19T04:50:58.682579Z"
    }
   },
   "outputs": [],
   "source": "channel_list = [24, 48, 64, 96, 128, 192]\nresolution = 64\n\nprint(\"=\" * 130)\nprint(\"GPU Latency vs Channel Size\")\nprint(\"=\" * 130)\n\nresults = {name: [] for name in [\"InvBN(2)\", \"InvBN(4)\", \"PRCM\", \"PRCM_SE(4)\", \"AdaPool(4x4)\", \"BasisExp\"]}\n\nfor ch in channel_list:\n    x = torch.randn(1, ch, resolution, resolution)\n    \n    m = InvertedBottleneck(ch, kernel_size=7, expansion=2, dropout_rate=0)\n    mean, _ = measure_latency_gpu(m, x)\n    results[\"InvBN(2)\"].append(mean)\n    \n    m = InvertedBottleneck(ch, kernel_size=7, expansion=4, dropout_rate=0)\n    mean, _ = measure_latency_gpu(m, x)\n    results[\"InvBN(4)\"].append(mean)\n    \n    m = PRCM(ch, num_basis=8, dropout_rate=0)\n    mean, _ = measure_latency_gpu(m, x)\n    results[\"PRCM\"].append(mean)\n    \n    m = PRCM_SE(ch, expansion=4, dropout_rate=0)\n    mean, _ = measure_latency_gpu(m, x)\n    results[\"PRCM_SE(4)\"].append(mean)\n    \n    m = PRCM_AdaptivePool(ch, pool_size=4, expansion=2, dropout_rate=0)\n    mean, _ = measure_latency_gpu(m, x)\n    results[\"AdaPool(4x4)\"].append(mean)\n    \n    m = PRCM_BasisExpand(ch, num_basis=8, expansion=4, dropout_rate=0)\n    mean, _ = measure_latency_gpu(m, x)\n    results[\"BasisExp\"].append(mean)\n\nprint(f\"\\n{'Ch':<8}\", end=\"\")\nfor name in results.keys():\n    print(f\"{name:<20}\", end=\"\")\nprint()\nprint(\"-\" * 130)\n\nfor i, ch in enumerate(channel_list):\n    print(f\"{ch:<8}\", end=\"\")\n    for name in results.keys():\n        if results[name][i]:\n            print(f\"{results[name][i]:.4f} ms{'':<11}\", end=\"\")\n    print()\n\nprint(\"\\n[Note] InvBN = InvertedBottleneck (7x7 DW + 1x1 expand + 1x1 project)\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## 8. Expansion Ratio Ablation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cell-21",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T04:50:59.793852Z",
     "iopub.status.busy": "2026-02-19T04:50:59.793753Z",
     "iopub.status.idle": "2026-02-19T04:51:00.131380Z",
     "shell.execute_reply": "2026-02-19T04:51:00.130990Z",
     "shell.execute_reply.started": "2026-02-19T04:50:59.793845Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PRCM_SE Expansion Ratio Ablation (channels=64)\n",
      "================================================================================\n",
      "\n",
      "Expansion          Params     GPU (ms)     CPU (ms)   Hidden Dim\n",
      "-----------------------------------------------------------------\n",
      "1                   8,192       0.0288       0.0284           64\n",
      "2                  16,384       0.0321       0.7587          128\n",
      "4                  32,768       0.0303       0.5601          256\n",
      "8                  65,536       0.0294       0.4122          512\n",
      "16                131,072       0.0318       0.1196         1024\n",
      "\n",
      "[Insight] HW=1 이후라 expansion 늘려도 latency 증가 미미\n"
     ]
    }
   ],
   "source": [
    "channels = 64\n",
    "resolution = 64\n",
    "expansion_ratios = [1, 2, 4, 8, 16]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"PRCM_SE Expansion Ratio Ablation (channels={channels})\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "x = torch.randn(1, channels, resolution, resolution)\n",
    "\n",
    "print(f\"\\n{'Expansion':<12} {'Params':>12} {'GPU (ms)':>12} {'CPU (ms)':>12} {'Hidden Dim':>12}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "for exp in expansion_ratios:\n",
    "    m = PRCM_SE(channels, expansion=exp, dropout_rate=0)\n",
    "    params = count_params(m)\n",
    "    gpu_mean, _ = measure_latency_gpu(m, x)\n",
    "    cpu_mean, _ = measure_latency_cpu(m, x)\n",
    "    hidden = channels * exp\n",
    "    \n",
    "    print(f\"{exp:<12} {params:>12,} {gpu_mean:>12.4f} {cpu_mean:>12.4f} {hidden:>12}\")\n",
    "\n",
    "print(\"\\n[Insight] HW=1 이후라 expansion 늘려도 latency 증가 미미\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "## 9. AdaptivePool Size Ablation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cell-23",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T04:51:00.132075Z",
     "iopub.status.busy": "2026-02-19T04:51:00.131895Z",
     "iopub.status.idle": "2026-02-19T04:51:00.588231Z",
     "shell.execute_reply": "2026-02-19T04:51:00.587955Z",
     "shell.execute_reply.started": "2026-02-19T04:51:00.132061Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PRCM_AdaptivePool Size Ablation (channels=64, expansion=2)\n",
      "================================================================================\n",
      "(1x1 Conv 사용으로 파라미터는 pool_size와 무관)\n",
      "\n",
      "Pool Size          Params     GPU (ms)     CPU (ms)   Spatial Info\n",
      "----------------------------------------------------------------------\n",
      "1x1                16,384       0.0430       1.1623        1x1 = 1\n",
      "2x2                16,384       0.0619       0.1901        2x2 = 4\n",
      "4x4                16,384       0.0446       0.1715       4x4 = 16\n",
      "8x8                16,384       0.0443       0.1749       8x8 = 64\n",
      "16x16               16,384       0.0446       0.2253    16x16 = 256\n",
      "32x32               16,384       0.0445       0.3310   32x32 = 1024\n",
      "\n",
      "[Key Insight]\n",
      "- 1x1 Conv 사용으로 params는 pool_size와 무관 (모두 동일)\n",
      "- pool_size 커지면 공간 정보 더 많이 유지\n",
      "- Latency는 upsample 크기에 따라 약간 증가 (큰 pool → 적은 upsample)\n"
     ]
    }
   ],
   "source": [
    "channels = 64\n",
    "resolution = 64\n",
    "pool_sizes = [1, 2, 4, 8, 16, 32]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"PRCM_AdaptivePool Size Ablation (channels={channels}, expansion=2)\")\n",
    "print(\"=\" * 80)\n",
    "print(\"(1x1 Conv 사용으로 파라미터는 pool_size와 무관)\")\n",
    "\n",
    "x = torch.randn(1, channels, resolution, resolution)\n",
    "\n",
    "print(f\"\\n{'Pool Size':<12} {'Params':>12} {'GPU (ms)':>12} {'CPU (ms)':>12} {'Spatial Info':>14}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for ps in pool_sizes:\n",
    "    m = PRCM_AdaptivePool(channels, pool_size=ps, expansion=2, dropout_rate=0)\n",
    "    params = count_params(m)\n",
    "    gpu_mean, _ = measure_latency_gpu(m, x)\n",
    "    cpu_mean, _ = measure_latency_cpu(m, x)\n",
    "    spatial_info = f\"{ps}x{ps} = {ps*ps}\"\n",
    "    \n",
    "    print(f\"{ps}x{ps:<10} {params:>12,} {gpu_mean:>12.4f} {cpu_mean:>12.4f} {spatial_info:>14}\")\n",
    "\n",
    "print(\"\\n[Key Insight]\")\n",
    "print(\"- 1x1 Conv 사용으로 params는 pool_size와 무관 (모두 동일)\")\n",
    "print(\"- pool_size 커지면 공간 정보 더 많이 유지\")\n",
    "print(\"- Latency는 upsample 크기에 따라 약간 증가 (큰 pool → 적은 upsample)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "## 10. Multi-Resolution (Small Feature Maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T04:51:00.588621Z",
     "iopub.status.busy": "2026-02-19T04:51:00.588535Z",
     "iopub.status.idle": "2026-02-19T04:51:01.283417Z",
     "shell.execute_reply": "2026-02-19T04:51:01.283171Z",
     "shell.execute_reply.started": "2026-02-19T04:51:00.588612Z"
    }
   },
   "outputs": [],
   "source": "channels = 128\nresolutions = [4, 8, 16, 32, 64]\n\nprint(\"=\" * 130)\nprint(f\"GPU Latency at Different Resolutions (channels={channels})\")\nprint(\"=\" * 130)\nprint(\"(Deep layer에서는 resolution 작음 - 여기서 각 블록의 효율성 비교)\")\n\nresults = {name: [] for name in [\"InvBN(2)\", \"InvBN(4)\", \"PRCM\", \"PRCM_SE(4)\", \"AdaPool(4x4)\"]}\n\nfor res in resolutions:\n    x = torch.randn(1, channels, res, res)\n    \n    m = InvertedBottleneck(channels, kernel_size=7, expansion=2, dropout_rate=0)\n    mean, _ = measure_latency_gpu(m, x)\n    results[\"InvBN(2)\"].append(mean)\n    \n    m = InvertedBottleneck(channels, kernel_size=7, expansion=4, dropout_rate=0)\n    mean, _ = measure_latency_gpu(m, x)\n    results[\"InvBN(4)\"].append(mean)\n    \n    m = PRCM(channels, num_basis=8, dropout_rate=0)\n    mean, _ = measure_latency_gpu(m, x)\n    results[\"PRCM\"].append(mean)\n    \n    m = PRCM_SE(channels, expansion=4, dropout_rate=0)\n    mean, _ = measure_latency_gpu(m, x)\n    results[\"PRCM_SE(4)\"].append(mean)\n    \n    m = PRCM_AdaptivePool(channels, pool_size=4, expansion=2, dropout_rate=0)\n    mean, _ = measure_latency_gpu(m, x)\n    results[\"AdaPool(4x4)\"].append(mean)\n\nprint(f\"\\n{'Res':<10}\", end=\"\")\nfor name in results.keys():\n    print(f\"{name:<22}\", end=\"\")\nprint()\nprint(\"-\" * 120)\n\nfor i, res in enumerate(resolutions):\n    print(f\"{res}x{res:<8}\", end=\"\")\n    for name in results.keys():\n        if results[name][i]:\n            print(f\"{results[name][i]:.4f} ms{'':<13}\", end=\"\")\n    print()\n\nprint(\"\\n[Key Insight]\")\nprint(\"- InvertedBottleneck: resolution 작아질수록 latency 급감 (7x7 conv 연산량 감소)\")\nprint(\"- PRCM variants: GAP 기반이라 resolution 변화에 덜 민감\")\nprint(\"- Deep layer에서 InvBN과 PRCM의 latency 차이 줄어듦\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "## 11. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T04:51:01.283800Z",
     "iopub.status.busy": "2026-02-19T04:51:01.283725Z",
     "iopub.status.idle": "2026-02-19T04:51:01.286621Z",
     "shell.execute_reply": "2026-02-19T04:51:01.286398Z",
     "shell.execute_reply.started": "2026-02-19T04:51:01.283792Z"
    }
   },
   "outputs": [],
   "source": "print(\"=\" * 80)\nprint(\"SUMMARY\")\nprint(\"=\" * 80)\nprint(\"\"\"\n┌─────────────────────────────────────────────────────────────────────────────┐\n│ Key Findings                                                                │\n├─────────────────────────────────────────────────────────────────────────────┤\n│ 1. InvertedBottleneck vs PRCM variants                                      │\n│    - InvBN: 7x7 DW Conv로 공간 정보 직접 처리, resolution에 민감             │\n│    - PRCM: GAP 기반으로 resolution과 무관한 연산량                           │\n│    - Deep layer (작은 resolution)에서 둘의 latency 차이 줄어듦               │\n│                                                                             │\n│ 2. GAP 이후 HW=1이므로 expansion 늘려도 latency 증가 미미                   │\n│    - PRCM_SE expansion=16도 baseline 대비 큰 차이 없음                      │\n│                                                                             │\n│ 3. PRCM_AdaptivePool (1x1 Conv 버전)                                        │\n│    - 1x1 Conv 사용 → pool_size와 무관하게 파라미터 동일!                    │\n│    - NOT a modulator - 직접 feature 변환 (x * w 아님)                       │\n│    - pool_size 크면 공간 정보 더 많이 유지                                  │\n└─────────────────────────────────────────────────────────────────────────────┘\n\n┌─────────────────────────────────────────────────────────────────────────────┐\n│ Block Type Comparison                                                       │\n├─────────────────────────────────────────────────────────────────────────────┤\n│ InvertedBottleneck (Baseline Conv):                                         │\n│   - 7x7 DW Conv → 1x1 Expand → ReLU → 1x1 Project                           │\n│   - 공간 정보 직접 처리 (spatial convolution)                               │\n│   - resolution이 클수록 연산량 증가                                         │\n│                                                                             │\n│ PRCM variants (Channel Recalibration):                                      │\n│   - GAP 이후 처리로 resolution과 무관                                       │\n│   - Modulator (x * w) 또는 Direct Transform                                 │\n│   - 파라미터 효율적                                                         │\n└─────────────────────────────────────────────────────────────────────────────┘\n\n┌─────────────────────────────────────────────────────────────────────────────┐\n│ PRCM_AdaptivePool Pool Size 선택 가이드                                     │\n├─────────────────────────────────────────────────────────────────────────────┤\n│ Pool Size   공간 정보   적합한 입력 해상도                                   │\n│ ─────────────────────────────────────────────────────────────               │\n│   1x1       없음        GAP와 동일 (baseline처럼 동작)                       │\n│   2x2       4 pixels    모든 해상도 (최소 공간 정보)                         │\n│   4x4       16 pixels   ≥ 8x8 입력 (중간 정도 공간 정보)                     │\n│   8x8       64 pixels   ≥ 16x16 입력 (더 많은 공간 정보)                     │\n│                                                                             │\n│ * 파라미터는 모두 동일 (1x1 Conv 덕분)                                       │\n└─────────────────────────────────────────────────────────────────────────────┘\n\n┌─────────────────────────────────────────────────────────────────────────────┐\n│ Recommendations                                                             │\n├─────────────────────────────────────────────────────────────────────────────┤\n│ - Full spatial processing:   InvertedBottleneck (7x7 DW baseline)           │\n│ - Shallow layers (large HW): PRCM baseline or AdaptivePool(8x8, 16x16)      │\n│ - Deep layers (small HW):    PRCM_SE(exp=8~16) or AdaptivePool(2x2, 4x4)    │\n│ - Parameter budget:          PRCM_BasisExpand (basis 압축 후 expansion)     │\n│ - Spatial info + transform:  PRCM_AdaptivePool (1x1 Conv, NOT modulator)    │\n│ - Affine transform:          PRCM_SE_Affine (scale + shift)                 │\n└─────────────────────────────────────────────────────────────────────────────┘\n\"\"\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Skin_Lesion_Seg (Python 3.8)",
   "language": "python",
   "name": "skin_lesion_seg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}