{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "958e206c-dbed-4af7-8de1-b58c51258b48",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T01:05:21.237293Z",
     "iopub.status.busy": "2025-08-22T01:05:21.237177Z",
     "iopub.status.idle": "2025-08-22T01:05:22.936940Z",
     "shell.execute_reply": "2025-08-22T01:05:22.936563Z",
     "shell.execute_reply.started": "2025-08-22T01:05:21.237285Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import timeit\n",
    "from datetime import datetime\n",
    "import os\n",
    "import glob\n",
    "import natsort\n",
    "import shutil\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import random\n",
    "import copy\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# import ipynbname\n",
    "FILENAME = os.getcwd()+'/'+str(__session__).split('/')[-1]\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "from torch.utils.data import Dataset, DataLoader \n",
    "from monai.losses import TverskyLoss as TverskyLoss\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from itertools import product  # [ABLATION] 카테시안 곱\n",
    "Project_Name = 'Crack_Segmentation'\n",
    "\n",
    "model_dir = 'models'\n",
    "\n",
    "module_names = ['CrackFormer II','CSNet','DECSNet','LMNet','RSNet']\n",
    "ablation_space = {\n",
    "    'use_ms':  [True],\n",
    "    'use_gate':[True],\n",
    "}\n",
    "# ablation_space = {\n",
    "#     'use_ms':  [True],\n",
    "#     'use_gate':[True],\n",
    "# }\n",
    "model_names = module_names\n",
    "\n",
    "for module_name in module_names:\n",
    "    exec(f'from {model_dir}.{module_name} import *')\n",
    "\n",
    "iterations = [1,5]\n",
    "train_mode = 'train' # train or inference\n",
    "output_root = 'output'\n",
    "# inference시 아래 model root(output)와 실험을 원하는 results csv(전체 실험 파일)을 변수로 입력\n",
    "if train_mode=='inference':\n",
    "    past_output_root = f'output' # if train_mode=='inference'\n",
    "    past_result_csv = 'past.csv' # output vis시 수정 1/2\n",
    "    past_result_df = pd.read_csv(past_result_csv)\n",
    "    iteration_dataset_model_tuples = list(zip(past_result_df['Iteration'], past_result_df['Dataset Name'], past_result_df['Model Name']))\n",
    "if train_mode=='train_again':\n",
    "    past_result_csv = '241104_past_models_errors.csv'\n",
    "    past_result_df = pd.read_csv(past_result_csv)\n",
    "    iteration_dataset_model_tuples = list(zip(past_result_df['Iteration'], past_result_df['Dataset Name'], past_result_df['Model Name']))\n",
    "\n",
    "# visualization시 true로 만들고 저장원하는 샘플 수를 지정 # output vis시 수정 2/2\n",
    "SAVE_RESULT = False\n",
    "SAVE_N = 1500\n",
    "vis_root = f'TEST_OUTPUTS_{datetime.now().strftime(\"%y%m%d_%H%M%S\")}'\n",
    "# train시 수행하는 visualization\n",
    "TRAININGSET_VIS=False\n",
    "TRAININGSET_VIS_dir = f'TRAININGSET_VIS_{datetime.now().strftime(\"%y%m%d_%H%M%S\")}'\n",
    "if TRAININGSET_VIS:\n",
    "    os.makedirs(TRAININGSET_VIS_dir, exist_ok=True)\n",
    "\n",
    "Dataset_root = '../Total_Datasets/Crack_Segmentation_Dataset'\n",
    "# Dataset_informs = Dataset_root+'/250507_Crack_Segmentation_Dataset_Inform_v12.csv'\n",
    "Dataset_informs = Dataset_root+'/250822_Fast_Crack_Segmentation_Dataset_Inform_v3.csv'\n",
    "# Dataset_informs = Dataset_root+'/241229_Crack_Segmentation_Dataset_Inform_tmp.csv'\n",
    "df_Dataset_informs = pd.read_csv(Dataset_informs)\n",
    "\n",
    "# 실험 원하는 데이터셋에 대해서 인덱싱\n",
    "#df_Dataset_informs = pd.concat([df_Dataset_informs[:19], df_Dataset_informs[-13:], df_Dataset_informs[38:38+20]], ignore_index=True) # without cityscapes\n",
    "Dataset_Name_list = natsort.natsorted(df_Dataset_informs['Dataset Name'].tolist())\n",
    "# Dataset_Name_list.remove('L2. Pascal-VOC-2012')\n",
    "for Dataset_Name in Dataset_Name_list:\n",
    "    assert os.path.exists(os.path.join(Dataset_root, Dataset_Name)), f\"Error: The dataset path '{Dataset_root}/{Dataset_Name}' does not exist.\"\n",
    "ratio_combinations = [\n",
    "    (0.6, 0.2, 0.2)\n",
    "]\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "epochs = 100\n",
    "EARLY_STOP = 100\n",
    "batch_size = 4\n",
    "drop_last = True\n",
    "fill_last_batch = False\n",
    "devices = [0]\n",
    "\n",
    "optimizer = 'AdamW'\n",
    "lr = 1e-3\n",
    "momentum = 0.9\n",
    "weight_decay = 1e-4\n",
    "optim_args = {'optimizer': optimizer, 'lr': lr, 'momentum': momentum, 'weight_decay': weight_decay}\n",
    "\n",
    "lr_scheduler = 'CosineAnnealingLR'\n",
    "T_max = epochs\n",
    "T_0 = epochs\n",
    "eta_min = 1e-6\n",
    "lr_scheduler_args = {'lr_scheduler': lr_scheduler, 'T_max': T_max, 'T_0': T_0, 'eta_min': eta_min}\n",
    "\n",
    "loss_function = 'DiceCELoss'\n",
    "#loss_function = 'Tversky Focal Loss'\n",
    "reduction = 'mean'\n",
    "gamma = 2.0\n",
    "weight = None\n",
    "loss_function_args = {'loss_function': loss_function, 'reduction': reduction, 'gamma': gamma, 'weight': weight}\n",
    "\n",
    "EXCLUDE_BACKGROUND=True\n",
    "\n",
    "BINARY_SEG = None\n",
    "THRESHOLD = 0.5\n",
    "\n",
    "####\n",
    "augmentation_strs = [\"FLIP(HV)=>ROT([90,180,270])\"]\n",
    "\n",
    "ablation_keys = list(ablation_space.keys())\n",
    "ablation_grid = [dict(zip(ablation_keys, vals)) for vals in product(*ablation_space.values())]\n",
    "\n",
    "def ablation_to_tag(cfg: dict) -> str:\n",
    "    \"\"\"디렉토리/로그 표기를 위한 태그 문자열\"\"\"\n",
    "    return \"_\".join([f\"{k}={str(v)}\" for k, v in cfg.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96ec659d-2ef6-4323-8dc2-7a0105a5732d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T01:05:22.937929Z",
     "iopub.status.busy": "2025-08-22T01:05:22.937791Z",
     "iopub.status.idle": "2025-08-22T01:05:22.982913Z",
     "shell.execute_reply": "2025-08-22T01:05:22.982628Z",
     "shell.execute_reply.started": "2025-08-22T01:05:22.937917Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def control_random_seed(seed, pytorch=True):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    try:\n",
    "        torch.manual_seed(seed)\n",
    "        if torch.cuda.is_available()==True:\n",
    "            torch.cuda.manual_seed(seed)\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "            torch.backends.cudnn.deterministic = True\n",
    "            torch.backends.cudnn.benchmark = False\n",
    "    except:\n",
    "        pass\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "def imread_kor ( filePath, mode=cv2.IMREAD_UNCHANGED ) : \n",
    "    stream = open( filePath.encode(\"utf-8\") , \"rb\") \n",
    "    bytes = bytearray(stream.read()) \n",
    "    numpyArray = np.asarray(bytes, dtype=np.uint8)\n",
    "    return cv2.imdecode(numpyArray , mode)\n",
    "def imwrite_kor(filename, img, params=None): \n",
    "    try: \n",
    "        ext = os.path.splitext(filename)[1] \n",
    "        result, n = cv2.imencode(ext, img, params) \n",
    "        if result:\n",
    "            with open(filename, mode='w+b') as f: \n",
    "                n.tofile(f) \n",
    "                return True\n",
    "        else: \n",
    "            return False \n",
    "    except Exception as e: \n",
    "        print(e) \n",
    "        return False\n",
    "    \n",
    "def parse_dimensions(input_str):\n",
    "    # 정규 표현식을 사용하여 괄호 안의 숫자들을 찾음\n",
    "    dimensions = re.findall(r'\\((\\d+),\\s*(\\d+),\\s*(\\d+)\\)', input_str)\n",
    "    \n",
    "    # 괄호가 두 개 이상이면 에러를 출력\n",
    "    if len(dimensions) > 1:\n",
    "        return \"Error: Multiple or no valid dimensions found.\"\n",
    "    \n",
    "    # 괄호가 하나일 때는 height, width, channels 추출\n",
    "    if len(dimensions) == 1:\n",
    "        height, width, _ = map(int, dimensions[0])\n",
    "        return height, width\n",
    "    \n",
    "    # 괄호가 없을 경우 Max Width와 Max Height를 추출\n",
    "    max_width_match = re.search(r'Max Width:\\s*(\\d+)', input_str)\n",
    "    max_height_match = re.search(r'Max Height:\\s*(\\d+)', input_str)\n",
    "    \n",
    "    if max_width_match and max_height_match:\n",
    "        max_width = int(max_width_match.group(1))\n",
    "        max_height = int(max_height_match.group(1))\n",
    "        return max_width, max_height\n",
    "    \n",
    "    return \"Error: No valid dimensions or Max Width/Height found.\"\n",
    "def random_rotation(image, mask, angle_range=(-30, 30)):\n",
    "    # 지정된 각도 범위 내에서 무작위로 각도 선택\n",
    "    angle = random.uniform(angle_range[0], angle_range[1])\n",
    "    # 이미지와 마스크를 동일한 각도로 회전\n",
    "    image = TF.rotate(image, angle)\n",
    "    mask = TF.rotate(mask, angle)\n",
    "    return image, mask\n",
    "class ImageTransforms:\n",
    "    def __init__(self, image, mask=None, in_channels=3):\n",
    "        self.image = image\n",
    "        self.mask = mask\n",
    "        self.in_channels = in_channels\n",
    "\n",
    "    def HWR(self, h, w):\n",
    "        # 현이미지와 동일한 비율을 유지하며 (h, w) 안에 들어갈 수 있는 최대 크기로 리사이징\n",
    "        orig_h, orig_w = self.image.shape[:2]\n",
    "        scale = min(h / orig_h, w / orig_w)\n",
    "        new_h, new_w = int(orig_h * scale), int(orig_w * scale)\n",
    "        self.image = cv2.resize(self.image, (new_w, new_h), interpolation=cv2.INTER_LINEAR)\n",
    "        if self.mask is not None:\n",
    "            self.mask = cv2.resize(self.mask, (new_w, new_h), interpolation=cv2.INTER_NEAREST)\n",
    "        return self\n",
    "\n",
    "    def P(self, h, w):\n",
    "        # 이미지에 zero-padding을 추가하여 (h, w) 크기로 만듦\n",
    "        pad_h = max(0, h - self.image.shape[0])\n",
    "        pad_w = max(0, w - self.image.shape[1])\n",
    "        top = pad_h // 2\n",
    "        bottom = pad_h - top\n",
    "        left = pad_w // 2\n",
    "        right = pad_w - left\n",
    "        if self.in_channels == 1:\n",
    "            value = 0\n",
    "        else:\n",
    "            value = [0] * self.in_channels\n",
    "        self.image = cv2.copyMakeBorder(self.image, top, bottom, left, right, cv2.BORDER_CONSTANT, value=value)\n",
    "        if self.mask is not None:\n",
    "            self.mask = cv2.copyMakeBorder(self.mask, top, bottom, left, right, cv2.BORDER_CONSTANT, value=0)\n",
    "        return self\n",
    "\n",
    "    def R(self, h, w):\n",
    "        # 이미지를 (h, w) 크기로 리사이즈\n",
    "        self.image = cv2.resize(self.image, (w, h), interpolation=cv2.INTER_LINEAR)\n",
    "        if self.mask is not None:\n",
    "            self.mask = cv2.resize(self.mask, (w, h), interpolation=cv2.INTER_NEAREST)\n",
    "        return self\n",
    "\n",
    "    def SQP(self):\n",
    "        # 정사각형으로 만들기 위해 짧은 축 기준으로 zero-padding\n",
    "        h, w = self.image.shape[:2]\n",
    "        size = max(h, w)\n",
    "        self.P(size, size)\n",
    "        return self\n",
    "    def C(self, h, w):\n",
    "        # 이미지를 (h, w) 크기로 랜덤 크롭핑\n",
    "        img_h, img_w = self.image.shape[:2]\n",
    "        if img_h < h or img_w < w:\n",
    "            raise ValueError(\"크롭 크기가 이미지 크기보다 큽니다. 먼저 패딩 또는 리사이징을 수행하세요.\")\n",
    "        \n",
    "        top = np.random.randint(0, img_h - h + 1)\n",
    "        left = np.random.randint(0, img_w - w + 1)\n",
    "        \n",
    "        self.image = self.image[top:top + h, left:left + w]\n",
    "        if self.mask is not None:\n",
    "            self.mask = self.mask[top:top + h, left:left + w]\n",
    "        return self\n",
    "    def get_image_and_mask(self):\n",
    "        return self.image, self.mask\n",
    "\n",
    "    def get_image_and_mask(self):\n",
    "        return self.image, self.mask\n",
    "\n",
    "class ImagesDataset(Dataset):\n",
    "    def __init__(self, image_path_list, target_path_list, Height, Width, augmentation_str=None, transform_str=None):\n",
    "        self.image_path_list = image_path_list\n",
    "        self.target_path_list = target_path_list\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "        self.Height = Height\n",
    "        self.Width = Width\n",
    "        self.transform_str = transform_str\n",
    "        self.augmentation_str = augmentation_str\n",
    "    def __len__(self):\n",
    "        return len(self.image_path_list)\n",
    "\n",
    "    def apply_augmentation(self, image_pil, mask_pil):\n",
    "        operations = self.augmentation_str.split(\"=>\")\n",
    "        for op in operations:\n",
    "            op = op.strip()\n",
    "            match_flip = re.match(r\"FLIP\\((H|V|HV)\\)\", op)\n",
    "            match_rot_list = re.search(r\"ROT\\(\\[([\\d,]+)\\]\\)\", op)\n",
    "            match_rot_range = re.match(r\"ROT\\((-?\\d+(?:\\.\\d+)?),\\s*(-?\\d+(?:\\.\\d+)?)\\)\", op)\n",
    "            match_zoom = re.match(r\"ZOOM\\((\\d+(?:\\.\\d+)?),\\s*(\\d+(?:\\.\\d+)?)\\)\", op)\n",
    "            angle = 0\n",
    "            flip = 'NoFlip'\n",
    "            resize_scale = 1 \n",
    "            if match_flip:\n",
    "                direction = match_flip.group(1)\n",
    "                if direction == \"H\":\n",
    "                    if random.random() < 0.5:\n",
    "                        image_pil = TF.hflip(image_pil)\n",
    "                        mask_pil = TF.hflip(mask_pil)\n",
    "                elif direction == \"V\":\n",
    "                    if random.random() < 0.5:\n",
    "                        image_pil = TF.vflip(image_pil)\n",
    "                        mask_pil = TF.vflip(mask_pil)\n",
    "                elif direction == \"HV\":\n",
    "                    if random.random() < 0.5:\n",
    "                        image_pil = TF.hflip(image_pil)\n",
    "                        mask_pil = TF.hflip(mask_pil)\n",
    "                    if random.random() < 0.5:\n",
    "                        image_pil = TF.vflip(image_pil)\n",
    "                        mask_pil = TF.vflip(mask_pil)\n",
    "            elif match_rot_list:\n",
    "                angles = [int(num) for num in match_rot_list.group(1).split(\",\")]\n",
    "                angle = random.choice(angles)\n",
    "                image_pil = TF.rotate(image_pil, angle)\n",
    "                mask_pil = TF.rotate(mask_pil, angle, interpolation=Image.NEAREST)\n",
    "            elif match_rot_range:\n",
    "                angle_min = float(match_rot_range.group(1))\n",
    "                angle_max = float(match_rot_range.group(2))\n",
    "                angle = random.uniform(angle_min, angle_max)\n",
    "                image_pil = TF.rotate(image_pil, angle)\n",
    "                mask_pil = TF.rotate(mask_pil, angle, interpolation=Image.NEAREST)\n",
    "            elif match_zoom:\n",
    "                zoom_min = float(match_zoom.group(1))\n",
    "                zoom_max = float(match_zoom.group(2))\n",
    "                scale = random.uniform(zoom_min, zoom_max)\n",
    "                w, h = image_pil.size\n",
    "                new_w, new_h = int(w * scale), int(h * scale)\n",
    "                image_pil = TF.resize(image_pil, (new_h, new_w))\n",
    "                mask_pil = TF.resize(mask_pil, (new_h, new_w), interpolation=Image.NEAREST)\n",
    "                image_pil = TF.center_crop(image_pil, (h, w))\n",
    "                mask_pil = TF.center_crop(mask_pil, (h, w))\n",
    "                resize_scale = scale\n",
    "        return image_pil, mask_pil, (flip, angle, resize_scale)\n",
    "        \n",
    "    def parse_transform_str(self, image, mask):\n",
    "        in_channels = 1 if len(image.shape) == 2 else image.shape[2]\n",
    "        transform = ImageTransforms(image, mask, in_channels)\n",
    "        if self.transform_str:\n",
    "            operations = self.transform_str.split(\"=>\")\n",
    "            for op in operations:\n",
    "                op = op.strip()\n",
    "                match_hwr = re.match(r\"(HWR|P|R|C)\\((\\d+),\\s*(\\d+)\\)\", op)\n",
    "                match_sqp = re.match(r\"SQP\", op)\n",
    "                if match_hwr:\n",
    "                    func = match_hwr.group(1)\n",
    "                    h = int(match_hwr.group(2))\n",
    "                    w = int(match_hwr.group(3))\n",
    "                    if func == \"HWR\":\n",
    "                        transform.HWR(h, w)\n",
    "                    elif func == \"P\":\n",
    "                        transform.P(h, w)\n",
    "                    elif func == \"R\":\n",
    "                        transform.R(h, w)\n",
    "                    elif func == \"C\":\n",
    "                        transform.C(h, w)\n",
    "                elif match_sqp:\n",
    "                    transform.SQP()\n",
    "        return transform.get_image_and_mask()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_path_list[idx]\n",
    "        mask_path = self.target_path_list[idx]\n",
    "        image = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)\n",
    "        if image is None:\n",
    "            raise ValueError(f\"Image not found at {image_path}\")\n",
    "\n",
    "        mask = np.load(mask_path)\n",
    "        mask = mask.astype(np.uint8)  # Ensure mask is in the correct format\n",
    "\n",
    "        # 데이터 증강 적용\n",
    "        flip='NoFlip'; angle=0; resize_scale = 1.0;\n",
    "        if self.augmentation_str and (random.random() > 0.5):\n",
    "            if isinstance(image, np.ndarray):\n",
    "                image_pil = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB) if len(image.shape) == 3 else image)\n",
    "                mask_pil = Image.fromarray(mask)\n",
    "            else:\n",
    "                image_pil = image\n",
    "                mask_pil = mask\n",
    "            image_pil, mask_pil, aug_params = self.apply_augmentation(image_pil, mask_pil)\n",
    "            flip, angle, resize_scale = aug_params\n",
    "\n",
    "            image = np.array(image_pil)\n",
    "            mask = np.array(mask_pil)\n",
    "            if len(image.shape) == 3:\n",
    "                image = image[..., ::-1]  # Convert RGB back to BGR for OpenCV\n",
    "\n",
    "        # Apply custom transformations\n",
    "        transformed_image, transformed_mask = self.parse_transform_str(image, mask)\n",
    "        \n",
    "        if TRAININGSET_VIS==True and self.augmentation_str:\n",
    "            TRAININGSET_DATA_VIS_dir = TRAININGSET_VIS_dir+'/'+Dataset_Name\n",
    "            os.makedirs(TRAININGSET_DATA_VIS_dir, exist_ok=True)\n",
    "            shutil.copy(image_path, os.path.join(TRAININGSET_DATA_VIS_dir, f'{Dataset_Name}_{idx}_{os.path.basename(image_path)}_1_image_original.png'))\n",
    "            cv2.imwrite(os.path.join(TRAININGSET_DATA_VIS_dir, f'{Dataset_Name}_{idx}_{os.path.basename(image_path)}_3_mask_original.png'), colorize_mask(np.load(mask_path)))\n",
    "\n",
    "            # 변환된 이미지와 마스크 시각화 저장\n",
    "            # 강조: 변환된 이미지를 시각화 목적으로 저장\n",
    "            cv2.imwrite(os.path.join(TRAININGSET_DATA_VIS_dir, f'{Dataset_Name}_{idx}_{os.path.basename(image_path)}_2_image_RZ{resize_scale:.2f}_{flip}_RT{int(angle)}.png'), transformed_image)\n",
    "            cv2.imwrite(os.path.join(TRAININGSET_DATA_VIS_dir, f'{Dataset_Name}_{idx}_{os.path.basename(image_path)}_4_mask_RZ{resize_scale:.2f}_{flip}_RT{int(angle)}.png'), colorize_mask(transformed_mask))\n",
    "\n",
    "        image = self.transform(transformed_image.copy()).float()\n",
    "        mask = torch.tensor(transformed_mask).long().unsqueeze(0)\n",
    "        # print('Image-Mask verification',image.shape, image.mean(), image.max(),image.min(), np.unique(mask.numpy()), end=' ')\n",
    "        return image, mask, image_path\n",
    "        \n",
    "class SegDataLoader(DataLoader):\n",
    "    def __init__(self, dataset, batch_size=1, shuffle=False, sampler=None,\n",
    "                 batch_sampler=None, num_workers=0, collate_fn=None,\n",
    "                 pin_memory=False, drop_last=False, fill_last_batch=False):\n",
    "        self.fill_last_batch = fill_last_batch\n",
    "        self.dataset = dataset\n",
    "        super().__init__(dataset, batch_size, shuffle, sampler,\n",
    "                         batch_sampler, num_workers, collate_fn,\n",
    "                         pin_memory, drop_last)\n",
    "\n",
    "    def __iter__(self):\n",
    "        batch_iter = super().__iter__()\n",
    "        for batch in batch_iter:\n",
    "            if self.fill_last_batch and len(batch[0]) < self.batch_size:\n",
    "                additional_samples_needed = self.batch_size - len(batch[0])\n",
    "                additional_indices = random.choices(range(len(self.dataset)), k=additional_samples_needed)\n",
    "                additional_samples = [self.dataset[idx] for idx in additional_indices]\n",
    "\n",
    "                if isinstance(batch, (list, tuple)):\n",
    "                    batch = list(batch)\n",
    "                    for i in range(len(batch) - 1):  # Process tensor elements (image, mask)\n",
    "                        batch[i] = torch.cat([batch[i], torch.stack([sample[i] for sample in additional_samples])])\n",
    "                    batch[-1] = batch[-1] + [sample[-1] for sample in additional_samples]  # Process string elements (image paths)\n",
    "                    batch = tuple(batch)\n",
    "                else:\n",
    "                    batch = torch.cat([batch, torch.stack(additional_samples)])\n",
    "            yield batch\n",
    "# 클래스별 고유 색상 지정 (클래스 0은 배경, 나머지는 각 클래스에 고유 색상)\n",
    "class_colors = {\n",
    "    0: [0, 0, 0],         # 클래스 0: 검정색 (배경)\n",
    "    1: [255, 0, 0],       # 클래스 1: 빨강\n",
    "    2: [0, 255, 0],       # 클래스 2: 초록\n",
    "    3: [0, 0, 255],       # 클래스 3: 파랑\n",
    "    4: [255, 255, 0],     # 클래스 4: 노랑\n",
    "    5: [255, 0, 255],     # 클래스 5: 분홍\n",
    "    6: [0, 255, 255],     # 클래스 6: 하늘색\n",
    "    7: [128, 0, 128],     # 클래스 7: 보라색\n",
    "    8: [128, 128, 0],     # 클래스 8: 올리브색\n",
    "    9: [0, 128, 128],     # 클래스 9: 청록색\n",
    "    10: [128, 128, 128],  # 클래스 10: 회색\n",
    "    11: [192, 0, 0],      # 클래스 11: 진한 빨강\n",
    "    12: [0, 192, 0],      # 클래스 12: 진한 초록\n",
    "    13: [0, 0, 192],      # 클래스 13: 진한 파랑\n",
    "    14: [192, 192, 0],    # 클래스 14: 연한 노랑\n",
    "    15: [192, 0, 192],    # 클래스 15: 연한 분홍\n",
    "    16: [0, 192, 192],    # 클래스 16: 연한 하늘색\n",
    "    17: [128, 64, 0],     # 클래스 17: 갈색\n",
    "    18: [64, 128, 0],     # 클래스 18: 연두색\n",
    "    19: [0, 64, 128],     # 클래스 19: 어두운 청록색\n",
    "    20: [255, 128, 0],    # 클래스 20: 주황색\n",
    "    21: [128, 0, 255],    # 클래스 21: 보라색 (밝은)\n",
    "    22: [0, 128, 255],    # 클래스 22: 밝은 파랑\n",
    "    23: [255, 128, 128],  # 클래스 23: 연한 빨강\n",
    "    24: [128, 255, 128],  # 클래스 24: 연한 초록\n",
    "    25: [128, 128, 255],  # 클래스 25: 연한 파랑\n",
    "    26: [255, 255, 128],  # 클래스 26: 연한 노랑\n",
    "    27: [255, 0, 128],    # 클래스 27: 핫핑크\n",
    "    28: [128, 255, 0],    # 클래스 28: 형광 초록\n",
    "    29: [0, 255, 128],    # 클래스 29: 밝은 청록색\n",
    "}\n",
    "\n",
    "    \n",
    "def colorize_mask(mask):\n",
    "    # 마스크에 색상 입히기\n",
    "    color_mask = np.zeros((mask.shape[0], mask.shape[1], 3), dtype=np.uint8)\n",
    "    for cls, color in class_colors.items():\n",
    "        color_mask[mask == cls] = color\n",
    "    return color_mask\n",
    "    \n",
    "class DiceCELoss:\n",
    "    \"\"\"\n",
    "    Dice Loss와 CE Loss의 결합 손실 클래스\n",
    "    \"\"\"\n",
    "    def __init__(self, weight=0.5, epsilon=1e-6, mode='multiclass'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            weight (float): Dice Loss와 BCE Loss 사이의 가중치 (0~1 사이의 값)\n",
    "            epsilon (float): 0으로 나누는 것을 방지하기 위한 작은 값\n",
    "            mode (str): 'binary' 또는 'multiclass'로 손실 계산 모드를 설정\n",
    "        \"\"\"\n",
    "        self.weight = weight\n",
    "        self.epsilon = epsilon\n",
    "        self.mode = mode\n",
    "    \n",
    "    def __call__(self, pred, target):\n",
    "        \"\"\"\n",
    "        결합 손실 계산 함수\n",
    "        \n",
    "        Args:\n",
    "            pred (torch.Tensor): 예측된 확률맵, \n",
    "                - binary segmentation: shape이 (batchsize, 1, H, W)\n",
    "                - multiclass segmentation: shape이 (batchsize, num_classes, H, W)\n",
    "            target (torch.Tensor): 정답 마스크, shape이 (batchsize, 1, H, W)\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: 계산된 결합 손실 값\n",
    "        \"\"\"\n",
    "        if self.mode == 'binary':\n",
    "            # Binary Dice Loss 계산\n",
    "            pred = pred.squeeze(1)  # shape: (batchsize, H, W)\n",
    "            target = target.squeeze(1).float()\n",
    "            intersection = torch.sum(pred * target, dim=(1, 2))\n",
    "            union = torch.sum(pred, dim=(1, 2)) + torch.sum(target, dim=(1, 2))\n",
    "            dice = (2 * intersection + self.epsilon) / (union + self.epsilon)\n",
    "            dice_loss = 1 - dice.mean()\n",
    "            \n",
    "            # BCE Loss 계산\n",
    "            ce_loss = F.binary_cross_entropy(pred, target)\n",
    "        \n",
    "        elif self.mode == 'multiclass':\n",
    "            # Multiclass Dice Loss 계산\n",
    "            batchsize, num_classes, H, W = pred.shape\n",
    "            target = target.squeeze(1)\n",
    "            target_one_hot = F.one_hot(target, num_classes=num_classes).squeeze(1).permute(0, 3, 1, 2).float()\n",
    "            intersection = torch.sum(pred * target_one_hot, dim=(2, 3))\n",
    "            union = torch.sum(pred, dim=(2, 3)) + torch.sum(target_one_hot, dim=(2, 3))\n",
    "            dice = (2 * intersection + self.epsilon) / (union + self.epsilon)\n",
    "            dice_loss = 1 - dice.mean()\n",
    "            \n",
    "            # Cross Entropy Loss 계산\n",
    "            ce_loss = F.cross_entropy(pred, target)\n",
    "        else:\n",
    "            raise ValueError(\"mode should be 'binary' or 'multiclass'\")\n",
    "        \n",
    "        # 결합 손실 계산\n",
    "        combined_loss = self.weight * dice_loss + (1 - self.weight) * ce_loss\n",
    "        \n",
    "        return combined_loss\n",
    "\n",
    "class DiceCoefficient:\n",
    "    def __init__(self, num_classes=None):\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def __call__(self, y_pred, y_true):\n",
    "        y_true_one_hot = np.eye(self.num_classes)[y_true]\n",
    "        y_pred_one_hot = np.eye(self.num_classes)[y_pred]\n",
    "        intersection = np.sum(y_true_one_hot * y_pred_one_hot, axis=(1, 2))\n",
    "        union = np.sum(y_true_one_hot, axis=(1, 2)) + np.sum(y_pred_one_hot, axis=(1, 2))\n",
    "        dice = (2. * intersection) / (union + 1e-6)\n",
    "        return dice\n",
    "\n",
    "class IoU:\n",
    "    def __init__(self, num_classes=None):\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def __call__(self, y_pred, y_true):\n",
    "        y_true_one_hot = np.eye(self.num_classes)[y_true]\n",
    "        y_pred_one_hot = np.eye(self.num_classes)[y_pred]\n",
    "        intersection = np.sum(y_true_one_hot* y_pred_one_hot, axis=(1, 2))\n",
    "        union = np.sum(y_true_one_hot, axis=(1, 2)) + np.sum(y_pred_one_hot, axis=(1, 2)) - intersection\n",
    "        iou = intersection / (union + 1e-6)\n",
    "        return iou\n",
    "\n",
    "class Precision:\n",
    "    def __init__(self, num_classes=None):\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def __call__(self, y_pred, y_true):\n",
    "        y_true_one_hot = np.eye(self.num_classes)[y_true]\n",
    "        y_pred_one_hot = np.eye(self.num_classes)[y_pred]\n",
    "        tp = np.sum(y_true_one_hot* y_pred_one_hot, axis=(1, 2))\n",
    "        fp = np.sum((y_pred_one_hot == 1) & (y_true_one_hot== 0), axis=(1, 2))\n",
    "        precision = tp / (tp + fp + 1e-6)\n",
    "        return precision\n",
    "\n",
    "class Recall:\n",
    "    def __init__(self,  num_classes=None):\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def __call__(self, y_pred, y_true):\n",
    "        y_true_one_hot = np.eye(self.num_classes)[y_true]\n",
    "        y_pred_one_hot = np.eye(self.num_classes)[y_pred]\n",
    "        tp = np.sum(y_true_one_hot* y_pred_one_hot, axis=(1, 2))\n",
    "        fn = np.sum((y_true_one_hot == 1) & (y_pred_one_hot == 0), axis=(1, 2))\n",
    "        recall = tp / (tp + fn + 1e-6)\n",
    "        return recall\n",
    "    \n",
    "def check_class_presence(batch_masks, num_classes):\n",
    "    \"\"\"\n",
    "    각 샘플이 각 클래스에 대해 positive를 갖는지 체크하는 함수 (binary, multi class 동일)\n",
    "\n",
    "    Args:\n",
    "        batch_masks (numpy.ndarray): shape이 (batchsize, H, W)인 멀티 클래스 GT 마스크 파일\n",
    "        num_classes (int): 클래스의 수 (0부터 num_classes-1까지의 레이블을 가짐)\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: shape이 (batchsize, num_classes)인 Boolean 배열로,\n",
    "                       각 샘플이 각 클래스에 대해 positive를 가지면 True, 그렇지 않으면 False\n",
    "    \"\"\"\n",
    "    batchsize = batch_masks.shape[0]\n",
    "    presence_matrix = np.zeros((batchsize, num_classes), dtype=bool)\n",
    "    for i in range(batchsize):\n",
    "        for c in range(num_classes):\n",
    "            presence_matrix[i, c] = np.any(batch_masks[i] == c)\n",
    "\n",
    "    return presence_matrix\n",
    "\n",
    "    \n",
    "def train(train_loader, epoch, model, criterion, optimizer, device, activation ):\n",
    "    model.train()\n",
    "    train_losses=AverageMeter()\n",
    "\n",
    "    # for i, (input, target, _) in enumerate(tqdm(train_loader, desc=\"Training\", unit=\"batch\", leave=False)):\n",
    "    for i, (input, target, _) in enumerate(train_loader):\n",
    "        # print( f'{i+1}/{len(train_loader)} {datetime.now().strftime(\"%y%m%d_%H%M%S\")}', end=' ')\n",
    "        input = input.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        output = activation(model(input)) \n",
    "\n",
    "        loss = criterion(output,target).float()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.update(loss.detach().cpu().numpy(),input.shape[0])\n",
    "    Train_Loss=np.round(train_losses.avg,6)\n",
    "    return Train_Loss\n",
    "def validate(validation_loader, model, criterion, num_classes, device, activation, model_path=False):\n",
    "    if model_path:\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "    iou_calculator = IoU(num_classes)\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "    \n",
    "    columns = ['image_path'] + [f'class_{i}' for i in range(num_classes)]\n",
    "    metrics_df = pd.DataFrame(columns=['image_path'] + [f'{metric}_class_{i}' for metric in ['iou'] for i in range(num_classes)])\n",
    "\n",
    "    # for i, (input, target, image_path) in enumerate(tqdm(validation_loader, desc=\"Valiation\", unit=\"batch\", leave=False)):\n",
    "    for i, (input, target, image_path) in enumerate(validation_loader):\n",
    "        input = input.to(device)\n",
    "        target = target.to(device)\n",
    "        batch_size = input.size(0)\n",
    "        with torch.no_grad():\n",
    "            output = activation(model(input))\n",
    "            loss = criterion(output, target).float()\n",
    "            \n",
    "        # 예측 결과 및 타겟을 numpy 배열로 변환\n",
    "        output_np = np.squeeze(np.where(output.cpu().numpy() > THRESHOLD, 1, 0), axis=1) if BINARY_SEG else torch.argmax(output, dim=1).cpu().numpy()\n",
    "        target_np = target.cpu().numpy()\n",
    "        target_np = np.squeeze(target_np, axis=1)\n",
    "        \n",
    "        # 클래스 존재 여부 확인\n",
    "        presence_matrix = check_class_presence(target_np, num_classes=num_classes)\n",
    "        \n",
    "        # IoU 계산 (클래스가 존재하는 경우에만)\n",
    "        iou_values = iou_calculator(output_np, target_np)\n",
    "        iou_values *= presence_matrix\n",
    "\n",
    "        # presence_matrix가 True인 경우에만 데이터프레임에 IoU 값 추가\n",
    "        metrics_row = {'image_path': image_path}\n",
    "        for j in range(num_classes):\n",
    "            if presence_matrix[:, j].any():\n",
    "                metrics_row[f'iou_class_{j}'] = iou_values[:, j].mean() if presence_matrix[:, j].any() else np.nan\n",
    "        metrics_df = metrics_df.append(metrics_row, ignore_index=True)\n",
    "        \n",
    "        # 손실 업데이트\n",
    "        total_loss += loss.item() * batch_size\n",
    "        total_samples += batch_size\n",
    "\n",
    "    mean_loss = total_loss / total_samples\n",
    "    return metrics_df, mean_loss \n",
    "\n",
    "def test(test_loader, model, criterion, device, num_classes, activation, model_path=False):\n",
    "    if model_path:\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "    \n",
    "    dice_calculator = DiceCoefficient(num_classes)\n",
    "    iou_calculator = IoU(num_classes)\n",
    "    precision_calculator = Precision(num_classes) \n",
    "    recall_calculator = Recall(num_classes) \n",
    "    \n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "    if SAVE_RESULT:\n",
    "        save_n = 0\n",
    "        save_bool = False\n",
    "    columns = ['image_path'] + [f'class_{i}' for i in range(num_classes)]\n",
    "    metrics_df = pd.DataFrame(columns=['image_path'] + [f'{metric}_class_{i}' for metric in ['iou', 'dice', 'precision', 'recall'] for i in range(num_classes)])\n",
    "\n",
    "    for i, (input, target, image_path) in enumerate(tqdm(test_loader, desc=\"Test\", unit=\"batch\")):\n",
    "        input = input.to(device)\n",
    "        target = target.to(device)\n",
    "        with torch.no_grad():\n",
    "            output = activation(model(input))\n",
    "            loss = criterion(output, target).float()\n",
    "\n",
    "        # 예측 결과 및 타겟을 numpy 배열로 변환\n",
    "        output_np = np.squeeze(np.where(output.cpu().numpy() > THRESHOLD, 1, 0), axis=1) if BINARY_SEG else torch.argmax(output, dim=1).cpu().numpy()\n",
    "        target_np = target.cpu().numpy()\n",
    "        target_np = np.squeeze(target_np, axis=1)\n",
    "\n",
    "        # 클래스 존재 여부 확인\n",
    "        presence_matrix = check_class_presence(target_np, num_classes=num_classes)\n",
    "\n",
    "        # 메트릭 계산 (클래스가 존재하는 경우에만)\n",
    "        iou_values = iou_calculator(output_np, target_np)\n",
    "        dice_values = dice_calculator(output_np, target_np)\n",
    "        precision_values = precision_calculator(output_np, target_np)\n",
    "        recall_values = recall_calculator(output_np, target_np)\n",
    "\n",
    "        # presence_matrix가 True인 경우에만 데이터프레임에 메트릭 값 추가\n",
    "        batch_metrics = []\n",
    "        for b in range(input.shape[0]):\n",
    "            metrics_row = {'image_path': os.path.basename(image_path[b])}\n",
    "            for j in range(num_classes):\n",
    "                metrics_row[f'iou_class_{j}'] = iou_values[b, j] if presence_matrix[b, j] else np.nan\n",
    "                metrics_row[f'dice_class_{j}'] = dice_values[b, j] if presence_matrix[b, j] else np.nan\n",
    "                metrics_row[f'precision_class_{j}'] = precision_values[b, j] if presence_matrix[b, j] else np.nan\n",
    "                metrics_row[f'recall_class_{j}'] = recall_values[b, j] if presence_matrix[b, j] else np.nan\n",
    "            batch_metrics.append(metrics_row)\n",
    "\n",
    "        metrics_df = pd.concat([metrics_df, pd.DataFrame(batch_metrics)], ignore_index=True)\n",
    "\n",
    "        # 손실 업데이트\n",
    "        total_loss += loss.item() * input.shape[0]\n",
    "        total_samples += input.shape[0]\n",
    "\n",
    "        if SAVE_RESULT:\n",
    "            os.makedirs(vis_root, exist_ok=True)\n",
    "            for j, (out, tar, path) in enumerate(zip(output_np, target_np, image_path)):\n",
    "                save_n+=1\n",
    "                img_name = os.path.basename(path)\n",
    "                np.save(vis_root+f'/{Dataset_Name}_{save_n}_{img_name}_{model_name}_output_Iter_{iteration}.npy', out)\n",
    "                np.save(vis_root+f'/{Dataset_Name}_{save_n}_{img_name}_{model_name}_target_Iter_{iteration}.npy', tar)\n",
    "                if SAVE_RESULT and save_n>=SAVE_N:\n",
    "                    save_bool=True\n",
    "                    break\n",
    "        if SAVE_RESULT and save_bool:\n",
    "            break\n",
    "    mean_loss = total_loss / total_samples\n",
    "    return metrics_df, mean_loss\n",
    "def aggregate_measures(metrics_df, exclude_background=True, metric_types=['iou', 'dice', 'precision', 'recall']):\n",
    "    # Calculate classwise_metrics: average across all image paths for each class and metric\n",
    "    classwise_metrics = metrics_df.iloc[:, 1:].mean(skipna=True)  # Ignoring the 'image_path' column\n",
    "\n",
    "    # Calculate samplewise_metrics: average across all classes for each sample (ignoring NaN values)\n",
    "    # Grouping the metrics into IoU, Dice, Precision, and Recall\n",
    "    samplewise_metrics_dict = {}\n",
    "\n",
    "    for metric in metric_types:\n",
    "        metric_columns = [col for col in metrics_df.columns if metric in col and (not exclude_background or not col.endswith('_0'))]\n",
    "        samplewise_metrics_dict[f'mean_{metric}'] = metrics_df[metric_columns].mean(axis=1, skipna=True)\n",
    "\n",
    "    # Create DataFrames for both results with the appropriate column names\n",
    "    classwise_metrics_df = classwise_metrics.to_frame().T  # Convert to DataFrame with a single row\n",
    "\n",
    "    # For samplewise_metrics, create a DataFrame with 'image_path' and all averaged metrics\n",
    "    samplewise_metrics_df = metrics_df[['image_path']].copy()  # Keep 'image_path' column\n",
    "\n",
    "    for key, value in samplewise_metrics_dict.items():\n",
    "        samplewise_metrics_df[key] = value\n",
    "    overall_metrics = {}\n",
    "\n",
    "    for metric in metric_types:\n",
    "        metric_columns = [col for col in classwise_metrics_df.columns if metric in col and (not exclude_background or not col.endswith('_0'))]\n",
    "        overall_metrics[metric] = classwise_metrics_df[metric_columns].mean(axis=1, skipna=True).values[0]\n",
    "\n",
    "    overall_metrics_df = pd.DataFrame(overall_metrics, index=[0])\n",
    "    \n",
    "    return overall_metrics_df, classwise_metrics_df, samplewise_metrics_df\n",
    "def str_to_class(classname):\n",
    "    return getattr(sys.modules[__name__], classname)\n",
    "\n",
    "def copy_sourcefile(output_dir, src_dir = 'src' ):    \n",
    "    import os \n",
    "    import shutil\n",
    "    import glob \n",
    "    source_dir = os.path.join(output_dir, src_dir)\n",
    "\n",
    "    os.makedirs(source_dir, exist_ok=True)\n",
    "    org_files1 = os.path.join('./', '*.py' )\n",
    "    org_files2 = os.path.join('./', '*.sh' )\n",
    "    org_files3 = os.path.join('./', '*.ipynb' )\n",
    "    org_files4 = os.path.join('./', '*.txt' )\n",
    "    org_files5 = os.path.join('./', '*.json' )    \n",
    "    files =[]\n",
    "    files = glob.glob(org_files1 )\n",
    "    files += glob.glob(org_files2  )\n",
    "    files += glob.glob(org_files3  )\n",
    "    files += glob.glob(org_files4  ) \n",
    "    files += glob.glob(org_files5  )     \n",
    "\n",
    "    # print(\"COPY source to output/source dir \", files)\n",
    "    tgt_files = os.path.join( source_dir, '.' )\n",
    "    for i, file in enumerate(files):\n",
    "        shutil.copy(file, tgt_files)\n",
    "class LossSaver(object):\n",
    "    def __init__(self):\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "    def reset(self):\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "    def update(self, train_loss, val_loss):\n",
    "        self.train_losses.append(train_loss)\n",
    "        self.val_losses.append(val_loss)\n",
    "    def return_list(self):\n",
    "        return self.train_losses, self.val_losses\n",
    "    def save_as_csv(self, csv_file):\n",
    "        df = pd.DataFrame({'Train Losses': self.train_losses, 'Validation Losses': self.val_losses})\n",
    "        df.index = [f\"{i+1} Epoch\" for i in df.index]\n",
    "        df.to_csv(csv_file, index=True)\n",
    "class AverageMeter (object):\n",
    "    def __init__(self):\n",
    "        self.reset ()\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count        \n",
    "def create_dataset_lists(Dataset_dir, iteration, data_split_csv):\n",
    "    # CSV 파일을 읽어옴\n",
    "    df = pd.read_csv(data_split_csv)\n",
    "    Originals_dir = Dataset_dir+'/Originals'\n",
    "    Masks_dir = Dataset_dir+'/Masks'\n",
    "    \n",
    "    # 주어진 iteration에 맞는 split 선택\n",
    "    split_name = f'split{str(iteration).zfill(2)}'  # iteration 번호를 01, 02, ... 형식으로 맞춤\n",
    "    \n",
    "    # 선택된 split에 해당하는 데이터만 필터링\n",
    "    train_data = df[(df['split'] == split_name) & (df['data_type'] == 'training')]\n",
    "    validation_data = df[(df['split'] == split_name) & (df['data_type'] == 'validation')]\n",
    "    test_data = df[(df['split'] == split_name) & (df['data_type'] == 'test')]\n",
    "    \n",
    "    # 데이터 경로에 Dataset_dir 추가하여 절대 경로 생성\n",
    "    train_image_path_list = [os.path.join(Originals_dir, path) for path in train_data['image'].tolist()]\n",
    "    train_target_path_list = [os.path.join(Masks_dir, path) for path in train_data['mask'].tolist()]\n",
    "    \n",
    "    validation_image_path_list = [os.path.join(Originals_dir, path) for path in validation_data['image'].tolist()]\n",
    "    validation_target_path_list = [os.path.join(Masks_dir, path) for path in validation_data['mask'].tolist()]\n",
    "    \n",
    "    test_image_path_list = [os.path.join(Originals_dir, path) for path in test_data['image'].tolist()]\n",
    "    test_target_path_list = [os.path.join(Masks_dir, path) for path in test_data['mask'].tolist()]\n",
    "    \n",
    "    \n",
    "    # 결과 반환\n",
    "    return (\n",
    "        train_image_path_list, train_target_path_list, \n",
    "        validation_image_path_list, validation_target_path_list, \n",
    "        test_image_path_list, test_target_path_list\n",
    "    )\n",
    "\n",
    "def Do_Experiment(iteration, model_name, model, train_loader, validation_loader, test_loader, Optimizer, lr,  number_of_classes, epochs, Metrics,df,device, transform, train_mode):\n",
    "    start = timeit.default_timer()\n",
    "    if train_mode=='train' or train_mode=='train_again':\n",
    "        train_bool=True\n",
    "        test_bool=True\n",
    "    elif train_mode=='inference':\n",
    "        past_result_df = pd.read_csv(past_result_csv)\n",
    "        corresponding_df = past_result_df[(past_result_df['Model Name']==model_name)&\n",
    "        (past_result_df['Iteration']==iteration)&\n",
    "        (past_result_df['Dataset Name']==Dataset_Name)]\n",
    "        Train_date = corresponding_df['Train Time'].item()\n",
    "        ex_time = corresponding_df['Experiment Time'].item()\n",
    "        train_bool=False\n",
    "        test_bool=True\n",
    "        model_path=past_output_root+f'/output_{ex_time}/{model_name}_{Dataset_Name}_Split_{int(ratio_combination[0]*100)}_{int(ratio_combination[1]*100)}_{int(ratio_combination[2]*100)}_Iter_{iteration}/{Train_date}_{model_name}_{Dataset_Name}_Split_{int(ratio_combination[0]*100)}_{int(ratio_combination[1]*100)}_{int(ratio_combination[2]*100)}_Iter_{iteration}.pt'\n",
    "    if loss_function == 'Tversky Focal Loss':\n",
    "        criterion=TverskyLoss()\n",
    "    elif loss_function == 'DiceCELoss':\n",
    "        criterion=DiceCELoss(mode='binary') if BINARY_SEG else DiceCELoss(mode='multiclass') \n",
    "    if Optimizer=='Adam':\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    elif Optimizer == 'SGD':\n",
    "        momentum = 0.9\n",
    "        weight_decay = 1e-4\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum ,weight_decay=weight_decay)\n",
    "    elif Optimizer =='AdamW':\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    if lr_scheduler_args['lr_scheduler'] == 'CosineAnnealingLR':\n",
    "        lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = lr_scheduler_args['T_max'], eta_min = lr_scheduler_args['eta_min'])\n",
    "    activation = nn.Sigmoid() if BINARY_SEG else nn.Softmax(1)\n",
    "    os.makedirs(output_dir, exist_ok = True)\n",
    "    control_random_seed(seed)\n",
    "    try:\n",
    "        if train_bool:\n",
    "            now = datetime.now()\n",
    "            Train_date=now.strftime(\"%y%m%d_%H%M%S\")\n",
    "            print('Training Start Time:',Train_date)\n",
    "            model_path=f'{output_dir}/{Train_date}_{model_name}_{Dataset_Name}_Split_{int(ratio_combination[0]*100)}_{int(ratio_combination[1]*100)}_{int(ratio_combination[2]*100)}_Iter_{iteration}.pt'\n",
    "            best=9999\n",
    "            best_epoch=1\n",
    "            Early_Stop=0\n",
    "            loss_saver = LossSaver()\n",
    "            train_start_time = timeit.default_timer()\n",
    "            for epoch in range(1, epochs+1):\n",
    "                Train_Loss = train(train_loader, epoch, model, criterion, optimizer, device, activation)\n",
    "                lr_scheduler.step()\n",
    "                metrics_df, Val_Loss  = validate(validation_loader, model, criterion, number_of_classes, device, activation)\n",
    "                \n",
    "                overall_metrics_df, classwise_metrics_df, samplewise_metrics_df = aggregate_measures(metrics_df, exclude_background, metric_types = ['iou'])\n",
    "                \n",
    "                Val_IoU = overall_metrics_df['iou'].item(); \n",
    "    \n",
    "                date = datetime.now().strftime(\"%y%m%d_%H%M%S\")\n",
    "                print(f\"{epoch}EP({date}): T_Loss: {Train_Loss:.6f} V_Loss: {Val_Loss:.6f} IoU: {Val_IoU:.4f}\", end=' ')\n",
    "                \n",
    "                loss_saver.update(Train_Loss, Val_Loss)\n",
    "                loss_saver.save_as_csv(f'{output_dir}/Losses_{Train_date}.csv')\n",
    "                if Val_Loss<best:\n",
    "                    Early_Stop = 0\n",
    "                    torch.save(model.state_dict(), model_path)\n",
    "                    best_epoch = epoch\n",
    "                    best = Val_Loss\n",
    "                    print(f'Best Epoch: {best_epoch} Loss: {Val_Loss:.6f}')\n",
    "                else:\n",
    "                    print('')\n",
    "                    Early_Stop+=1\n",
    "                if Early_Stop>=EARLY_STOP:\n",
    "                    break\n",
    "            train_stop_time = timeit.default_timer()\n",
    "        if test_bool:\n",
    "            now = datetime.now()\n",
    "            date=now.strftime(\"%y%m%d_%H%M%S\")\n",
    "            print('Test Start Time:',date)\n",
    "            metrics_df, Test_Loss = test(test_loader, model, criterion, device, number_of_classes, activation, model_path=model_path)\n",
    "            overall_metrics_df, classwise_metrics_df, samplewise_metrics_df = aggregate_measures(metrics_df, exclude_background, metric_types = ['iou', 'dice', 'precision', 'recall'])\n",
    "            \n",
    "            metrics_df.to_csv(f'{output_dir}/Test_sample_class_wise_{model_name}_{Dataset_Name}_Iter_{iteration}_{Train_date}.csv', index=False, header=True, encoding=\"cp949\")\n",
    "            \n",
    "            # 각 클래스 평균 계산하여 CSV 저장\n",
    "            classwise_metrics_df.to_csv(f'{output_dir}/Test_class_wise_{model_name}_Iter_{Dataset_Name}_{iteration}_{Train_date}.csv', index=False, header=True, encoding=\"cp949\")\n",
    "            # 샘플-클래스 레벨 eval 저장\n",
    "            samplewise_metrics_df.to_csv(f'{output_dir}/Test_sample_wise_{model_name}_{Dataset_Name}_Iter_{iteration}_{Train_date}.csv', index=False, header=True, encoding=\"cp949\")\n",
    "                    \n",
    "            iou = overall_metrics_df['iou'].item(); dice = overall_metrics_df['dice'].item(); \n",
    "            precision = overall_metrics_df['precision'].item(); recall = overall_metrics_df['recall'].item();\n",
    "    \n",
    "            date = datetime.now().strftime(\"%y%m%d_%H%M%S\")\n",
    "            if train_mode == 'train' or train_mode == 'train_again' :\n",
    "                print('Best Epoch:', best_epoch)\n",
    "    \n",
    "            print(f\"Test({date}): Loss: {Test_Loss:.6f} IoU: {iou:.4f} Dice: {dice:.4f} Precision: {precision:.4f} Recall: {recall:.4f}\")\n",
    "\n",
    "            if train_mode == 'train' or train_mode == 'train_again' :\n",
    "                stop = timeit.default_timer(); m, s = divmod((train_stop_time - train_start_time)/epoch, 60); h, m = divmod(m, 60); Time_per_Epoch = \"%02d:%02d:%02d\" % (h, m, s)\n",
    "                m, s = divmod(stop - start, 60); h, m = divmod(m, 60); Time = \"%02d:%02d:%02d\" % (h, m, s)\n",
    "            else:\n",
    "                Time_per_Epoch = best_epoch = best = Time = 'infer mode'\n",
    "            \n",
    "            total_params = sum(p.numel() for p in model.parameters()); total_params = format(total_params , ',')\n",
    "            \n",
    "            Performances = [Experiments_Time, Train_date, iteration, Dataset_Name, \n",
    "                            f\"'{int(ratio_combination[0]*100)}:{int(ratio_combination[1]*100)}:{int(ratio_combination[2]*100)}\",\n",
    "                            model_name, best, Test_Loss, iou, dice, precision, recall, total_params, Time, best_epoch, Time_per_Epoch,\n",
    "                            loss_function, lr, batch_size, epochs, transform_str, augmentation_str, \n",
    "                            f'{len(train_image_path_list)}/{len(validation_image_path_list)}/{len(test_image_path_list)}', FILENAME]\n",
    "            \n",
    "            # =========================== [SAFE-ROW-APPEND] ===========================\n",
    "            # 1) Performances가 채우는 \"기본 컬럼 24개\"를 명시적으로 정의\n",
    "            _base_cols = [\n",
    "                'Experiment Time','Train Time','Iteration','Dataset Name','Data Split','Model Name',\n",
    "                'Val Loss','Test Loss','IoU','Dice','Precision','Recall','Total Params',\n",
    "                'Train-Predction Time','Best Epoch','Time per Epoch',\n",
    "                'Loss Function','LR','Batch size','#Epochs','Preprocessing','Augmentation',\n",
    "                'Sample Size','DIR'\n",
    "            ]\n",
    "            # 2) 기본 컬럼 → 값 매핑\n",
    "            _row = dict(zip(_base_cols, Performances))\n",
    "            \n",
    "            # 3) df.columns에 존재하지만 기본 컬럼에 없는 컬럼들(대개 ablation 변수들)을 채움\n",
    "            _extra_cols = [c for c in df.columns if c not in _base_cols]\n",
    "            for c in _extra_cols:\n",
    "                # ab_cfg가 있으면 그 값, 없으면 None\n",
    "                _row[c] = (ab_cfg.get(c, None) if 'ab_cfg' in locals() else None)\n",
    "            \n",
    "            # 4) df.columns 순서에 맞춰 1행 DataFrame 생성 후 concat\n",
    "            _row_df = pd.DataFrame([[ _row.get(c, None) for c in df.columns ]], columns=df.columns)\n",
    "            df = pd.concat([df, _row_df], ignore_index=True)\n",
    "            # ================================================================================\n",
    "    except torch.cuda.OutOfMemoryError:\n",
    "        error_print = f\"torch.cuda.OutOfMemoryError: {model_name} and {Dataset_Name} ({df_Dataset_informs[df_Dataset_informs['Dataset Name']==Dataset_Name]['Input Shape'].item()})\"\n",
    "        Performances = [Experiments_Time, Train_date, iteration, Dataset_Name, \n",
    "                        f\"'{int(ratio_combination[0]*100)}:{int(ratio_combination[1]*100)}:{int(ratio_combination[2]*100)}\",\n",
    "                        model_name, error_print, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "                        0, 0, 0, 0, 0, transform_str, augmentation_str,\n",
    "                        f\"'{len(train_image_path_list)}/{len(validation_image_path_list)}/{len(test_image_path_list)}\", FILENAME]\n",
    "    \n",
    "        # =========================== [SAFE-ROW-APPEND] ===========================\n",
    "        _base_cols = [\n",
    "            'Experiment Time','Train Time','Iteration','Dataset Name','Data Split','Model Name',\n",
    "            'Val Loss','Test Loss','IoU','Dice','Precision','Recall','Total Params',\n",
    "            'Train-Predction Time','Best Epoch','Time per Epoch',\n",
    "            'Loss Function','LR','Batch size','#Epochs','Preprocessing','Augmentation',\n",
    "            'Sample Size','DIR'\n",
    "        ]\n",
    "        _row = dict(zip(_base_cols, Performances))\n",
    "        _extra_cols = [c for c in df.columns if c not in _base_cols]\n",
    "        for c in _extra_cols:\n",
    "            _row[c] = (ab_cfg.get(c, None) if 'ab_cfg' in locals() else None)\n",
    "        _row_df = pd.DataFrame([[ _row.get(c, None) for c in df.columns ]], columns=df.columns)\n",
    "        df = pd.concat([df, _row_df], ignore_index=True)\n",
    "        # =======================================================================\n",
    "    \n",
    "        print('ERROR:', error_print)\n",
    "    now = datetime.now()\n",
    "    date=now.strftime(\"%y%m%d_%H%M%S\")\n",
    "    print('End',date)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def extend_to_full_batch(image_path_list, target_path_list, batch_size):\n",
    "    # 원본 리스트의 길이를 batch_size로 나누어떨어지게 늘리는 함수\n",
    "    num_samples = len(image_path_list)\n",
    "    remainder = num_samples % batch_size\n",
    "\n",
    "    if remainder != 0:\n",
    "        # 필요한 만큼을 무작위로 추가해서 채운다\n",
    "        extra_needed = batch_size - remainder\n",
    "        available_indices = list(range(num_samples))  # 모든 인덱스에서 무작위로 추가할 인덱스 선택\n",
    "        random_indices = random.choices(available_indices, k=extra_needed)  # 필요한 개수만큼 무작위로 선택\n",
    "\n",
    "        for i in random_indices:\n",
    "            image_path_list.append(image_path_list[i])\n",
    "            target_path_list.append(target_path_list[i])\n",
    "    return image_path_list,target_path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691908be-458c-416b-8edd-1e07c042414d",
   "metadata": {
    "editable": true,
    "execution": {
     "execution_failed": "2025-08-22T02:53:57.165Z",
     "iopub.execute_input": "2025-08-22T01:05:22.983415Z",
     "iopub.status.busy": "2025-08-22T01:05:22.983312Z"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment Start Time: 250822_100522\n",
      "(Iter 4)\n",
      "Dataset: CS01. Ceramic (1/5)\n",
      "Preprocessing: None\n",
      "Augmentation: FLIP(HV)=>ROT([90,180,270])\n",
      "Evaluation: None\n",
      "train/val/test: 60/20/20\n",
      "UNet_MS_IG (1/1) (Iter 4) Dataset: CS01. Ceramic (Shape: (256, 256, 3)) (1/5) Data Split: 60:20:20\n",
      "  -> Ablation: use_ms=True_use_gate=True\n",
      "Training Start Time: 250822_100523\n",
      "1EP(250822_100524): T_Loss: 0.553865 V_Loss: 0.491728 IoU: 0.0001 Best Epoch: 1 Loss: 0.491728\n",
      "2EP(250822_100525): T_Loss: 0.473288 V_Loss: 0.420907 IoU: 0.2119 Best Epoch: 2 Loss: 0.420907\n",
      "3EP(250822_100526): T_Loss: 0.424763 V_Loss: 0.399773 IoU: 0.2230 Best Epoch: 3 Loss: 0.399773\n",
      "4EP(250822_100527): T_Loss: 0.409030 V_Loss: 0.393598 IoU: 0.2259 Best Epoch: 4 Loss: 0.393598\n",
      "5EP(250822_100528): T_Loss: 0.396033 V_Loss: 0.368121 IoU: 0.2534 Best Epoch: 5 Loss: 0.368121\n",
      "6EP(250822_100530): T_Loss: 0.386291 V_Loss: 0.342053 IoU: 0.2954 Best Epoch: 6 Loss: 0.342053\n",
      "7EP(250822_100531): T_Loss: 0.377498 V_Loss: 0.354328 IoU: 0.2732 \n",
      "8EP(250822_100532): T_Loss: 0.372922 V_Loss: 0.335355 IoU: 0.3032 Best Epoch: 8 Loss: 0.335355\n",
      "9EP(250822_100533): T_Loss: 0.358266 V_Loss: 0.333155 IoU: 0.3059 Best Epoch: 9 Loss: 0.333155\n",
      "10EP(250822_100534): T_Loss: 0.353510 V_Loss: 0.335106 IoU: 0.3005 \n",
      "11EP(250822_100535): T_Loss: 0.348450 V_Loss: 0.337854 IoU: 0.3016 \n",
      "12EP(250822_100536): T_Loss: 0.340015 V_Loss: 0.335467 IoU: 0.3033 \n",
      "13EP(250822_100537): T_Loss: 0.352142 V_Loss: 0.334025 IoU: 0.3031 \n",
      "14EP(250822_100539): T_Loss: 0.341842 V_Loss: 0.337056 IoU: 0.3004 \n",
      "15EP(250822_100540): T_Loss: 0.333423 V_Loss: 0.322240 IoU: 0.3215 Best Epoch: 15 Loss: 0.322240\n",
      "16EP(250822_100541): T_Loss: 0.318000 V_Loss: 0.312002 IoU: 0.3363 Best Epoch: 16 Loss: 0.312002\n",
      "17EP(250822_100542): T_Loss: 0.317400 V_Loss: 0.328119 IoU: 0.3143 \n",
      "18EP(250822_100543): T_Loss: 0.324830 V_Loss: 0.308858 IoU: 0.3400 Best Epoch: 18 Loss: 0.308858\n",
      "19EP(250822_100544): T_Loss: 0.313744 V_Loss: 0.314227 IoU: 0.3335 \n",
      "20EP(250822_100545): T_Loss: 0.307065 V_Loss: 0.311578 IoU: 0.3377 \n",
      "21EP(250822_100546): T_Loss: 0.301755 V_Loss: 0.302122 IoU: 0.3497 Best Epoch: 21 Loss: 0.302122\n",
      "22EP(250822_100547): T_Loss: 0.294167 V_Loss: 0.299646 IoU: 0.3545 Best Epoch: 22 Loss: 0.299646\n",
      "23EP(250822_100549): T_Loss: 0.300861 V_Loss: 0.307396 IoU: 0.3411 \n",
      "24EP(250822_100550): T_Loss: 0.294169 V_Loss: 0.302821 IoU: 0.3517 \n",
      "25EP(250822_100551): T_Loss: 0.296260 V_Loss: 0.314085 IoU: 0.3300 \n",
      "26EP(250822_100552): T_Loss: 0.293487 V_Loss: 0.312138 IoU: 0.3326 \n",
      "27EP(250822_100553): T_Loss: 0.281065 V_Loss: 0.304345 IoU: 0.3474 \n",
      "28EP(250822_100554): T_Loss: 0.286635 V_Loss: 0.293173 IoU: 0.3614 Best Epoch: 28 Loss: 0.293173\n",
      "29EP(250822_100555): T_Loss: 0.283275 V_Loss: 0.311124 IoU: 0.3375 \n",
      "30EP(250822_100556): T_Loss: 0.281403 V_Loss: 0.298866 IoU: 0.3544 \n",
      "31EP(250822_100558): T_Loss: 0.282170 V_Loss: 0.312635 IoU: 0.3323 \n",
      "32EP(250822_100559): T_Loss: 0.280717 V_Loss: 0.300317 IoU: 0.3521 \n",
      "33EP(250822_100600): T_Loss: 0.273882 V_Loss: 0.290064 IoU: 0.3664 Best Epoch: 33 Loss: 0.290064\n",
      "34EP(250822_100601): T_Loss: 0.257820 V_Loss: 0.297248 IoU: 0.3606 \n",
      "35EP(250822_100602): T_Loss: 0.259620 V_Loss: 0.307473 IoU: 0.3364 \n",
      "36EP(250822_100603): T_Loss: 0.266409 V_Loss: 0.288334 IoU: 0.3715 Best Epoch: 36 Loss: 0.288334\n",
      "37EP(250822_100604): T_Loss: 0.255740 V_Loss: 0.292655 IoU: 0.3636 \n",
      "38EP(250822_100605): T_Loss: 0.254925 V_Loss: 0.290607 IoU: 0.3674 \n",
      "39EP(250822_100606): T_Loss: 0.250032 V_Loss: 0.314235 IoU: 0.3331 \n",
      "40EP(250822_100608): T_Loss: 0.257404 V_Loss: 0.291615 IoU: 0.3666 \n",
      "41EP(250822_100609): T_Loss: 0.245249 V_Loss: 0.295393 IoU: 0.3628 \n",
      "42EP(250822_100610): T_Loss: 0.239888 V_Loss: 0.301572 IoU: 0.3500 \n",
      "43EP(250822_100611): T_Loss: 0.236604 V_Loss: 0.302976 IoU: 0.3509 \n",
      "44EP(250822_100612): T_Loss: 0.220374 V_Loss: 0.305787 IoU: 0.3422 \n",
      "45EP(250822_100613): T_Loss: 0.233201 V_Loss: 0.301985 IoU: 0.3473 \n",
      "46EP(250822_100614): T_Loss: 0.236346 V_Loss: 0.301001 IoU: 0.3530 \n",
      "47EP(250822_100615): T_Loss: 0.239173 V_Loss: 0.295400 IoU: 0.3600 \n",
      "48EP(250822_100616): T_Loss: 0.226405 V_Loss: 0.322166 IoU: 0.3214 \n",
      "49EP(250822_100618): T_Loss: 0.223836 V_Loss: 0.300198 IoU: 0.3546 \n",
      "50EP(250822_100619): T_Loss: 0.207205 V_Loss: 0.288020 IoU: 0.3727 Best Epoch: 50 Loss: 0.288020\n",
      "51EP(250822_100620): T_Loss: 0.212441 V_Loss: 0.322159 IoU: 0.3181 \n",
      "52EP(250822_100621): T_Loss: 0.211213 V_Loss: 0.299397 IoU: 0.3546 \n",
      "53EP(250822_100622): T_Loss: 0.205594 V_Loss: 0.298099 IoU: 0.3581 \n",
      "54EP(250822_100623): T_Loss: 0.209947 V_Loss: 0.308711 IoU: 0.3389 \n",
      "55EP(250822_100624): T_Loss: 0.205315 V_Loss: 0.306568 IoU: 0.3467 \n",
      "56EP(250822_100625): T_Loss: 0.205801 V_Loss: 0.302388 IoU: 0.3494 \n",
      "57EP(250822_100626): T_Loss: 0.184911 V_Loss: 0.301447 IoU: 0.3489 \n",
      "58EP(250822_100628): T_Loss: 0.201742 V_Loss: 0.313021 IoU: 0.3367 \n",
      "59EP(250822_100629): T_Loss: 0.202698 V_Loss: 0.299371 IoU: 0.3536 \n",
      "60EP(250822_100630): T_Loss: 0.194335 V_Loss: 0.294561 IoU: 0.3628 \n",
      "61EP(250822_100631): T_Loss: 0.185054 V_Loss: 0.286462 IoU: 0.3778 Best Epoch: 61 Loss: 0.286462\n",
      "62EP(250822_100632): T_Loss: 0.189854 V_Loss: 0.293403 IoU: 0.3607 \n",
      "63EP(250822_100633): T_Loss: 0.184669 V_Loss: 0.285565 IoU: 0.3748 Best Epoch: 63 Loss: 0.285565\n",
      "64EP(250822_100634): T_Loss: 0.185748 V_Loss: 0.300760 IoU: 0.3533 \n",
      "65EP(250822_100635): T_Loss: 0.173088 V_Loss: 0.295407 IoU: 0.3602 \n",
      "66EP(250822_100636): T_Loss: 0.174854 V_Loss: 0.304712 IoU: 0.3478 \n",
      "67EP(250822_100638): T_Loss: 0.179431 V_Loss: 0.284560 IoU: 0.3796 Best Epoch: 67 Loss: 0.284560\n",
      "68EP(250822_100639): T_Loss: 0.188522 V_Loss: 0.300725 IoU: 0.3540 \n",
      "69EP(250822_100640): T_Loss: 0.166749 V_Loss: 0.298756 IoU: 0.3582 \n",
      "70EP(250822_100641): T_Loss: 0.162014 V_Loss: 0.300379 IoU: 0.3570 \n",
      "71EP(250822_100642): T_Loss: 0.179432 V_Loss: 0.306988 IoU: 0.3441 \n",
      "72EP(250822_100643): T_Loss: 0.162155 V_Loss: 0.299043 IoU: 0.3590 \n",
      "73EP(250822_100644): T_Loss: 0.169821 V_Loss: 0.296061 IoU: 0.3618 \n",
      "74EP(250822_100645): T_Loss: 0.165681 V_Loss: 0.306108 IoU: 0.3478 \n",
      "75EP(250822_100646): T_Loss: 0.168623 V_Loss: 0.292757 IoU: 0.3679 \n",
      "76EP(250822_100648): T_Loss: 0.161928 V_Loss: 0.298918 IoU: 0.3570 \n",
      "77EP(250822_100649): T_Loss: 0.171511 V_Loss: 0.291658 IoU: 0.3687 \n",
      "78EP(250822_100650): T_Loss: 0.165145 V_Loss: 0.298861 IoU: 0.3581 \n",
      "79EP(250822_100651): T_Loss: 0.161125 V_Loss: 0.298615 IoU: 0.3582 \n",
      "80EP(250822_100652): T_Loss: 0.146177 V_Loss: 0.305172 IoU: 0.3492 \n",
      "81EP(250822_100653): T_Loss: 0.147049 V_Loss: 0.300795 IoU: 0.3565 \n",
      "82EP(250822_100654): T_Loss: 0.154764 V_Loss: 0.298208 IoU: 0.3609 \n",
      "83EP(250822_100655): T_Loss: 0.153538 V_Loss: 0.303419 IoU: 0.3526 \n",
      "84EP(250822_100656): T_Loss: 0.141082 V_Loss: 0.299046 IoU: 0.3580 \n",
      "85EP(250822_100658): T_Loss: 0.152696 V_Loss: 0.298412 IoU: 0.3605 \n",
      "86EP(250822_100659): T_Loss: 0.148445 V_Loss: 0.298544 IoU: 0.3618 \n",
      "87EP(250822_100700): T_Loss: 0.148933 V_Loss: 0.300928 IoU: 0.3575 \n",
      "88EP(250822_100701): T_Loss: 0.139150 V_Loss: 0.297892 IoU: 0.3611 \n",
      "89EP(250822_100702): T_Loss: 0.147829 V_Loss: 0.297218 IoU: 0.3624 \n",
      "90EP(250822_100703): T_Loss: 0.145160 V_Loss: 0.298276 IoU: 0.3610 \n",
      "91EP(250822_100704): T_Loss: 0.142814 V_Loss: 0.300285 IoU: 0.3584 \n",
      "92EP(250822_100705): T_Loss: 0.141762 V_Loss: 0.299215 IoU: 0.3596 \n",
      "93EP(250822_100706): T_Loss: 0.148832 V_Loss: 0.298518 IoU: 0.3607 \n",
      "94EP(250822_100708): T_Loss: 0.145221 V_Loss: 0.298321 IoU: 0.3611 \n",
      "95EP(250822_100709): T_Loss: 0.154770 V_Loss: 0.298076 IoU: 0.3612 \n",
      "96EP(250822_100710): T_Loss: 0.129381 V_Loss: 0.298036 IoU: 0.3616 \n",
      "97EP(250822_100711): T_Loss: 0.140113 V_Loss: 0.298160 IoU: 0.3613 \n",
      "98EP(250822_100712): T_Loss: 0.148631 V_Loss: 0.298234 IoU: 0.3612 \n",
      "99EP(250822_100713): T_Loss: 0.143814 V_Loss: 0.297980 IoU: 0.3617 \n",
      "100EP(250822_100714): T_Loss: 0.146277 V_Loss: 0.297991 IoU: 0.3618 \n",
      "Test Start Time: 250822_100714\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a143a8023f04119a906804846d14bc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test:   0%|          | 0/5 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Epoch: 67\n",
      "Test(250822_100715): Loss: 0.311596 IoU: 0.3351 Dice: 0.4632 Precision: 0.4806 Recall: 0.5578\n",
      "End 250822_100715\n",
      "Dataset: CS02. CFD (2/5)\n",
      "Preprocessing: HWR(384,512)=>P(384,512)\n",
      "Augmentation: FLIP(HV)=>ROT([90,180,270])\n",
      "Evaluation: None\n",
      "train/val/test: 70/23/25\n",
      "UNet_MS_IG (1/1) (Iter 4) Dataset: CS02. CFD (Shape: (384, 512, 3)) (2/5) Data Split: 60:20:20\n",
      "  -> Ablation: use_ms=True_use_gate=True\n",
      "Training Start Time: 250822_100715\n",
      "1EP(250822_100718): T_Loss: 0.537227 V_Loss: 0.459985 IoU: 0.0000 Best Epoch: 1 Loss: 0.459985\n",
      "2EP(250822_100721): T_Loss: 0.383950 V_Loss: 0.338947 IoU: 0.2950 Best Epoch: 2 Loss: 0.338947\n",
      "3EP(250822_100724): T_Loss: 0.285798 V_Loss: 0.277043 IoU: 0.3873 Best Epoch: 3 Loss: 0.277043\n",
      "4EP(250822_100728): T_Loss: 0.240694 V_Loss: 0.238773 IoU: 0.4356 Best Epoch: 4 Loss: 0.238773\n",
      "5EP(250822_100731): T_Loss: 0.213014 V_Loss: 0.233763 IoU: 0.4386 Best Epoch: 5 Loss: 0.233763\n",
      "6EP(250822_100734): T_Loss: 0.209834 V_Loss: 0.210346 IoU: 0.4793 Best Epoch: 6 Loss: 0.210346\n",
      "7EP(250822_100738): T_Loss: 0.200959 V_Loss: 0.218828 IoU: 0.4645 \n",
      "8EP(250822_100741): T_Loss: 0.197092 V_Loss: 0.206619 IoU: 0.4877 Best Epoch: 8 Loss: 0.206619\n",
      "9EP(250822_100744): T_Loss: 0.197485 V_Loss: 0.197947 IoU: 0.4996 Best Epoch: 9 Loss: 0.197947\n",
      "10EP(250822_100747): T_Loss: 0.184912 V_Loss: 0.202952 IoU: 0.4900 \n",
      "11EP(250822_100751): T_Loss: 0.180696 V_Loss: 0.192054 IoU: 0.5102 Best Epoch: 11 Loss: 0.192054\n",
      "12EP(250822_100754): T_Loss: 0.179623 V_Loss: 0.190756 IoU: 0.5122 Best Epoch: 12 Loss: 0.190756\n",
      "13EP(250822_100757): T_Loss: 0.176858 V_Loss: 0.188953 IoU: 0.5153 Best Epoch: 13 Loss: 0.188953\n",
      "14EP(250822_100801): T_Loss: 0.172166 V_Loss: 0.192007 IoU: 0.5080 \n",
      "15EP(250822_100804): T_Loss: 0.170524 V_Loss: 0.187260 IoU: 0.5164 Best Epoch: 15 Loss: 0.187260\n",
      "16EP(250822_100807): T_Loss: 0.166105 V_Loss: 0.189720 IoU: 0.5128 \n",
      "17EP(250822_100810): T_Loss: 0.172684 V_Loss: 0.188742 IoU: 0.5136 \n",
      "18EP(250822_100814): T_Loss: 0.163855 V_Loss: 0.192528 IoU: 0.5090 \n",
      "19EP(250822_100817): T_Loss: 0.163640 V_Loss: 0.188891 IoU: 0.5148 \n",
      "20EP(250822_100820): T_Loss: 0.165997 V_Loss: 0.185371 IoU: 0.5198 Best Epoch: 20 Loss: 0.185371\n",
      "21EP(250822_100823): T_Loss: 0.162068 V_Loss: 0.182643 IoU: 0.5258 Best Epoch: 21 Loss: 0.182643\n",
      "22EP(250822_100827): T_Loss: 0.166626 V_Loss: 0.194628 IoU: 0.5036 \n",
      "23EP(250822_100830): T_Loss: 0.167010 V_Loss: 0.194118 IoU: 0.5035 \n",
      "24EP(250822_100833): T_Loss: 0.171325 V_Loss: 0.197482 IoU: 0.4987 \n",
      "25EP(250822_100836): T_Loss: 0.162667 V_Loss: 0.185839 IoU: 0.5222 \n",
      "26EP(250822_100840): T_Loss: 0.162631 V_Loss: 0.184487 IoU: 0.5225 \n",
      "27EP(250822_100843): T_Loss: 0.162262 V_Loss: 0.183897 IoU: 0.5219 \n",
      "28EP(250822_100846): T_Loss: 0.159969 V_Loss: 0.192622 IoU: 0.5071 \n",
      "29EP(250822_100849): T_Loss: 0.157484 V_Loss: 0.187145 IoU: 0.5144 \n",
      "30EP(250822_100853): T_Loss: 0.154549 V_Loss: 0.180151 IoU: 0.5293 Best Epoch: 30 Loss: 0.180151\n",
      "31EP(250822_100856): T_Loss: 0.152927 V_Loss: 0.183422 IoU: 0.5219 \n",
      "32EP(250822_100859): T_Loss: 0.149501 V_Loss: 0.189502 IoU: 0.5110 \n",
      "33EP(250822_100903): T_Loss: 0.152249 V_Loss: 0.186721 IoU: 0.5166 \n",
      "34EP(250822_100906): T_Loss: 0.153774 V_Loss: 0.187790 IoU: 0.5157 \n",
      "35EP(250822_100909): T_Loss: 0.150234 V_Loss: 0.182336 IoU: 0.5259 \n",
      "36EP(250822_100912): T_Loss: 0.151346 V_Loss: 0.188651 IoU: 0.5139 \n",
      "37EP(250822_100916): T_Loss: 0.146500 V_Loss: 0.184455 IoU: 0.5208 \n",
      "38EP(250822_100919): T_Loss: 0.144668 V_Loss: 0.181750 IoU: 0.5256 \n",
      "39EP(250822_100922): T_Loss: 0.148849 V_Loss: 0.183711 IoU: 0.5226 \n",
      "40EP(250822_100925): T_Loss: 0.148778 V_Loss: 0.184255 IoU: 0.5201 \n",
      "41EP(250822_100929): T_Loss: 0.140079 V_Loss: 0.184354 IoU: 0.5203 \n",
      "42EP(250822_100932): T_Loss: 0.135602 V_Loss: 0.183664 IoU: 0.5207 \n",
      "43EP(250822_100935): T_Loss: 0.137552 V_Loss: 0.184528 IoU: 0.5201 \n",
      "44EP(250822_100939): T_Loss: 0.133295 V_Loss: 0.184870 IoU: 0.5187 \n",
      "45EP(250822_100942): T_Loss: 0.140989 V_Loss: 0.184732 IoU: 0.5209 \n",
      "46EP(250822_100945): T_Loss: 0.138328 V_Loss: 0.188176 IoU: 0.5136 \n",
      "47EP(250822_100948): T_Loss: 0.140503 V_Loss: 0.182649 IoU: 0.5227 \n",
      "48EP(250822_100952): T_Loss: 0.131962 V_Loss: 0.185696 IoU: 0.5170 \n",
      "49EP(250822_100955): T_Loss: 0.137531 V_Loss: 0.186512 IoU: 0.5176 \n",
      "50EP(250822_100958): T_Loss: 0.123005 V_Loss: 0.183474 IoU: 0.5229 \n",
      "51EP(250822_101001): T_Loss: 0.129055 V_Loss: 0.189498 IoU: 0.5098 \n",
      "52EP(250822_101005): T_Loss: 0.132263 V_Loss: 0.187988 IoU: 0.5111 \n",
      "53EP(250822_101008): T_Loss: 0.124464 V_Loss: 0.185460 IoU: 0.5199 \n",
      "54EP(250822_101011): T_Loss: 0.126526 V_Loss: 0.189379 IoU: 0.5105 \n",
      "55EP(250822_101014): T_Loss: 0.125454 V_Loss: 0.185167 IoU: 0.5191 \n",
      "56EP(250822_101018): T_Loss: 0.126482 V_Loss: 0.189487 IoU: 0.5106 \n",
      "57EP(250822_101021): T_Loss: 0.118969 V_Loss: 0.193862 IoU: 0.5047 \n",
      "58EP(250822_101024): T_Loss: 0.130847 V_Loss: 0.187529 IoU: 0.5134 \n",
      "59EP(250822_101027): T_Loss: 0.122858 V_Loss: 0.185501 IoU: 0.5182 \n",
      "60EP(250822_101031): T_Loss: 0.120331 V_Loss: 0.191103 IoU: 0.5083 \n",
      "61EP(250822_101034): T_Loss: 0.120130 V_Loss: 0.188007 IoU: 0.5140 \n",
      "62EP(250822_101037): T_Loss: 0.115844 V_Loss: 0.184721 IoU: 0.5218 \n",
      "63EP(250822_101041): T_Loss: 0.115749 V_Loss: 0.189732 IoU: 0.5087 \n",
      "64EP(250822_101044): T_Loss: 0.120677 V_Loss: 0.188926 IoU: 0.5114 \n",
      "65EP(250822_101047): T_Loss: 0.113790 V_Loss: 0.188552 IoU: 0.5111 \n",
      "66EP(250822_101050): T_Loss: 0.105838 V_Loss: 0.191274 IoU: 0.5087 \n",
      "67EP(250822_101054): T_Loss: 0.111019 V_Loss: 0.187835 IoU: 0.5130 \n",
      "68EP(250822_101057): T_Loss: 0.116504 V_Loss: 0.190167 IoU: 0.5081 \n",
      "69EP(250822_101100): T_Loss: 0.106235 V_Loss: 0.193131 IoU: 0.5053 \n",
      "70EP(250822_101103): T_Loss: 0.103292 V_Loss: 0.193000 IoU: 0.5048 \n",
      "71EP(250822_101107): T_Loss: 0.104684 V_Loss: 0.190007 IoU: 0.5100 \n",
      "72EP(250822_101110): T_Loss: 0.101047 V_Loss: 0.191361 IoU: 0.5086 \n",
      "73EP(250822_101113): T_Loss: 0.104307 V_Loss: 0.193631 IoU: 0.5036 \n",
      "74EP(250822_101116): T_Loss: 0.103772 V_Loss: 0.199906 IoU: 0.4925 \n",
      "75EP(250822_101120): T_Loss: 0.104615 V_Loss: 0.191629 IoU: 0.5072 \n",
      "76EP(250822_101123): T_Loss: 0.101196 V_Loss: 0.191879 IoU: 0.5065 \n",
      "77EP(250822_101126): T_Loss: 0.112039 V_Loss: 0.193763 IoU: 0.5031 \n",
      "78EP(250822_101130): T_Loss: 0.095991 V_Loss: 0.193327 IoU: 0.5038 \n",
      "79EP(250822_101133): T_Loss: 0.103137 V_Loss: 0.193483 IoU: 0.5041 \n",
      "80EP(250822_101136): T_Loss: 0.095137 V_Loss: 0.195025 IoU: 0.5022 \n",
      "81EP(250822_101139): T_Loss: 0.089862 V_Loss: 0.195396 IoU: 0.5012 \n",
      "82EP(250822_101143): T_Loss: 0.087952 V_Loss: 0.193803 IoU: 0.5055 \n",
      "83EP(250822_101146): T_Loss: 0.086500 V_Loss: 0.196198 IoU: 0.5012 \n",
      "84EP(250822_101149): T_Loss: 0.086608 V_Loss: 0.195193 IoU: 0.5022 \n",
      "85EP(250822_101152): T_Loss: 0.094639 V_Loss: 0.195854 IoU: 0.5012 \n",
      "86EP(250822_101156): T_Loss: 0.086380 V_Loss: 0.195341 IoU: 0.5018 \n",
      "87EP(250822_101159): T_Loss: 0.085816 V_Loss: 0.195647 IoU: 0.5012 \n",
      "88EP(250822_101202): T_Loss: 0.083916 V_Loss: 0.193837 IoU: 0.5056 \n",
      "89EP(250822_101205): T_Loss: 0.087408 V_Loss: 0.195634 IoU: 0.5022 \n",
      "90EP(250822_101209): T_Loss: 0.092197 V_Loss: 0.196292 IoU: 0.5009 \n",
      "91EP(250822_101212): T_Loss: 0.087256 V_Loss: 0.195181 IoU: 0.5029 \n",
      "92EP(250822_101215): T_Loss: 0.088167 V_Loss: 0.195271 IoU: 0.5026 \n",
      "93EP(250822_101219): T_Loss: 0.092024 V_Loss: 0.195044 IoU: 0.5034 \n",
      "94EP(250822_101222): T_Loss: 0.084131 V_Loss: 0.195832 IoU: 0.5014 \n",
      "95EP(250822_101225): T_Loss: 0.085981 V_Loss: 0.195417 IoU: 0.5026 \n",
      "96EP(250822_101228): T_Loss: 0.079926 V_Loss: 0.195755 IoU: 0.5023 \n",
      "97EP(250822_101232): T_Loss: 0.087716 V_Loss: 0.195606 IoU: 0.5026 \n",
      "98EP(250822_101235): T_Loss: 0.084837 V_Loss: 0.195685 IoU: 0.5024 \n",
      "99EP(250822_101238): T_Loss: 0.093490 V_Loss: 0.195752 IoU: 0.5023 \n",
      "100EP(250822_101241): T_Loss: 0.089690 V_Loss: 0.195819 IoU: 0.5022 \n",
      "Test Start Time: 250822_101241\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7bb843a023c48d493bae5dfd7bcf7ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test:   0%|          | 0/7 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Epoch: 30\n",
      "Test(250822_101243): Loss: 0.187752 IoU: 0.5173 Dice: 0.6666 Precision: 0.6166 Recall: 0.7421\n",
      "End 250822_101243\n",
      "Dataset: CS03. DeepCrack237 (3/5)\n",
      "Preprocessing: P(384,640)\n",
      "Augmentation: FLIP(HV)=>ROT([90,180,270])\n",
      "Evaluation: None\n",
      "train/val/test: 142/47/48\n",
      "UNet_MS_IG (1/1) (Iter 4) Dataset: CS03. DeepCrack237 (Shape: (384, 640, 3)) (3/5) Data Split: 60:20:20\n",
      "  -> Ablation: use_ms=True_use_gate=True\n",
      "Training Start Time: 250822_101243\n",
      "1EP(250822_101251): T_Loss: 0.351615 V_Loss: 0.161044 IoU: 0.6593 Best Epoch: 1 Loss: 0.161044\n",
      "2EP(250822_101259): T_Loss: 0.156923 V_Loss: 0.137029 IoU: 0.6666 Best Epoch: 2 Loss: 0.137029\n",
      "3EP(250822_101307): T_Loss: 0.145522 V_Loss: 0.119399 IoU: 0.7028 Best Epoch: 3 Loss: 0.119399\n",
      "4EP(250822_101315): T_Loss: 0.125518 V_Loss: 0.130577 IoU: 0.6772 \n",
      "5EP(250822_101322): T_Loss: 0.120428 V_Loss: 0.126251 IoU: 0.6899 \n",
      "6EP(250822_101330): T_Loss: 0.116210 V_Loss: 0.113991 IoU: 0.7085 Best Epoch: 6 Loss: 0.113991\n",
      "7EP(250822_101338): T_Loss: 0.109905 V_Loss: 0.098271 IoU: 0.7480 Best Epoch: 7 Loss: 0.098271\n",
      "8EP(250822_101346): T_Loss: 0.111781 V_Loss: 0.093525 IoU: 0.7553 Best Epoch: 8 Loss: 0.093525\n",
      "9EP(250822_101354): T_Loss: 0.104593 V_Loss: 0.094480 IoU: 0.7549 \n",
      "10EP(250822_101402): T_Loss: 0.104057 V_Loss: 0.095977 IoU: 0.7530 \n",
      "11EP(250822_101410): T_Loss: 0.102429 V_Loss: 0.094707 IoU: 0.7567 \n",
      "12EP(250822_101418): T_Loss: 0.097384 V_Loss: 0.089588 IoU: 0.7661 Best Epoch: 12 Loss: 0.089588\n",
      "13EP(250822_101426): T_Loss: 0.092833 V_Loss: 0.085857 IoU: 0.7728 Best Epoch: 13 Loss: 0.085857\n",
      "14EP(250822_101434): T_Loss: 0.090385 V_Loss: 0.086492 IoU: 0.7728 \n",
      "15EP(250822_101442): T_Loss: 0.089806 V_Loss: 0.097241 IoU: 0.7518 \n",
      "16EP(250822_101450): T_Loss: 0.093598 V_Loss: 0.092516 IoU: 0.7598 \n",
      "17EP(250822_101458): T_Loss: 0.088274 V_Loss: 0.083162 IoU: 0.7783 Best Epoch: 17 Loss: 0.083162\n",
      "18EP(250822_101506): T_Loss: 0.087566 V_Loss: 0.090653 IoU: 0.7613 \n",
      "19EP(250822_101514): T_Loss: 0.085412 V_Loss: 0.093032 IoU: 0.7526 \n",
      "20EP(250822_101522): T_Loss: 0.083267 V_Loss: 0.082126 IoU: 0.7800 Best Epoch: 20 Loss: 0.082126\n",
      "21EP(250822_101530): T_Loss: 0.080274 V_Loss: 0.081838 IoU: 0.7796 Best Epoch: 21 Loss: 0.081838\n",
      "22EP(250822_101538): T_Loss: 0.084026 V_Loss: 0.085973 IoU: 0.7695 \n",
      "23EP(250822_101546): T_Loss: 0.081635 V_Loss: 0.090408 IoU: 0.7596 \n",
      "24EP(250822_101554): T_Loss: 0.082318 V_Loss: 0.078115 IoU: 0.7890 Best Epoch: 24 Loss: 0.078115\n",
      "25EP(250822_101602): T_Loss: 0.082766 V_Loss: 0.087168 IoU: 0.7673 \n",
      "26EP(250822_101610): T_Loss: 0.084808 V_Loss: 0.105374 IoU: 0.7317 \n",
      "27EP(250822_101618): T_Loss: 0.084353 V_Loss: 0.083078 IoU: 0.7766 \n",
      "28EP(250822_101626): T_Loss: 0.081521 V_Loss: 0.088294 IoU: 0.7634 \n",
      "29EP(250822_101634): T_Loss: 0.081022 V_Loss: 0.090555 IoU: 0.7593 \n",
      "30EP(250822_101642): T_Loss: 0.089032 V_Loss: 0.086014 IoU: 0.7706 \n",
      "31EP(250822_101650): T_Loss: 0.085975 V_Loss: 0.095247 IoU: 0.7556 \n",
      "32EP(250822_101658): T_Loss: 0.083784 V_Loss: 0.078142 IoU: 0.7877 \n",
      "33EP(250822_101706): T_Loss: 0.074904 V_Loss: 0.080235 IoU: 0.7833 \n",
      "34EP(250822_101714): T_Loss: 0.072231 V_Loss: 0.077608 IoU: 0.7891 Best Epoch: 34 Loss: 0.077608\n",
      "35EP(250822_101722): T_Loss: 0.070695 V_Loss: 0.078717 IoU: 0.7868 \n",
      "36EP(250822_101730): T_Loss: 0.070370 V_Loss: 0.077474 IoU: 0.7886 Best Epoch: 36 Loss: 0.077474\n",
      "37EP(250822_101738): T_Loss: 0.073706 V_Loss: 0.078397 IoU: 0.7871 \n",
      "38EP(250822_101746): T_Loss: 0.068714 V_Loss: 0.081079 IoU: 0.7790 \n",
      "39EP(250822_101754): T_Loss: 0.073888 V_Loss: 0.084331 IoU: 0.7761 \n",
      "40EP(250822_101802): T_Loss: 0.070195 V_Loss: 0.075555 IoU: 0.7929 Best Epoch: 40 Loss: 0.075555\n",
      "41EP(250822_101810): T_Loss: 0.066253 V_Loss: 0.077179 IoU: 0.7903 \n",
      "42EP(250822_101818): T_Loss: 0.067654 V_Loss: 0.083045 IoU: 0.7741 \n",
      "43EP(250822_101826): T_Loss: 0.067771 V_Loss: 0.075787 IoU: 0.7937 \n",
      "44EP(250822_101834): T_Loss: 0.064379 V_Loss: 0.075737 IoU: 0.7938 \n",
      "45EP(250822_101842): T_Loss: 0.062654 V_Loss: 0.074880 IoU: 0.7952 Best Epoch: 45 Loss: 0.074880\n",
      "46EP(250822_101850): T_Loss: 0.062461 V_Loss: 0.075651 IoU: 0.7924 \n",
      "47EP(250822_101858): T_Loss: 0.065160 V_Loss: 0.075567 IoU: 0.7926 \n",
      "48EP(250822_101906): T_Loss: 0.065239 V_Loss: 0.074424 IoU: 0.7943 Best Epoch: 48 Loss: 0.074424\n",
      "49EP(250822_101914): T_Loss: 0.063925 V_Loss: 0.080562 IoU: 0.7830 \n",
      "50EP(250822_101922): T_Loss: 0.061707 V_Loss: 0.075094 IoU: 0.7934 \n",
      "51EP(250822_101930): T_Loss: 0.062295 V_Loss: 0.078271 IoU: 0.7889 \n",
      "52EP(250822_101938): T_Loss: 0.061370 V_Loss: 0.077042 IoU: 0.7887 \n",
      "53EP(250822_101946): T_Loss: 0.060458 V_Loss: 0.073804 IoU: 0.7978 Best Epoch: 53 Loss: 0.073804\n",
      "54EP(250822_101954): T_Loss: 0.057460 V_Loss: 0.075532 IoU: 0.7942 \n",
      "55EP(250822_102002): T_Loss: 0.056895 V_Loss: 0.073612 IoU: 0.7980 Best Epoch: 55 Loss: 0.073612\n",
      "56EP(250822_102010): T_Loss: 0.058597 V_Loss: 0.077206 IoU: 0.7898 \n",
      "57EP(250822_102018): T_Loss: 0.057578 V_Loss: 0.076032 IoU: 0.7920 \n",
      "58EP(250822_102026): T_Loss: 0.059927 V_Loss: 0.076275 IoU: 0.7921 \n",
      "59EP(250822_102034): T_Loss: 0.055803 V_Loss: 0.074346 IoU: 0.7960 \n",
      "60EP(250822_102042): T_Loss: 0.055528 V_Loss: 0.074219 IoU: 0.7969 \n",
      "61EP(250822_102050): T_Loss: 0.054017 V_Loss: 0.073635 IoU: 0.7979 \n",
      "62EP(250822_102058): T_Loss: 0.054934 V_Loss: 0.072525 IoU: 0.7995 Best Epoch: 62 Loss: 0.072525\n",
      "63EP(250822_102106): T_Loss: 0.053594 V_Loss: 0.072356 IoU: 0.7994 Best Epoch: 63 Loss: 0.072356\n",
      "64EP(250822_102114): T_Loss: 0.054581 V_Loss: 0.077472 IoU: 0.7884 \n",
      "65EP(250822_102122): T_Loss: 0.052810 V_Loss: 0.073364 IoU: 0.7988 \n",
      "66EP(250822_102130): T_Loss: 0.051748 V_Loss: 0.073481 IoU: 0.7981 \n",
      "67EP(250822_102138): T_Loss: 0.052749 V_Loss: 0.076095 IoU: 0.7914 \n",
      "68EP(250822_102146): T_Loss: 0.051734 V_Loss: 0.071458 IoU: 0.8013 Best Epoch: 68 Loss: 0.071458\n",
      "69EP(250822_102154): T_Loss: 0.052081 V_Loss: 0.072420 IoU: 0.7996 \n",
      "70EP(250822_102202): T_Loss: 0.050173 V_Loss: 0.071162 IoU: 0.8026 Best Epoch: 70 Loss: 0.071162\n",
      "71EP(250822_102210): T_Loss: 0.050283 V_Loss: 0.071044 IoU: 0.8031 Best Epoch: 71 Loss: 0.071044\n",
      "72EP(250822_102218): T_Loss: 0.050236 V_Loss: 0.072224 IoU: 0.8008 \n",
      "73EP(250822_102226): T_Loss: 0.052508 V_Loss: 0.072285 IoU: 0.8014 \n",
      "74EP(250822_102234): T_Loss: 0.050550 V_Loss: 0.072245 IoU: 0.8009 \n",
      "75EP(250822_102242): T_Loss: 0.050161 V_Loss: 0.072419 IoU: 0.8005 \n",
      "76EP(250822_102250): T_Loss: 0.049880 V_Loss: 0.072448 IoU: 0.8006 \n",
      "77EP(250822_102258): T_Loss: 0.048407 V_Loss: 0.071626 IoU: 0.8025 \n",
      "78EP(250822_102306): T_Loss: 0.048189 V_Loss: 0.071720 IoU: 0.8019 \n",
      "79EP(250822_102314): T_Loss: 0.052119 V_Loss: 0.073191 IoU: 0.7987 \n",
      "80EP(250822_102322): T_Loss: 0.047387 V_Loss: 0.073162 IoU: 0.7989 \n",
      "81EP(250822_102330): T_Loss: 0.047047 V_Loss: 0.072323 IoU: 0.8006 \n",
      "82EP(250822_102338): T_Loss: 0.047771 V_Loss: 0.072348 IoU: 0.8009 \n",
      "83EP(250822_102346): T_Loss: 0.047471 V_Loss: 0.072506 IoU: 0.8005 \n",
      "84EP(250822_102354): T_Loss: 0.046186 V_Loss: 0.071704 IoU: 0.8023 \n",
      "85EP(250822_102402): T_Loss: 0.047142 V_Loss: 0.072221 IoU: 0.8015 \n",
      "86EP(250822_102410): T_Loss: 0.046690 V_Loss: 0.072155 IoU: 0.8016 \n",
      "87EP(250822_102418): T_Loss: 0.047038 V_Loss: 0.072950 IoU: 0.8000 \n",
      "88EP(250822_102426): T_Loss: 0.046781 V_Loss: 0.072443 IoU: 0.8008 \n",
      "89EP(250822_102433): T_Loss: 0.046887 V_Loss: 0.071959 IoU: 0.8018 \n",
      "90EP(250822_102441): T_Loss: 0.046923 V_Loss: 0.072579 IoU: 0.8011 \n",
      "91EP(250822_102449): T_Loss: 0.046999 V_Loss: 0.071826 IoU: 0.8024 \n",
      "92EP(250822_102457): T_Loss: 0.046901 V_Loss: 0.072245 IoU: 0.8019 \n",
      "93EP(250822_102505): T_Loss: 0.047874 V_Loss: 0.071972 IoU: 0.8021 \n",
      "94EP(250822_102513): T_Loss: 0.045035 V_Loss: 0.072034 IoU: 0.8020 \n",
      "95EP(250822_102521): T_Loss: 0.048285 V_Loss: 0.071912 IoU: 0.8022 \n",
      "96EP(250822_102529): T_Loss: 0.043767 V_Loss: 0.072008 IoU: 0.8021 \n",
      "97EP(250822_102537): T_Loss: 0.045535 V_Loss: 0.072041 IoU: 0.8020 \n",
      "98EP(250822_102545): T_Loss: 0.045362 V_Loss: 0.072134 IoU: 0.8018 \n",
      "99EP(250822_102553): T_Loss: 0.046412 V_Loss: 0.072150 IoU: 0.8018 \n",
      "100EP(250822_102601): T_Loss: 0.045869 V_Loss: 0.072171 IoU: 0.8018 \n",
      "Test Start Time: 250822_102601\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "374f911ea1e24214b26691a17fd501a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test:   0%|          | 0/12 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Epoch: 71\n",
      "Test(250822_102604): Loss: 0.071964 IoU: 0.8114 Dice: 0.8884 Precision: 0.8829 Recall: 0.9089\n",
      "End 250822_102604\n",
      "Dataset: CS04. Masonry (4/5)\n",
      "Preprocessing: R(256,256)\n",
      "Augmentation: FLIP(HV)=>ROT([90,180,270])\n",
      "Evaluation: None\n",
      "train/val/test: 144/48/48\n",
      "UNet_MS_IG (1/1) (Iter 4) Dataset: CS04. Masonry (Shape: (256, 256, 3)) (4/5) Data Split: 60:20:20\n",
      "  -> Ablation: use_ms=True_use_gate=True\n",
      "Training Start Time: 250822_102604\n",
      "1EP(250822_102606): T_Loss: 0.352472 V_Loss: 0.219986 IoU: 0.5705 Best Epoch: 1 Loss: 0.219986\n",
      "2EP(250822_102609): T_Loss: 0.219505 V_Loss: 0.191456 IoU: 0.6069 Best Epoch: 2 Loss: 0.191456\n",
      "3EP(250822_102611): T_Loss: 0.209743 V_Loss: 0.181432 IoU: 0.6168 Best Epoch: 3 Loss: 0.181432\n",
      "4EP(250822_102614): T_Loss: 0.194086 V_Loss: 0.187676 IoU: 0.6066 \n",
      "5EP(250822_102616): T_Loss: 0.184723 V_Loss: 0.191709 IoU: 0.6087 \n",
      "6EP(250822_102618): T_Loss: 0.187771 V_Loss: 0.189757 IoU: 0.5947 \n",
      "7EP(250822_102621): T_Loss: 0.185376 V_Loss: 0.178437 IoU: 0.6203 Best Epoch: 7 Loss: 0.178437\n",
      "8EP(250822_102623): T_Loss: 0.168947 V_Loss: 0.168602 IoU: 0.6322 Best Epoch: 8 Loss: 0.168602\n",
      "9EP(250822_102626): T_Loss: 0.171823 V_Loss: 0.176153 IoU: 0.6280 \n",
      "10EP(250822_102628): T_Loss: 0.163077 V_Loss: 0.168161 IoU: 0.6319 Best Epoch: 10 Loss: 0.168161\n",
      "11EP(250822_102631): T_Loss: 0.164014 V_Loss: 0.167932 IoU: 0.6341 Best Epoch: 11 Loss: 0.167932\n",
      "12EP(250822_102633): T_Loss: 0.165570 V_Loss: 0.170379 IoU: 0.6260 \n",
      "13EP(250822_102635): T_Loss: 0.159741 V_Loss: 0.170518 IoU: 0.6266 \n",
      "14EP(250822_102638): T_Loss: 0.170231 V_Loss: 0.162965 IoU: 0.6423 Best Epoch: 14 Loss: 0.162965\n",
      "15EP(250822_102640): T_Loss: 0.157956 V_Loss: 0.153494 IoU: 0.6629 Best Epoch: 15 Loss: 0.153494\n",
      "16EP(250822_102643): T_Loss: 0.162527 V_Loss: 0.166266 IoU: 0.6347 \n",
      "17EP(250822_102645): T_Loss: 0.150998 V_Loss: 0.150528 IoU: 0.6644 Best Epoch: 17 Loss: 0.150528\n",
      "18EP(250822_102648): T_Loss: 0.149807 V_Loss: 0.151218 IoU: 0.6613 \n",
      "19EP(250822_102650): T_Loss: 0.152926 V_Loss: 0.169730 IoU: 0.6295 \n",
      "20EP(250822_102652): T_Loss: 0.145199 V_Loss: 0.157707 IoU: 0.6470 \n",
      "21EP(250822_102655): T_Loss: 0.145552 V_Loss: 0.156294 IoU: 0.6537 \n",
      "22EP(250822_102657): T_Loss: 0.141204 V_Loss: 0.158628 IoU: 0.6467 \n",
      "23EP(250822_102700): T_Loss: 0.139598 V_Loss: 0.148167 IoU: 0.6672 Best Epoch: 23 Loss: 0.148167\n",
      "24EP(250822_102702): T_Loss: 0.134096 V_Loss: 0.152578 IoU: 0.6618 \n",
      "25EP(250822_102704): T_Loss: 0.130448 V_Loss: 0.143959 IoU: 0.6743 Best Epoch: 25 Loss: 0.143959\n",
      "26EP(250822_102707): T_Loss: 0.127153 V_Loss: 0.155140 IoU: 0.6583 \n",
      "27EP(250822_102709): T_Loss: 0.134684 V_Loss: 0.148664 IoU: 0.6643 \n",
      "28EP(250822_102712): T_Loss: 0.134130 V_Loss: 0.143300 IoU: 0.6755 Best Epoch: 28 Loss: 0.143300\n",
      "29EP(250822_102714): T_Loss: 0.123775 V_Loss: 0.145054 IoU: 0.6693 \n",
      "30EP(250822_102717): T_Loss: 0.131964 V_Loss: 0.150622 IoU: 0.6612 \n",
      "31EP(250822_102719): T_Loss: 0.142407 V_Loss: 0.151258 IoU: 0.6586 \n",
      "32EP(250822_102721): T_Loss: 0.141155 V_Loss: 0.153959 IoU: 0.6571 \n",
      "33EP(250822_102724): T_Loss: 0.132822 V_Loss: 0.146250 IoU: 0.6698 \n",
      "34EP(250822_102726): T_Loss: 0.123167 V_Loss: 0.140238 IoU: 0.6780 Best Epoch: 34 Loss: 0.140238\n",
      "35EP(250822_102729): T_Loss: 0.122615 V_Loss: 0.142400 IoU: 0.6770 \n",
      "36EP(250822_102731): T_Loss: 0.119996 V_Loss: 0.138583 IoU: 0.6840 Best Epoch: 36 Loss: 0.138583\n",
      "37EP(250822_102733): T_Loss: 0.117836 V_Loss: 0.142434 IoU: 0.6763 \n",
      "38EP(250822_102736): T_Loss: 0.114236 V_Loss: 0.139760 IoU: 0.6841 \n",
      "39EP(250822_102738): T_Loss: 0.114033 V_Loss: 0.153537 IoU: 0.6569 \n",
      "40EP(250822_102741): T_Loss: 0.113805 V_Loss: 0.136526 IoU: 0.6869 Best Epoch: 40 Loss: 0.136526\n",
      "41EP(250822_102743): T_Loss: 0.108032 V_Loss: 0.144421 IoU: 0.6750 \n",
      "42EP(250822_102746): T_Loss: 0.116957 V_Loss: 0.144641 IoU: 0.6714 \n",
      "43EP(250822_102748): T_Loss: 0.110084 V_Loss: 0.135202 IoU: 0.6877 Best Epoch: 43 Loss: 0.135202\n",
      "44EP(250822_102750): T_Loss: 0.103781 V_Loss: 0.136828 IoU: 0.6840 \n",
      "45EP(250822_102753): T_Loss: 0.103451 V_Loss: 0.141415 IoU: 0.6768 \n",
      "46EP(250822_102755): T_Loss: 0.103918 V_Loss: 0.135840 IoU: 0.6866 \n",
      "47EP(250822_102758): T_Loss: 0.101647 V_Loss: 0.131931 IoU: 0.6938 Best Epoch: 47 Loss: 0.131931\n",
      "48EP(250822_102800): T_Loss: 0.105138 V_Loss: 0.137807 IoU: 0.6842 \n",
      "49EP(250822_102802): T_Loss: 0.101262 V_Loss: 0.137243 IoU: 0.6843 \n",
      "50EP(250822_102805): T_Loss: 0.099144 V_Loss: 0.138156 IoU: 0.6805 \n",
      "51EP(250822_102807): T_Loss: 0.099982 V_Loss: 0.132269 IoU: 0.6936 \n",
      "52EP(250822_102810): T_Loss: 0.094603 V_Loss: 0.137070 IoU: 0.6842 \n",
      "53EP(250822_102812): T_Loss: 0.095356 V_Loss: 0.135715 IoU: 0.6861 \n",
      "54EP(250822_102815): T_Loss: 0.094501 V_Loss: 0.133958 IoU: 0.6903 \n",
      "55EP(250822_102817): T_Loss: 0.091466 V_Loss: 0.136524 IoU: 0.6871 \n",
      "56EP(250822_102819): T_Loss: 0.096204 V_Loss: 0.142165 IoU: 0.6752 \n",
      "57EP(250822_102822): T_Loss: 0.092876 V_Loss: 0.137225 IoU: 0.6849 \n",
      "58EP(250822_102824): T_Loss: 0.087211 V_Loss: 0.133618 IoU: 0.6926 \n",
      "59EP(250822_102827): T_Loss: 0.086908 V_Loss: 0.134440 IoU: 0.6931 \n",
      "60EP(250822_102829): T_Loss: 0.091394 V_Loss: 0.131882 IoU: 0.6954 Best Epoch: 60 Loss: 0.131882\n",
      "61EP(250822_102831): T_Loss: 0.084064 V_Loss: 0.132184 IoU: 0.6960 \n",
      "62EP(250822_102834): T_Loss: 0.083661 V_Loss: 0.135419 IoU: 0.6909 \n",
      "63EP(250822_102836): T_Loss: 0.081789 V_Loss: 0.136378 IoU: 0.6870 \n",
      "64EP(250822_102839): T_Loss: 0.082697 V_Loss: 0.136277 IoU: 0.6879 \n",
      "65EP(250822_102841): T_Loss: 0.079410 V_Loss: 0.131038 IoU: 0.6984 Best Epoch: 65 Loss: 0.131038\n",
      "66EP(250822_102844): T_Loss: 0.075548 V_Loss: 0.131689 IoU: 0.6959 \n",
      "67EP(250822_102846): T_Loss: 0.078110 V_Loss: 0.139029 IoU: 0.6818 \n",
      "68EP(250822_102848): T_Loss: 0.078188 V_Loss: 0.135042 IoU: 0.6895 \n",
      "69EP(250822_102851): T_Loss: 0.074313 V_Loss: 0.135652 IoU: 0.6875 \n",
      "70EP(250822_102853): T_Loss: 0.072582 V_Loss: 0.135243 IoU: 0.6910 \n",
      "71EP(250822_102856): T_Loss: 0.073369 V_Loss: 0.135468 IoU: 0.6879 \n",
      "72EP(250822_102858): T_Loss: 0.071634 V_Loss: 0.136814 IoU: 0.6856 \n",
      "73EP(250822_102900): T_Loss: 0.076520 V_Loss: 0.134456 IoU: 0.6914 \n",
      "74EP(250822_102903): T_Loss: 0.076754 V_Loss: 0.136668 IoU: 0.6865 \n",
      "75EP(250822_102905): T_Loss: 0.076428 V_Loss: 0.134502 IoU: 0.6905 \n",
      "76EP(250822_102908): T_Loss: 0.068905 V_Loss: 0.134699 IoU: 0.6906 \n",
      "77EP(250822_102910): T_Loss: 0.070154 V_Loss: 0.132706 IoU: 0.6963 \n",
      "78EP(250822_102913): T_Loss: 0.071544 V_Loss: 0.133748 IoU: 0.6929 \n",
      "79EP(250822_102915): T_Loss: 0.070775 V_Loss: 0.134836 IoU: 0.6914 \n",
      "80EP(250822_102917): T_Loss: 0.068609 V_Loss: 0.135545 IoU: 0.6884 \n",
      "81EP(250822_102920): T_Loss: 0.065977 V_Loss: 0.135537 IoU: 0.6890 \n",
      "82EP(250822_102922): T_Loss: 0.068745 V_Loss: 0.135070 IoU: 0.6900 \n",
      "83EP(250822_102925): T_Loss: 0.065954 V_Loss: 0.132334 IoU: 0.6969 \n",
      "84EP(250822_102927): T_Loss: 0.065989 V_Loss: 0.135436 IoU: 0.6904 \n",
      "85EP(250822_102929): T_Loss: 0.067133 V_Loss: 0.133016 IoU: 0.6944 \n",
      "86EP(250822_102932): T_Loss: 0.064548 V_Loss: 0.135583 IoU: 0.6892 \n",
      "87EP(250822_102934): T_Loss: 0.067919 V_Loss: 0.133559 IoU: 0.6933 \n",
      "88EP(250822_102937): T_Loss: 0.064800 V_Loss: 0.134054 IoU: 0.6920 \n",
      "89EP(250822_102939): T_Loss: 0.064571 V_Loss: 0.134909 IoU: 0.6909 \n",
      "90EP(250822_102942): T_Loss: 0.064892 V_Loss: 0.133997 IoU: 0.6924 \n",
      "91EP(250822_102944): T_Loss: 0.065801 V_Loss: 0.134239 IoU: 0.6919 \n",
      "92EP(250822_102946): T_Loss: 0.063720 V_Loss: 0.134394 IoU: 0.6919 \n",
      "93EP(250822_102949): T_Loss: 0.064426 V_Loss: 0.134676 IoU: 0.6914 \n",
      "94EP(250822_102951): T_Loss: 0.062072 V_Loss: 0.134312 IoU: 0.6922 \n",
      "95EP(250822_102954): T_Loss: 0.065149 V_Loss: 0.134522 IoU: 0.6919 \n",
      "96EP(250822_102956): T_Loss: 0.060536 V_Loss: 0.134577 IoU: 0.6916 \n",
      "97EP(250822_102958): T_Loss: 0.065023 V_Loss: 0.134470 IoU: 0.6921 \n",
      "98EP(250822_103001): T_Loss: 0.060588 V_Loss: 0.134440 IoU: 0.6922 \n",
      "99EP(250822_103003): T_Loss: 0.063925 V_Loss: 0.134388 IoU: 0.6923 \n",
      "100EP(250822_103006): T_Loss: 0.065587 V_Loss: 0.134440 IoU: 0.6923 \n",
      "Test Start Time: 250822_103006\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c3c6989584c464a8d40e51fd3fd7717",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test:   0%|          | 0/12 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Epoch: 65\n",
      "Test(250822_103006): Loss: 0.159118 IoU: 0.6444 Dice: 0.7673 Precision: 0.7591 Recall: 0.8157\n",
      "End 250822_103006\n",
      "Dataset: CS07. DeepCrack537 (5/5)\n",
      "Preprocessing: P(384,640)\n",
      "Augmentation: FLIP(HV)=>ROT([90,180,270])\n",
      "Evaluation: None\n",
      "train/val/test: 316/105/106\n",
      "UNet_MS_IG (1/1) (Iter 4) Dataset: CS07. DeepCrack537 (Shape: (384, 640, 3)) (5/5) Data Split: 60:20:20\n",
      "  -> Ablation: use_ms=True_use_gate=True\n",
      "Training Start Time: 250822_103006\n",
      "1EP(250822_103024): T_Loss: 0.266717 V_Loss: 0.179933 IoU: 0.5950 Best Epoch: 1 Loss: 0.179933\n",
      "2EP(250822_103042): T_Loss: 0.155191 V_Loss: 0.133580 IoU: 0.6797 Best Epoch: 2 Loss: 0.133580\n",
      "3EP(250822_103059): T_Loss: 0.140398 V_Loss: 0.133623 IoU: 0.6784 \n",
      "4EP(250822_103117): T_Loss: 0.134605 V_Loss: 0.129658 IoU: 0.6842 Best Epoch: 4 Loss: 0.129658\n",
      "5EP(250822_103135): T_Loss: 0.127601 V_Loss: 0.132287 IoU: 0.6812 \n",
      "6EP(250822_103153): T_Loss: 0.123925 V_Loss: 0.123909 IoU: 0.6945 Best Epoch: 6 Loss: 0.123909\n",
      "7EP(250822_103210): T_Loss: 0.118186 V_Loss: 0.113933 IoU: 0.7112 Best Epoch: 7 Loss: 0.113933\n",
      "8EP(250822_103228): T_Loss: 0.114550 V_Loss: 0.118877 IoU: 0.7001 \n",
      "9EP(250822_103246): T_Loss: 0.114931 V_Loss: 0.110979 IoU: 0.7166 Best Epoch: 9 Loss: 0.110979\n",
      "10EP(250822_103303): T_Loss: 0.106197 V_Loss: 0.109855 IoU: 0.7187 Best Epoch: 10 Loss: 0.109855\n",
      "11EP(250822_103321): T_Loss: 0.107189 V_Loss: 0.111242 IoU: 0.7172 \n",
      "12EP(250822_103339): T_Loss: 0.106322 V_Loss: 0.113466 IoU: 0.7129 \n",
      "13EP(250822_103356): T_Loss: 0.106471 V_Loss: 0.108543 IoU: 0.7225 Best Epoch: 13 Loss: 0.108543\n",
      "14EP(250822_103414): T_Loss: 0.101288 V_Loss: 0.101889 IoU: 0.7331 Best Epoch: 14 Loss: 0.101889\n",
      "15EP(250822_103432): T_Loss: 0.100158 V_Loss: 0.103929 IoU: 0.7278 \n",
      "16EP(250822_103449): T_Loss: 0.098211 V_Loss: 0.100478 IoU: 0.7347 Best Epoch: 16 Loss: 0.100478\n",
      "17EP(250822_103507): T_Loss: 0.098230 V_Loss: 0.097362 IoU: 0.7420 Best Epoch: 17 Loss: 0.097362\n",
      "18EP(250822_103525): T_Loss: 0.093684 V_Loss: 0.097614 IoU: 0.7394 \n",
      "19EP(250822_103542): T_Loss: 0.094817 V_Loss: 0.101181 IoU: 0.7341 \n",
      "20EP(250822_103600): T_Loss: 0.093266 V_Loss: 0.099615 IoU: 0.7372 \n",
      "21EP(250822_103618): T_Loss: 0.093023 V_Loss: 0.098180 IoU: 0.7392 \n",
      "22EP(250822_103635): T_Loss: 0.092014 V_Loss: 0.106426 IoU: 0.7236 \n",
      "23EP(250822_103653): T_Loss: 0.098629 V_Loss: 0.098547 IoU: 0.7399 \n",
      "24EP(250822_103711): T_Loss: 0.090755 V_Loss: 0.096873 IoU: 0.7420 Best Epoch: 24 Loss: 0.096873\n",
      "25EP(250822_103728): T_Loss: 0.090143 V_Loss: 0.097034 IoU: 0.7421 \n",
      "26EP(250822_103746): T_Loss: 0.090799 V_Loss: 0.096961 IoU: 0.7397 \n",
      "27EP(250822_103804): T_Loss: 0.088788 V_Loss: 0.093725 IoU: 0.7489 Best Epoch: 27 Loss: 0.093725\n",
      "28EP(250822_103821): T_Loss: 0.091215 V_Loss: 0.095915 IoU: 0.7442 \n",
      "29EP(250822_103839): T_Loss: 0.091576 V_Loss: 0.098568 IoU: 0.7394 \n",
      "30EP(250822_103857): T_Loss: 0.088375 V_Loss: 0.095422 IoU: 0.7451 \n",
      "31EP(250822_103914): T_Loss: 0.086890 V_Loss: 0.091948 IoU: 0.7513 Best Epoch: 31 Loss: 0.091948\n",
      "32EP(250822_103932): T_Loss: 0.091519 V_Loss: 0.095164 IoU: 0.7465 \n",
      "33EP(250822_103950): T_Loss: 0.084987 V_Loss: 0.096361 IoU: 0.7454 \n",
      "34EP(250822_104007): T_Loss: 0.085949 V_Loss: 0.093669 IoU: 0.7504 \n",
      "35EP(250822_104025): T_Loss: 0.085254 V_Loss: 0.092332 IoU: 0.7529 \n",
      "36EP(250822_104043): T_Loss: 0.084359 V_Loss: 0.093803 IoU: 0.7491 \n",
      "37EP(250822_104100): T_Loss: 0.084460 V_Loss: 0.094051 IoU: 0.7487 \n",
      "38EP(250822_104118): T_Loss: 0.083090 V_Loss: 0.094432 IoU: 0.7467 \n",
      "39EP(250822_104136): T_Loss: 0.082362 V_Loss: 0.093193 IoU: 0.7491 \n",
      "40EP(250822_104154): T_Loss: 0.081783 V_Loss: 0.092956 IoU: 0.7491 \n",
      "41EP(250822_104211): T_Loss: 0.081852 V_Loss: 0.090357 IoU: 0.7551 Best Epoch: 41 Loss: 0.090357\n",
      "42EP(250822_104229): T_Loss: 0.080208 V_Loss: 0.090600 IoU: 0.7543 \n",
      "43EP(250822_104246): T_Loss: 0.079224 V_Loss: 0.088561 IoU: 0.7592 Best Epoch: 43 Loss: 0.088561\n",
      "44EP(250822_104304): T_Loss: 0.078426 V_Loss: 0.091383 IoU: 0.7550 \n",
      "45EP(250822_104322): T_Loss: 0.080001 V_Loss: 0.094589 IoU: 0.7493 \n",
      "46EP(250822_104340): T_Loss: 0.085642 V_Loss: 0.093913 IoU: 0.7504 \n",
      "47EP(250822_104357): T_Loss: 0.080633 V_Loss: 0.087909 IoU: 0.7619 Best Epoch: 47 Loss: 0.087909\n",
      "48EP(250822_104415): T_Loss: 0.078357 V_Loss: 0.086890 IoU: 0.7637 Best Epoch: 48 Loss: 0.086890\n",
      "49EP(250822_104433): T_Loss: 0.078479 V_Loss: 0.086726 IoU: 0.7633 Best Epoch: 49 Loss: 0.086726\n",
      "50EP(250822_104450): T_Loss: 0.076707 V_Loss: 0.085797 IoU: 0.7653 Best Epoch: 50 Loss: 0.085797\n",
      "51EP(250822_104508): T_Loss: 0.076865 V_Loss: 0.086294 IoU: 0.7668 \n",
      "52EP(250822_104526): T_Loss: 0.078080 V_Loss: 0.086049 IoU: 0.7652 \n",
      "53EP(250822_104543): T_Loss: 0.078604 V_Loss: 0.090271 IoU: 0.7546 \n",
      "54EP(250822_104601): T_Loss: 0.074223 V_Loss: 0.084985 IoU: 0.7677 Best Epoch: 54 Loss: 0.084985\n",
      "55EP(250822_104619): T_Loss: 0.073808 V_Loss: 0.084957 IoU: 0.7679 Best Epoch: 55 Loss: 0.084957\n",
      "56EP(250822_104636): T_Loss: 0.073334 V_Loss: 0.084401 IoU: 0.7680 Best Epoch: 56 Loss: 0.084401\n",
      "57EP(250822_104654): T_Loss: 0.073117 V_Loss: 0.083473 IoU: 0.7711 Best Epoch: 57 Loss: 0.083473\n",
      "58EP(250822_104712): T_Loss: 0.074658 V_Loss: 0.083308 IoU: 0.7713 Best Epoch: 58 Loss: 0.083308\n",
      "59EP(250822_104729): T_Loss: 0.074099 V_Loss: 0.083916 IoU: 0.7705 \n",
      "60EP(250822_104747): T_Loss: 0.073553 V_Loss: 0.085921 IoU: 0.7665 \n",
      "61EP(250822_104805): T_Loss: 0.070872 V_Loss: 0.081765 IoU: 0.7754 Best Epoch: 61 Loss: 0.081765\n",
      "62EP(250822_104822): T_Loss: 0.071842 V_Loss: 0.081689 IoU: 0.7765 Best Epoch: 62 Loss: 0.081689\n",
      "63EP(250822_104840): T_Loss: 0.068777 V_Loss: 0.084086 IoU: 0.7705 \n",
      "64EP(250822_104858): T_Loss: 0.070647 V_Loss: 0.084235 IoU: 0.7697 \n",
      "65EP(250822_104915): T_Loss: 0.069436 V_Loss: 0.082172 IoU: 0.7753 \n",
      "66EP(250822_104933): T_Loss: 0.069627 V_Loss: 0.085388 IoU: 0.7692 \n",
      "67EP(250822_104951): T_Loss: 0.070666 V_Loss: 0.088929 IoU: 0.7595 \n",
      "68EP(250822_105008): T_Loss: 0.071484 V_Loss: 0.082053 IoU: 0.7759 \n",
      "69EP(250822_105026): T_Loss: 0.068829 V_Loss: 0.083110 IoU: 0.7735 \n",
      "70EP(250822_105044): T_Loss: 0.068771 V_Loss: 0.081009 IoU: 0.7768 Best Epoch: 70 Loss: 0.081009\n",
      "71EP(250822_105101): T_Loss: 0.067791 V_Loss: 0.079504 IoU: 0.7812 Best Epoch: 71 Loss: 0.079504\n",
      "72EP(250822_105119): T_Loss: 0.067601 V_Loss: 0.080573 IoU: 0.7788 \n",
      "73EP(250822_105137): T_Loss: 0.067528 V_Loss: 0.081169 IoU: 0.7789 \n",
      "74EP(250822_105154): T_Loss: 0.067367 V_Loss: 0.081803 IoU: 0.7773 \n",
      "75EP(250822_105212): T_Loss: 0.067293 V_Loss: 0.081880 IoU: 0.7771 \n",
      "76EP(250822_105230): T_Loss: 0.065490 V_Loss: 0.078914 IoU: 0.7833 Best Epoch: 76 Loss: 0.078914\n",
      "77EP(250822_105247): T_Loss: 0.066554 V_Loss: 0.079976 IoU: 0.7810 \n",
      "78EP(250822_105305): T_Loss: 0.067236 V_Loss: 0.079936 IoU: 0.7809 \n",
      "79EP(250822_105323): T_Loss: 0.066924 V_Loss: 0.080285 IoU: 0.7802 \n",
      "80EP(250822_105340): T_Loss: 0.064089 V_Loss: 0.078251 IoU: 0.7848 Best Epoch: 80 Loss: 0.078251\n",
      "81EP(250822_105358): T_Loss: 0.063713 V_Loss: 0.079381 IoU: 0.7822 \n",
      "82EP(250822_105416): T_Loss: 0.064100 V_Loss: 0.079095 IoU: 0.7830 \n",
      "83EP(250822_105434): T_Loss: 0.064310 V_Loss: 0.079605 IoU: 0.7817 \n",
      "84EP(250822_105451): T_Loss: 0.063768 V_Loss: 0.078910 IoU: 0.7834 \n",
      "85EP(250822_105509): T_Loss: 0.063193 V_Loss: 0.080245 IoU: 0.7810 \n",
      "86EP(250822_105527): T_Loss: 0.063554 V_Loss: 0.079236 IoU: 0.7833 \n",
      "87EP(250822_105544): T_Loss: 0.063048 V_Loss: 0.079254 IoU: 0.7827 \n",
      "88EP(250822_105602): T_Loss: 0.063214 V_Loss: 0.079275 IoU: 0.7831 \n",
      "89EP(250822_105620): T_Loss: 0.062334 V_Loss: 0.079190 IoU: 0.7833 \n",
      "90EP(250822_105637): T_Loss: 0.062682 V_Loss: 0.079340 IoU: 0.7830 \n",
      "91EP(250822_105655): T_Loss: 0.062363 V_Loss: 0.079220 IoU: 0.7835 \n",
      "92EP(250822_105713): T_Loss: 0.062309 V_Loss: 0.079318 IoU: 0.7833 \n",
      "93EP(250822_105730): T_Loss: 0.062801 V_Loss: 0.078899 IoU: 0.7841 \n",
      "94EP(250822_105748): T_Loss: 0.062078 V_Loss: 0.078776 IoU: 0.7845 \n",
      "95EP(250822_105806): T_Loss: 0.062773 V_Loss: 0.078923 IoU: 0.7840 \n",
      "96EP(250822_105823): T_Loss: 0.061390 V_Loss: 0.079011 IoU: 0.7839 \n",
      "97EP(250822_105841): T_Loss: 0.062594 V_Loss: 0.078947 IoU: 0.7840 \n",
      "98EP(250822_105859): T_Loss: 0.061739 V_Loss: 0.078888 IoU: 0.7842 \n",
      "99EP(250822_105917): T_Loss: 0.062439 V_Loss: 0.078865 IoU: 0.7842 \n",
      "100EP(250822_105934): T_Loss: 0.062278 V_Loss: 0.078882 IoU: 0.7842 \n",
      "Test Start Time: 250822_105934\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58ad3305a1524113bee1db0f7e7b69a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test:   0%|          | 0/27 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Epoch: 80\n",
      "Test(250822_105940): Loss: 0.082663 IoU: 0.7704 Dice: 0.8654 Precision: 0.8658 Recall: 0.8769\n",
      "End 250822_105940\n",
      "(Iter 5)\n",
      "Dataset: CS01. Ceramic (1/5)\n",
      "Preprocessing: None\n",
      "Augmentation: FLIP(HV)=>ROT([90,180,270])\n",
      "Evaluation: None\n",
      "train/val/test: 60/20/20\n",
      "UNet_MS_IG (1/1) (Iter 5) Dataset: CS01. Ceramic (Shape: (256, 256, 3)) (1/5) Data Split: 60:20:20\n",
      "  -> Ablation: use_ms=True_use_gate=True\n",
      "Training Start Time: 250822_105940\n",
      "1EP(250822_105941): T_Loss: 0.576700 V_Loss: 0.497400 IoU: 0.0008 Best Epoch: 1 Loss: 0.497400\n",
      "2EP(250822_105943): T_Loss: 0.468722 V_Loss: 0.403964 IoU: 0.2412 Best Epoch: 2 Loss: 0.403964\n",
      "3EP(250822_105944): T_Loss: 0.430564 V_Loss: 0.370475 IoU: 0.2676 Best Epoch: 3 Loss: 0.370475\n",
      "4EP(250822_105945): T_Loss: 0.402337 V_Loss: 0.366309 IoU: 0.2551 Best Epoch: 4 Loss: 0.366309\n",
      "5EP(250822_105946): T_Loss: 0.375449 V_Loss: 0.354853 IoU: 0.2726 Best Epoch: 5 Loss: 0.354853\n",
      "6EP(250822_105947): T_Loss: 0.375682 V_Loss: 0.321193 IoU: 0.3183 Best Epoch: 6 Loss: 0.321193\n",
      "7EP(250822_105948): T_Loss: 0.358830 V_Loss: 0.323481 IoU: 0.3198 \n",
      "8EP(250822_105949): T_Loss: 0.357139 V_Loss: 0.311366 IoU: 0.3356 Best Epoch: 8 Loss: 0.311366\n",
      "9EP(250822_105950): T_Loss: 0.341108 V_Loss: 0.327873 IoU: 0.3071 \n",
      "10EP(250822_105951): T_Loss: 0.336702 V_Loss: 0.310217 IoU: 0.3376 Best Epoch: 10 Loss: 0.310217\n",
      "11EP(250822_105953): T_Loss: 0.337402 V_Loss: 0.347355 IoU: 0.2804 \n",
      "12EP(250822_105954): T_Loss: 0.328136 V_Loss: 0.299234 IoU: 0.3569 Best Epoch: 12 Loss: 0.299234\n",
      "13EP(250822_105955): T_Loss: 0.317410 V_Loss: 0.302621 IoU: 0.3513 \n",
      "14EP(250822_105956): T_Loss: 0.312187 V_Loss: 0.305032 IoU: 0.3481 \n",
      "15EP(250822_105957): T_Loss: 0.309409 V_Loss: 0.308460 IoU: 0.3417 \n",
      "16EP(250822_105958): T_Loss: 0.315644 V_Loss: 0.302915 IoU: 0.3489 \n",
      "17EP(250822_105959): T_Loss: 0.315238 V_Loss: 0.301544 IoU: 0.3523 \n",
      "18EP(250822_110000): T_Loss: 0.310378 V_Loss: 0.309724 IoU: 0.3372 \n",
      "19EP(250822_110002): T_Loss: 0.302822 V_Loss: 0.301166 IoU: 0.3511 \n",
      "20EP(250822_110003): T_Loss: 0.286468 V_Loss: 0.301368 IoU: 0.3476 \n",
      "21EP(250822_110004): T_Loss: 0.275400 V_Loss: 0.296793 IoU: 0.3574 Best Epoch: 21 Loss: 0.296793\n",
      "22EP(250822_110005): T_Loss: 0.288091 V_Loss: 0.284260 IoU: 0.3788 Best Epoch: 22 Loss: 0.284260\n",
      "23EP(250822_110006): T_Loss: 0.260343 V_Loss: 0.298472 IoU: 0.3562 \n",
      "24EP(250822_110007): T_Loss: 0.284294 V_Loss: 0.273999 IoU: 0.3953 Best Epoch: 24 Loss: 0.273999\n",
      "25EP(250822_110008): T_Loss: 0.274709 V_Loss: 0.295968 IoU: 0.3587 \n",
      "26EP(250822_110009): T_Loss: 0.273839 V_Loss: 0.284567 IoU: 0.3805 \n",
      "27EP(250822_110011): T_Loss: 0.262839 V_Loss: 0.291271 IoU: 0.3667 \n",
      "28EP(250822_110012): T_Loss: 0.253824 V_Loss: 0.294994 IoU: 0.3656 \n",
      "29EP(250822_110013): T_Loss: 0.256525 V_Loss: 0.282740 IoU: 0.3809 \n",
      "30EP(250822_110014): T_Loss: 0.260840 V_Loss: 0.292323 IoU: 0.3655 \n",
      "31EP(250822_110015): T_Loss: 0.262162 V_Loss: 0.283304 IoU: 0.3789 \n",
      "32EP(250822_110016): T_Loss: 0.260031 V_Loss: 0.284780 IoU: 0.3765 \n",
      "33EP(250822_110017): T_Loss: 0.244873 V_Loss: 0.281361 IoU: 0.3839 \n",
      "34EP(250822_110018): T_Loss: 0.257619 V_Loss: 0.297638 IoU: 0.3625 \n",
      "35EP(250822_110020): T_Loss: 0.235400 V_Loss: 0.287142 IoU: 0.3742 \n",
      "36EP(250822_110021): T_Loss: 0.244566 V_Loss: 0.299186 IoU: 0.3612 \n",
      "37EP(250822_110022): T_Loss: 0.234384 V_Loss: 0.270639 IoU: 0.3986 Best Epoch: 37 Loss: 0.270639\n",
      "38EP(250822_110023): T_Loss: 0.236684 V_Loss: 0.274821 IoU: 0.3930 \n",
      "39EP(250822_110024): T_Loss: 0.248091 V_Loss: 0.301753 IoU: 0.3494 \n",
      "40EP(250822_110025): T_Loss: 0.222429 V_Loss: 0.280198 IoU: 0.3831 \n",
      "41EP(250822_110026): T_Loss: 0.225121 V_Loss: 0.295686 IoU: 0.3585 \n",
      "42EP(250822_110027): T_Loss: 0.227054 V_Loss: 0.279905 IoU: 0.3867 \n",
      "43EP(250822_110028): T_Loss: 0.212014 V_Loss: 0.290789 IoU: 0.3652 \n",
      "44EP(250822_110030): T_Loss: 0.219404 V_Loss: 0.272999 IoU: 0.3972 \n",
      "45EP(250822_110031): T_Loss: 0.205600 V_Loss: 0.308422 IoU: 0.3519 \n",
      "46EP(250822_110032): T_Loss: 0.217191 V_Loss: 0.297720 IoU: 0.3598 \n",
      "47EP(250822_110033): T_Loss: 0.208503 V_Loss: 0.293272 IoU: 0.3724 \n",
      "48EP(250822_110034): T_Loss: 0.212164 V_Loss: 0.321921 IoU: 0.3305 \n",
      "49EP(250822_110035): T_Loss: 0.189439 V_Loss: 0.297698 IoU: 0.3576 \n",
      "50EP(250822_110036): T_Loss: 0.196070 V_Loss: 0.301075 IoU: 0.3580 \n",
      "51EP(250822_110037): T_Loss: 0.194290 V_Loss: 0.278016 IoU: 0.3889 \n",
      "52EP(250822_110039): T_Loss: 0.179783 V_Loss: 0.302535 IoU: 0.3595 \n",
      "53EP(250822_110040): T_Loss: 0.188982 V_Loss: 0.300517 IoU: 0.3598 \n",
      "54EP(250822_110041): T_Loss: 0.177941 V_Loss: 0.282602 IoU: 0.3862 \n",
      "55EP(250822_110042): T_Loss: 0.181218 V_Loss: 0.295136 IoU: 0.3660 \n",
      "56EP(250822_110043): T_Loss: 0.177476 V_Loss: 0.313529 IoU: 0.3454 \n",
      "57EP(250822_110044): T_Loss: 0.181498 V_Loss: 0.305524 IoU: 0.3494 \n",
      "58EP(250822_110045): T_Loss: 0.168922 V_Loss: 0.294133 IoU: 0.3683 \n",
      "59EP(250822_110046): T_Loss: 0.160283 V_Loss: 0.302805 IoU: 0.3582 \n",
      "60EP(250822_110047): T_Loss: 0.162490 V_Loss: 0.304249 IoU: 0.3610 \n",
      "61EP(250822_110049): T_Loss: 0.175080 V_Loss: 0.297557 IoU: 0.3599 \n",
      "62EP(250822_110050): T_Loss: 0.148430 V_Loss: 0.293395 IoU: 0.3679 \n",
      "63EP(250822_110051): T_Loss: 0.155913 V_Loss: 0.323241 IoU: 0.3332 \n",
      "64EP(250822_110052): T_Loss: 0.170265 V_Loss: 0.292526 IoU: 0.3748 \n",
      "65EP(250822_110053): T_Loss: 0.155816 V_Loss: 0.299320 IoU: 0.3626 \n",
      "66EP(250822_110054): T_Loss: 0.166079 V_Loss: 0.286751 IoU: 0.3782 \n",
      "67EP(250822_110055): T_Loss: 0.158728 V_Loss: 0.309219 IoU: 0.3474 \n",
      "68EP(250822_110056): T_Loss: 0.137802 V_Loss: 0.321133 IoU: 0.3365 \n",
      "69EP(250822_110058): T_Loss: 0.161363 V_Loss: 0.297542 IoU: 0.3625 \n",
      "70EP(250822_110059): T_Loss: 0.150264 V_Loss: 0.299153 IoU: 0.3652 \n",
      "71EP(250822_110100): T_Loss: 0.138063 V_Loss: 0.295132 IoU: 0.3700 \n",
      "72EP(250822_110101): T_Loss: 0.143643 V_Loss: 0.300145 IoU: 0.3629 \n",
      "73EP(250822_110102): T_Loss: 0.120036 V_Loss: 0.302849 IoU: 0.3596 \n",
      "74EP(250822_110103): T_Loss: 0.138285 V_Loss: 0.296218 IoU: 0.3685 \n",
      "75EP(250822_110104): T_Loss: 0.133525 V_Loss: 0.294384 IoU: 0.3701 \n",
      "76EP(250822_110105): T_Loss: 0.147217 V_Loss: 0.294277 IoU: 0.3718 \n",
      "77EP(250822_110106): T_Loss: 0.143400 V_Loss: 0.294388 IoU: 0.3704 \n",
      "78EP(250822_110108): T_Loss: 0.130639 V_Loss: 0.301848 IoU: 0.3586 \n",
      "79EP(250822_110109): T_Loss: 0.128880 V_Loss: 0.303184 IoU: 0.3567 \n",
      "80EP(250822_110110): T_Loss: 0.128999 V_Loss: 0.300269 IoU: 0.3656 \n",
      "81EP(250822_110111): T_Loss: 0.134878 V_Loss: 0.301653 IoU: 0.3629 \n",
      "82EP(250822_110112): T_Loss: 0.125804 V_Loss: 0.297350 IoU: 0.3687 \n",
      "83EP(250822_110113): T_Loss: 0.127244 V_Loss: 0.303463 IoU: 0.3599 \n",
      "84EP(250822_110114): T_Loss: 0.128902 V_Loss: 0.296808 IoU: 0.3669 \n",
      "85EP(250822_110115): T_Loss: 0.125279 V_Loss: 0.300197 IoU: 0.3640 \n",
      "86EP(250822_110116): T_Loss: 0.115382 V_Loss: 0.298553 IoU: 0.3679 \n",
      "87EP(250822_110118): T_Loss: 0.127129 V_Loss: 0.304654 IoU: 0.3594 \n",
      "88EP(250822_110119): T_Loss: 0.122785 V_Loss: 0.304708 IoU: 0.3576 \n",
      "89EP(250822_110120): T_Loss: 0.118244 V_Loss: 0.302142 IoU: 0.3609 \n",
      "90EP(250822_110121): T_Loss: 0.122670 V_Loss: 0.301510 IoU: 0.3616 \n",
      "91EP(250822_110122): T_Loss: 0.125404 V_Loss: 0.305396 IoU: 0.3570 \n",
      "92EP(250822_110123): T_Loss: 0.124254 V_Loss: 0.305451 IoU: 0.3563 \n",
      "93EP(250822_110124): T_Loss: 0.123046 V_Loss: 0.305342 IoU: 0.3564 \n",
      "94EP(250822_110125): T_Loss: 0.109544 V_Loss: 0.304821 IoU: 0.3578 \n",
      "95EP(250822_110126): T_Loss: 0.120634 V_Loss: 0.305026 IoU: 0.3571 \n",
      "96EP(250822_110128): T_Loss: 0.111780 V_Loss: 0.305021 IoU: 0.3576 \n",
      "97EP(250822_110129): T_Loss: 0.114575 V_Loss: 0.304961 IoU: 0.3579 \n",
      "98EP(250822_110130): T_Loss: 0.126536 V_Loss: 0.305296 IoU: 0.3574 \n",
      "99EP(250822_110131): T_Loss: 0.116104 V_Loss: 0.305405 IoU: 0.3573 \n",
      "100EP(250822_110132): T_Loss: 0.139097 V_Loss: 0.305636 IoU: 0.3570 \n",
      "Test Start Time: 250822_110132\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf19de12675d44e6bc23dbfd8ab41b1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test:   0%|          | 0/5 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Epoch: 37\n",
      "Test(250822_110132): Loss: 0.321122 IoU: 0.2983 Dice: 0.4370 Precision: 0.4104 Recall: 0.5116\n",
      "End 250822_110132\n",
      "Dataset: CS02. CFD (2/5)\n",
      "Preprocessing: HWR(384,512)=>P(384,512)\n",
      "Augmentation: FLIP(HV)=>ROT([90,180,270])\n",
      "Evaluation: None\n",
      "train/val/test: 70/23/25\n",
      "UNet_MS_IG (1/1) (Iter 5) Dataset: CS02. CFD (Shape: (384, 512, 3)) (2/5) Data Split: 60:20:20\n",
      "  -> Ablation: use_ms=True_use_gate=True\n",
      "Training Start Time: 250822_110132\n",
      "1EP(250822_110136): T_Loss: 0.538837 V_Loss: 0.417942 IoU: 0.2973 Best Epoch: 1 Loss: 0.417942\n",
      "2EP(250822_110139): T_Loss: 0.361133 V_Loss: 0.298550 IoU: 0.3748 Best Epoch: 2 Loss: 0.298550\n",
      "3EP(250822_110142): T_Loss: 0.282767 V_Loss: 0.274012 IoU: 0.3798 Best Epoch: 3 Loss: 0.274012\n",
      "4EP(250822_110146): T_Loss: 0.251773 V_Loss: 0.231088 IoU: 0.4484 Best Epoch: 4 Loss: 0.231088\n",
      "5EP(250822_110149): T_Loss: 0.225446 V_Loss: 0.225702 IoU: 0.4489 Best Epoch: 5 Loss: 0.225702\n",
      "6EP(250822_110152): T_Loss: 0.216863 V_Loss: 0.205462 IoU: 0.4936 Best Epoch: 6 Loss: 0.205462\n",
      "7EP(250822_110155): T_Loss: 0.206985 V_Loss: 0.205895 IoU: 0.4835 \n",
      "8EP(250822_110159): T_Loss: 0.198413 V_Loss: 0.197660 IoU: 0.5005 Best Epoch: 8 Loss: 0.197660\n",
      "9EP(250822_110202): T_Loss: 0.184777 V_Loss: 0.204111 IoU: 0.4932 \n",
      "10EP(250822_110205): T_Loss: 0.200935 V_Loss: 0.196367 IoU: 0.4993 Best Epoch: 10 Loss: 0.196367\n",
      "11EP(250822_110208): T_Loss: 0.183521 V_Loss: 0.189067 IoU: 0.5166 Best Epoch: 11 Loss: 0.189067\n",
      "12EP(250822_110212): T_Loss: 0.182130 V_Loss: 0.187023 IoU: 0.5189 Best Epoch: 12 Loss: 0.187023\n",
      "13EP(250822_110215): T_Loss: 0.175938 V_Loss: 0.183899 IoU: 0.5256 Best Epoch: 13 Loss: 0.183899\n",
      "14EP(250822_110218): T_Loss: 0.171144 V_Loss: 0.189038 IoU: 0.5168 \n",
      "15EP(250822_110222): T_Loss: 0.175223 V_Loss: 0.188075 IoU: 0.5133 \n",
      "16EP(250822_110225): T_Loss: 0.170360 V_Loss: 0.179692 IoU: 0.5298 Best Epoch: 16 Loss: 0.179692\n",
      "17EP(250822_110228): T_Loss: 0.170175 V_Loss: 0.184628 IoU: 0.5202 \n",
      "18EP(250822_110231): T_Loss: 0.169372 V_Loss: 0.188496 IoU: 0.5128 \n",
      "19EP(250822_110235): T_Loss: 0.168999 V_Loss: 0.184046 IoU: 0.5235 \n",
      "20EP(250822_110238): T_Loss: 0.163322 V_Loss: 0.185789 IoU: 0.5177 \n",
      "21EP(250822_110241): T_Loss: 0.164588 V_Loss: 0.180392 IoU: 0.5293 \n",
      "22EP(250822_110244): T_Loss: 0.162121 V_Loss: 0.177382 IoU: 0.5344 Best Epoch: 22 Loss: 0.177382\n",
      "23EP(250822_110247): T_Loss: 0.161720 V_Loss: 0.181297 IoU: 0.5295 \n",
      "24EP(250822_110251): T_Loss: 0.162257 V_Loss: 0.191090 IoU: 0.5074 \n",
      "25EP(250822_110254): T_Loss: 0.158547 V_Loss: 0.183998 IoU: 0.5211 \n",
      "26EP(250822_110257): T_Loss: 0.158062 V_Loss: 0.185001 IoU: 0.5193 \n",
      "27EP(250822_110300): T_Loss: 0.155087 V_Loss: 0.183679 IoU: 0.5234 \n",
      "28EP(250822_110304): T_Loss: 0.159358 V_Loss: 0.184393 IoU: 0.5200 \n",
      "29EP(250822_110307): T_Loss: 0.152397 V_Loss: 0.180546 IoU: 0.5281 \n",
      "30EP(250822_110310): T_Loss: 0.154440 V_Loss: 0.182918 IoU: 0.5231 \n",
      "31EP(250822_110313): T_Loss: 0.152864 V_Loss: 0.184745 IoU: 0.5188 \n",
      "32EP(250822_110317): T_Loss: 0.156464 V_Loss: 0.181522 IoU: 0.5288 \n",
      "33EP(250822_110320): T_Loss: 0.146692 V_Loss: 0.180267 IoU: 0.5295 \n",
      "34EP(250822_110323): T_Loss: 0.150479 V_Loss: 0.181250 IoU: 0.5268 \n",
      "35EP(250822_110326): T_Loss: 0.143460 V_Loss: 0.183883 IoU: 0.5211 \n",
      "36EP(250822_110330): T_Loss: 0.146904 V_Loss: 0.182738 IoU: 0.5247 \n",
      "37EP(250822_110333): T_Loss: 0.146997 V_Loss: 0.176656 IoU: 0.5371 Best Epoch: 37 Loss: 0.176656\n",
      "38EP(250822_110336): T_Loss: 0.148335 V_Loss: 0.177442 IoU: 0.5348 \n",
      "39EP(250822_110339): T_Loss: 0.141103 V_Loss: 0.180768 IoU: 0.5289 \n",
      "40EP(250822_110343): T_Loss: 0.145826 V_Loss: 0.185641 IoU: 0.5172 \n",
      "41EP(250822_110346): T_Loss: 0.141662 V_Loss: 0.184093 IoU: 0.5207 \n",
      "42EP(250822_110349): T_Loss: 0.141174 V_Loss: 0.186839 IoU: 0.5163 \n",
      "43EP(250822_110352): T_Loss: 0.139796 V_Loss: 0.179568 IoU: 0.5310 \n",
      "44EP(250822_110356): T_Loss: 0.143310 V_Loss: 0.180005 IoU: 0.5304 \n",
      "45EP(250822_110359): T_Loss: 0.137327 V_Loss: 0.182461 IoU: 0.5252 \n",
      "46EP(250822_110402): T_Loss: 0.142157 V_Loss: 0.179856 IoU: 0.5293 \n",
      "47EP(250822_110405): T_Loss: 0.141711 V_Loss: 0.180530 IoU: 0.5299 \n",
      "48EP(250822_110409): T_Loss: 0.134158 V_Loss: 0.182446 IoU: 0.5247 \n",
      "49EP(250822_110412): T_Loss: 0.127320 V_Loss: 0.185807 IoU: 0.5186 \n",
      "50EP(250822_110415): T_Loss: 0.127068 V_Loss: 0.178732 IoU: 0.5333 \n",
      "51EP(250822_110419): T_Loss: 0.121086 V_Loss: 0.178511 IoU: 0.5308 \n",
      "52EP(250822_110422): T_Loss: 0.121839 V_Loss: 0.185690 IoU: 0.5195 \n",
      "53EP(250822_110425): T_Loss: 0.127152 V_Loss: 0.179652 IoU: 0.5295 \n",
      "54EP(250822_110428): T_Loss: 0.123090 V_Loss: 0.181060 IoU: 0.5272 \n",
      "55EP(250822_110432): T_Loss: 0.122803 V_Loss: 0.186815 IoU: 0.5181 \n",
      "56EP(250822_110435): T_Loss: 0.119203 V_Loss: 0.185557 IoU: 0.5196 \n",
      "57EP(250822_110438): T_Loss: 0.119799 V_Loss: 0.183587 IoU: 0.5233 \n",
      "58EP(250822_110441): T_Loss: 0.112814 V_Loss: 0.183695 IoU: 0.5243 \n",
      "59EP(250822_110444): T_Loss: 0.106975 V_Loss: 0.187064 IoU: 0.5181 \n",
      "60EP(250822_110448): T_Loss: 0.114586 V_Loss: 0.185595 IoU: 0.5196 \n",
      "61EP(250822_110451): T_Loss: 0.115385 V_Loss: 0.197275 IoU: 0.4967 \n",
      "62EP(250822_110454): T_Loss: 0.112139 V_Loss: 0.185193 IoU: 0.5209 \n",
      "63EP(250822_110457): T_Loss: 0.111219 V_Loss: 0.185409 IoU: 0.5199 \n",
      "64EP(250822_110501): T_Loss: 0.111389 V_Loss: 0.189443 IoU: 0.5139 \n",
      "65EP(250822_110504): T_Loss: 0.115093 V_Loss: 0.185879 IoU: 0.5197 \n",
      "66EP(250822_110507): T_Loss: 0.105931 V_Loss: 0.186868 IoU: 0.5188 \n",
      "67EP(250822_110511): T_Loss: 0.100639 V_Loss: 0.189267 IoU: 0.5151 \n",
      "68EP(250822_110514): T_Loss: 0.097608 V_Loss: 0.189080 IoU: 0.5147 \n",
      "69EP(250822_110517): T_Loss: 0.110880 V_Loss: 0.189647 IoU: 0.5134 \n",
      "70EP(250822_110520): T_Loss: 0.101799 V_Loss: 0.187802 IoU: 0.5171 \n",
      "71EP(250822_110524): T_Loss: 0.101400 V_Loss: 0.191808 IoU: 0.5085 \n",
      "72EP(250822_110527): T_Loss: 0.105279 V_Loss: 0.191343 IoU: 0.5101 \n",
      "73EP(250822_110530): T_Loss: 0.091515 V_Loss: 0.189782 IoU: 0.5135 \n",
      "74EP(250822_110533): T_Loss: 0.107091 V_Loss: 0.191821 IoU: 0.5090 \n",
      "75EP(250822_110537): T_Loss: 0.098456 V_Loss: 0.190687 IoU: 0.5101 \n",
      "76EP(250822_110540): T_Loss: 0.103219 V_Loss: 0.191308 IoU: 0.5106 \n",
      "77EP(250822_110543): T_Loss: 0.099511 V_Loss: 0.190246 IoU: 0.5111 \n",
      "78EP(250822_110546): T_Loss: 0.095617 V_Loss: 0.193444 IoU: 0.5076 \n",
      "79EP(250822_110550): T_Loss: 0.099920 V_Loss: 0.192430 IoU: 0.5082 \n",
      "80EP(250822_110553): T_Loss: 0.090558 V_Loss: 0.191840 IoU: 0.5103 \n",
      "81EP(250822_110556): T_Loss: 0.092091 V_Loss: 0.192479 IoU: 0.5090 \n",
      "82EP(250822_110559): T_Loss: 0.094771 V_Loss: 0.192083 IoU: 0.5095 \n",
      "83EP(250822_110603): T_Loss: 0.087432 V_Loss: 0.192508 IoU: 0.5088 \n",
      "84EP(250822_110606): T_Loss: 0.091640 V_Loss: 0.192193 IoU: 0.5097 \n",
      "85EP(250822_110609): T_Loss: 0.090474 V_Loss: 0.191953 IoU: 0.5099 \n",
      "86EP(250822_110613): T_Loss: 0.088246 V_Loss: 0.193622 IoU: 0.5074 \n",
      "87EP(250822_110616): T_Loss: 0.086631 V_Loss: 0.192554 IoU: 0.5101 \n",
      "88EP(250822_110619): T_Loss: 0.079839 V_Loss: 0.193651 IoU: 0.5078 \n",
      "89EP(250822_110622): T_Loss: 0.086948 V_Loss: 0.192877 IoU: 0.5088 \n",
      "90EP(250822_110626): T_Loss: 0.083412 V_Loss: 0.193179 IoU: 0.5092 \n",
      "91EP(250822_110629): T_Loss: 0.088420 V_Loss: 0.193079 IoU: 0.5090 \n",
      "92EP(250822_110632): T_Loss: 0.090270 V_Loss: 0.192775 IoU: 0.5094 \n",
      "93EP(250822_110635): T_Loss: 0.096611 V_Loss: 0.193423 IoU: 0.5081 \n",
      "94EP(250822_110639): T_Loss: 0.083541 V_Loss: 0.193178 IoU: 0.5086 \n",
      "95EP(250822_110642): T_Loss: 0.083340 V_Loss: 0.193147 IoU: 0.5090 \n",
      "96EP(250822_110645): T_Loss: 0.081635 V_Loss: 0.193339 IoU: 0.5089 \n",
      "97EP(250822_110648): T_Loss: 0.086753 V_Loss: 0.193358 IoU: 0.5089 \n",
      "98EP(250822_110652): T_Loss: 0.091032 V_Loss: 0.193269 IoU: 0.5091 \n",
      "99EP(250822_110655): T_Loss: 0.084580 V_Loss: 0.193245 IoU: 0.5092 \n",
      "100EP(250822_110658): T_Loss: 0.091019 V_Loss: 0.193234 IoU: 0.5092 \n",
      "Test Start Time: 250822_110658\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b33a07f113664a648173e76462a709c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test:   0%|          | 0/7 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Epoch: 37\n",
      "Test(250822_110659): Loss: 0.193260 IoU: 0.5087 Dice: 0.6586 Precision: 0.6082 Recall: 0.7285\n",
      "End 250822_110659\n",
      "Dataset: CS03. DeepCrack237 (3/5)\n",
      "Preprocessing: P(384,640)\n",
      "Augmentation: FLIP(HV)=>ROT([90,180,270])\n",
      "Evaluation: None\n",
      "train/val/test: 142/47/48\n",
      "UNet_MS_IG (1/1) (Iter 5) Dataset: CS03. DeepCrack237 (Shape: (384, 640, 3)) (3/5) Data Split: 60:20:20\n",
      "  -> Ablation: use_ms=True_use_gate=True\n",
      "Training Start Time: 250822_110659\n",
      "1EP(250822_110707): T_Loss: 0.341657 V_Loss: 0.227518 IoU: 0.5501 Best Epoch: 1 Loss: 0.227518\n",
      "2EP(250822_110715): T_Loss: 0.171723 V_Loss: 0.173155 IoU: 0.6327 Best Epoch: 2 Loss: 0.173155\n",
      "3EP(250822_110723): T_Loss: 0.130327 V_Loss: 0.125691 IoU: 0.7073 Best Epoch: 3 Loss: 0.125691\n",
      "4EP(250822_110731): T_Loss: 0.112345 V_Loss: 0.113973 IoU: 0.7294 Best Epoch: 4 Loss: 0.113973\n",
      "5EP(250822_110739): T_Loss: 0.119232 V_Loss: 0.125163 IoU: 0.7114 \n",
      "6EP(250822_110747): T_Loss: 0.116594 V_Loss: 0.109272 IoU: 0.7361 Best Epoch: 6 Loss: 0.109272\n",
      "7EP(250822_110755): T_Loss: 0.109892 V_Loss: 0.106927 IoU: 0.7418 Best Epoch: 7 Loss: 0.106927\n",
      "8EP(250822_110803): T_Loss: 0.101295 V_Loss: 0.105603 IoU: 0.7402 Best Epoch: 8 Loss: 0.105603\n",
      "9EP(250822_110811): T_Loss: 0.097748 V_Loss: 0.113541 IoU: 0.7356 \n",
      "10EP(250822_110819): T_Loss: 0.096428 V_Loss: 0.098281 IoU: 0.7588 Best Epoch: 10 Loss: 0.098281\n",
      "11EP(250822_110827): T_Loss: 0.089376 V_Loss: 0.097870 IoU: 0.7603 Best Epoch: 11 Loss: 0.097870\n",
      "12EP(250822_110835): T_Loss: 0.100119 V_Loss: 0.096337 IoU: 0.7635 Best Epoch: 12 Loss: 0.096337\n",
      "13EP(250822_110843): T_Loss: 0.088368 V_Loss: 0.091495 IoU: 0.7740 Best Epoch: 13 Loss: 0.091495\n",
      "14EP(250822_110851): T_Loss: 0.082142 V_Loss: 0.092362 IoU: 0.7712 \n",
      "15EP(250822_110859): T_Loss: 0.081103 V_Loss: 0.091232 IoU: 0.7707 Best Epoch: 15 Loss: 0.091232\n",
      "16EP(250822_110907): T_Loss: 0.079687 V_Loss: 0.092323 IoU: 0.7701 \n",
      "17EP(250822_110915): T_Loss: 0.085705 V_Loss: 0.089721 IoU: 0.7757 Best Epoch: 17 Loss: 0.089721\n",
      "18EP(250822_110923): T_Loss: 0.088700 V_Loss: 0.099618 IoU: 0.7547 \n",
      "19EP(250822_110931): T_Loss: 0.086999 V_Loss: 0.089829 IoU: 0.7737 \n",
      "20EP(250822_110939): T_Loss: 0.075708 V_Loss: 0.086894 IoU: 0.7835 Best Epoch: 20 Loss: 0.086894\n",
      "21EP(250822_110947): T_Loss: 0.075934 V_Loss: 0.093429 IoU: 0.7727 \n",
      "22EP(250822_110955): T_Loss: 0.084241 V_Loss: 0.093482 IoU: 0.7711 \n",
      "23EP(250822_111003): T_Loss: 0.081685 V_Loss: 0.103075 IoU: 0.7509 \n",
      "24EP(250822_111011): T_Loss: 0.074151 V_Loss: 0.087988 IoU: 0.7803 \n",
      "25EP(250822_111019): T_Loss: 0.074736 V_Loss: 0.086637 IoU: 0.7822 Best Epoch: 25 Loss: 0.086637\n",
      "26EP(250822_111027): T_Loss: 0.076177 V_Loss: 0.113708 IoU: 0.7375 \n",
      "27EP(250822_111035): T_Loss: 0.079508 V_Loss: 0.091891 IoU: 0.7746 \n",
      "28EP(250822_111043): T_Loss: 0.077647 V_Loss: 0.093987 IoU: 0.7698 \n",
      "29EP(250822_111051): T_Loss: 0.066848 V_Loss: 0.085773 IoU: 0.7887 Best Epoch: 29 Loss: 0.085773\n",
      "30EP(250822_111059): T_Loss: 0.069558 V_Loss: 0.084951 IoU: 0.7845 Best Epoch: 30 Loss: 0.084951\n",
      "31EP(250822_111107): T_Loss: 0.068880 V_Loss: 0.085333 IoU: 0.7861 \n",
      "32EP(250822_111115): T_Loss: 0.065138 V_Loss: 0.083520 IoU: 0.7894 Best Epoch: 32 Loss: 0.083520\n",
      "33EP(250822_111123): T_Loss: 0.066214 V_Loss: 0.083065 IoU: 0.7890 Best Epoch: 33 Loss: 0.083065\n",
      "34EP(250822_111131): T_Loss: 0.066022 V_Loss: 0.084212 IoU: 0.7884 \n",
      "35EP(250822_111139): T_Loss: 0.069234 V_Loss: 0.084515 IoU: 0.7856 \n",
      "36EP(250822_111147): T_Loss: 0.066903 V_Loss: 0.081399 IoU: 0.7943 Best Epoch: 36 Loss: 0.081399\n",
      "37EP(250822_111155): T_Loss: 0.066150 V_Loss: 0.081063 IoU: 0.7940 Best Epoch: 37 Loss: 0.081063\n",
      "38EP(250822_111203): T_Loss: 0.062721 V_Loss: 0.082572 IoU: 0.7907 \n",
      "39EP(250822_111211): T_Loss: 0.061723 V_Loss: 0.083171 IoU: 0.7871 \n",
      "40EP(250822_111219): T_Loss: 0.063148 V_Loss: 0.081495 IoU: 0.7927 \n",
      "41EP(250822_111227): T_Loss: 0.065977 V_Loss: 0.086291 IoU: 0.7839 \n",
      "42EP(250822_111235): T_Loss: 0.072241 V_Loss: 0.083582 IoU: 0.7894 \n",
      "43EP(250822_111243): T_Loss: 0.066965 V_Loss: 0.080638 IoU: 0.7942 Best Epoch: 43 Loss: 0.080638\n",
      "44EP(250822_111251): T_Loss: 0.063777 V_Loss: 0.079288 IoU: 0.7958 Best Epoch: 44 Loss: 0.079288\n",
      "45EP(250822_111259): T_Loss: 0.061224 V_Loss: 0.087493 IoU: 0.7818 \n",
      "46EP(250822_111307): T_Loss: 0.061312 V_Loss: 0.082482 IoU: 0.7894 \n",
      "47EP(250822_111315): T_Loss: 0.060995 V_Loss: 0.079378 IoU: 0.7946 \n",
      "48EP(250822_111323): T_Loss: 0.057434 V_Loss: 0.080166 IoU: 0.7958 \n",
      "49EP(250822_111331): T_Loss: 0.057334 V_Loss: 0.075984 IoU: 0.8006 Best Epoch: 49 Loss: 0.075984\n",
      "50EP(250822_111339): T_Loss: 0.056118 V_Loss: 0.076491 IoU: 0.8005 \n",
      "51EP(250822_111347): T_Loss: 0.053770 V_Loss: 0.076677 IoU: 0.8006 \n",
      "52EP(250822_111355): T_Loss: 0.054565 V_Loss: 0.075857 IoU: 0.8019 Best Epoch: 52 Loss: 0.075857\n",
      "53EP(250822_111403): T_Loss: 0.055754 V_Loss: 0.082982 IoU: 0.7847 \n",
      "54EP(250822_111411): T_Loss: 0.053421 V_Loss: 0.077787 IoU: 0.7990 \n",
      "55EP(250822_111419): T_Loss: 0.055768 V_Loss: 0.076411 IoU: 0.8015 \n",
      "56EP(250822_111427): T_Loss: 0.054883 V_Loss: 0.078608 IoU: 0.7970 \n",
      "57EP(250822_111435): T_Loss: 0.053655 V_Loss: 0.076258 IoU: 0.8016 \n",
      "58EP(250822_111443): T_Loss: 0.052636 V_Loss: 0.074007 IoU: 0.8048 Best Epoch: 58 Loss: 0.074007\n",
      "59EP(250822_111451): T_Loss: 0.051420 V_Loss: 0.076409 IoU: 0.8021 \n",
      "60EP(250822_111459): T_Loss: 0.052073 V_Loss: 0.074676 IoU: 0.8048 \n",
      "61EP(250822_111507): T_Loss: 0.051765 V_Loss: 0.074542 IoU: 0.8066 \n",
      "62EP(250822_111515): T_Loss: 0.050115 V_Loss: 0.075399 IoU: 0.8041 \n",
      "63EP(250822_111523): T_Loss: 0.047893 V_Loss: 0.074731 IoU: 0.8056 \n",
      "64EP(250822_111531): T_Loss: 0.049715 V_Loss: 0.074358 IoU: 0.8079 \n",
      "65EP(250822_111539): T_Loss: 0.050850 V_Loss: 0.077313 IoU: 0.7988 \n",
      "66EP(250822_111547): T_Loss: 0.049915 V_Loss: 0.075564 IoU: 0.8039 \n",
      "67EP(250822_111555): T_Loss: 0.047615 V_Loss: 0.074772 IoU: 0.8052 \n",
      "68EP(250822_111603): T_Loss: 0.050601 V_Loss: 0.076452 IoU: 0.8008 \n",
      "69EP(250822_111611): T_Loss: 0.049873 V_Loss: 0.074140 IoU: 0.8061 \n",
      "70EP(250822_111619): T_Loss: 0.048341 V_Loss: 0.074936 IoU: 0.8057 \n",
      "71EP(250822_111627): T_Loss: 0.048988 V_Loss: 0.075148 IoU: 0.8045 \n",
      "72EP(250822_111635): T_Loss: 0.047790 V_Loss: 0.074796 IoU: 0.8051 \n",
      "73EP(250822_111643): T_Loss: 0.045100 V_Loss: 0.075514 IoU: 0.8048 \n",
      "74EP(250822_111651): T_Loss: 0.044299 V_Loss: 0.075083 IoU: 0.8055 \n",
      "75EP(250822_111659): T_Loss: 0.044360 V_Loss: 0.075201 IoU: 0.8055 \n",
      "76EP(250822_111707): T_Loss: 0.044572 V_Loss: 0.074260 IoU: 0.8072 \n",
      "77EP(250822_111715): T_Loss: 0.047003 V_Loss: 0.076228 IoU: 0.8019 \n",
      "78EP(250822_111723): T_Loss: 0.045999 V_Loss: 0.076107 IoU: 0.8030 \n",
      "79EP(250822_111731): T_Loss: 0.044847 V_Loss: 0.074956 IoU: 0.8049 \n",
      "80EP(250822_111739): T_Loss: 0.044698 V_Loss: 0.077046 IoU: 0.8022 \n",
      "81EP(250822_111747): T_Loss: 0.042090 V_Loss: 0.076081 IoU: 0.8036 \n",
      "82EP(250822_111755): T_Loss: 0.045786 V_Loss: 0.075088 IoU: 0.8052 \n",
      "83EP(250822_111803): T_Loss: 0.044686 V_Loss: 0.076231 IoU: 0.8026 \n",
      "84EP(250822_111811): T_Loss: 0.042238 V_Loss: 0.076171 IoU: 0.8030 \n",
      "85EP(250822_111819): T_Loss: 0.042953 V_Loss: 0.075837 IoU: 0.8037 \n",
      "86EP(250822_111827): T_Loss: 0.041034 V_Loss: 0.076269 IoU: 0.8033 \n",
      "87EP(250822_111835): T_Loss: 0.042473 V_Loss: 0.076135 IoU: 0.8034 \n",
      "88EP(250822_111843): T_Loss: 0.042280 V_Loss: 0.075865 IoU: 0.8039 \n",
      "89EP(250822_111851): T_Loss: 0.042470 V_Loss: 0.076900 IoU: 0.8015 \n",
      "90EP(250822_111859): T_Loss: 0.041631 V_Loss: 0.076480 IoU: 0.8026 \n",
      "91EP(250822_111907): T_Loss: 0.042470 V_Loss: 0.076249 IoU: 0.8032 \n",
      "92EP(250822_111915): T_Loss: 0.041528 V_Loss: 0.076126 IoU: 0.8035 \n",
      "93EP(250822_111923): T_Loss: 0.041384 V_Loss: 0.076180 IoU: 0.8031 \n",
      "94EP(250822_111931): T_Loss: 0.041556 V_Loss: 0.076158 IoU: 0.8034 \n",
      "95EP(250822_111939): T_Loss: 0.041069 V_Loss: 0.076309 IoU: 0.8030 \n",
      "96EP(250822_111947): T_Loss: 0.041766 V_Loss: 0.076444 IoU: 0.8027 \n",
      "97EP(250822_111955): T_Loss: 0.043097 V_Loss: 0.076449 IoU: 0.8026 \n",
      "98EP(250822_112003): T_Loss: 0.041710 V_Loss: 0.076465 IoU: 0.8026 \n",
      "99EP(250822_112011): T_Loss: 0.041184 V_Loss: 0.076446 IoU: 0.8026 \n",
      "100EP(250822_112019): T_Loss: 0.043805 V_Loss: 0.076447 IoU: 0.8026 \n",
      "Test Start Time: 250822_112019\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f29c5c777bd4432093a331b01d972158",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test:   0%|          | 0/12 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Epoch: 58\n",
      "Test(250822_112022): Loss: 0.087310 IoU: 0.7827 Dice: 0.8678 Precision: 0.8687 Recall: 0.8835\n",
      "End 250822_112022\n",
      "Dataset: CS04. Masonry (4/5)\n",
      "Preprocessing: R(256,256)\n",
      "Augmentation: FLIP(HV)=>ROT([90,180,270])\n",
      "Evaluation: None\n",
      "train/val/test: 144/48/48\n",
      "UNet_MS_IG (1/1) (Iter 5) Dataset: CS04. Masonry (Shape: (256, 256, 3)) (4/5) Data Split: 60:20:20\n",
      "  -> Ablation: use_ms=True_use_gate=True\n",
      "Training Start Time: 250822_112022\n",
      "1EP(250822_112024): T_Loss: 0.346810 V_Loss: 0.248514 IoU: 0.5183 Best Epoch: 1 Loss: 0.248514\n",
      "2EP(250822_112027): T_Loss: 0.216906 V_Loss: 0.191660 IoU: 0.5899 Best Epoch: 2 Loss: 0.191660\n",
      "3EP(250822_112029): T_Loss: 0.199819 V_Loss: 0.183946 IoU: 0.5999 Best Epoch: 3 Loss: 0.183946\n",
      "4EP(250822_112031): T_Loss: 0.185053 V_Loss: 0.174104 IoU: 0.6118 Best Epoch: 4 Loss: 0.174104\n",
      "5EP(250822_112034): T_Loss: 0.174124 V_Loss: 0.199036 IoU: 0.5641 \n",
      "6EP(250822_112036): T_Loss: 0.180586 V_Loss: 0.175186 IoU: 0.6122 \n",
      "7EP(250822_112039): T_Loss: 0.175850 V_Loss: 0.184206 IoU: 0.5928 \n",
      "8EP(250822_112041): T_Loss: 0.174046 V_Loss: 0.178131 IoU: 0.6070 \n",
      "9EP(250822_112043): T_Loss: 0.167708 V_Loss: 0.169378 IoU: 0.6161 Best Epoch: 9 Loss: 0.169378\n",
      "10EP(250822_112046): T_Loss: 0.163529 V_Loss: 0.170187 IoU: 0.6194 \n",
      "11EP(250822_112048): T_Loss: 0.171310 V_Loss: 0.166758 IoU: 0.6218 Best Epoch: 11 Loss: 0.166758\n",
      "12EP(250822_112051): T_Loss: 0.160721 V_Loss: 0.180776 IoU: 0.5997 \n",
      "13EP(250822_112053): T_Loss: 0.152556 V_Loss: 0.166829 IoU: 0.6223 \n",
      "14EP(250822_112056): T_Loss: 0.145995 V_Loss: 0.165267 IoU: 0.6235 Best Epoch: 14 Loss: 0.165267\n",
      "15EP(250822_112058): T_Loss: 0.149893 V_Loss: 0.164067 IoU: 0.6297 Best Epoch: 15 Loss: 0.164067\n",
      "16EP(250822_112100): T_Loss: 0.146855 V_Loss: 0.149204 IoU: 0.6532 Best Epoch: 16 Loss: 0.149204\n",
      "17EP(250822_112103): T_Loss: 0.142661 V_Loss: 0.160560 IoU: 0.6315 \n",
      "18EP(250822_112105): T_Loss: 0.151715 V_Loss: 0.157993 IoU: 0.6414 \n",
      "19EP(250822_112108): T_Loss: 0.145692 V_Loss: 0.158082 IoU: 0.6396 \n",
      "20EP(250822_112110): T_Loss: 0.140546 V_Loss: 0.148231 IoU: 0.6581 Best Epoch: 20 Loss: 0.148231\n",
      "21EP(250822_112113): T_Loss: 0.140591 V_Loss: 0.161652 IoU: 0.6329 \n",
      "22EP(250822_112115): T_Loss: 0.129770 V_Loss: 0.164977 IoU: 0.6280 \n",
      "23EP(250822_112117): T_Loss: 0.127403 V_Loss: 0.145145 IoU: 0.6627 Best Epoch: 23 Loss: 0.145145\n",
      "24EP(250822_112120): T_Loss: 0.129828 V_Loss: 0.162088 IoU: 0.6333 \n",
      "25EP(250822_112122): T_Loss: 0.130681 V_Loss: 0.152229 IoU: 0.6482 \n",
      "26EP(250822_112125): T_Loss: 0.126773 V_Loss: 0.153854 IoU: 0.6461 \n",
      "27EP(250822_112127): T_Loss: 0.122980 V_Loss: 0.151188 IoU: 0.6511 \n",
      "28EP(250822_112129): T_Loss: 0.118084 V_Loss: 0.153867 IoU: 0.6438 \n",
      "29EP(250822_112132): T_Loss: 0.122134 V_Loss: 0.144171 IoU: 0.6671 Best Epoch: 29 Loss: 0.144171\n",
      "30EP(250822_112134): T_Loss: 0.122861 V_Loss: 0.156256 IoU: 0.6440 \n",
      "31EP(250822_112137): T_Loss: 0.119348 V_Loss: 0.150478 IoU: 0.6535 \n",
      "32EP(250822_112139): T_Loss: 0.124581 V_Loss: 0.149790 IoU: 0.6593 \n",
      "33EP(250822_112142): T_Loss: 0.123930 V_Loss: 0.149547 IoU: 0.6544 \n",
      "34EP(250822_112144): T_Loss: 0.122008 V_Loss: 0.153935 IoU: 0.6503 \n",
      "35EP(250822_112146): T_Loss: 0.113563 V_Loss: 0.150950 IoU: 0.6523 \n",
      "36EP(250822_112149): T_Loss: 0.114627 V_Loss: 0.151979 IoU: 0.6506 \n",
      "37EP(250822_112151): T_Loss: 0.112356 V_Loss: 0.148825 IoU: 0.6542 \n",
      "38EP(250822_112154): T_Loss: 0.102443 V_Loss: 0.148756 IoU: 0.6574 \n",
      "39EP(250822_112156): T_Loss: 0.103713 V_Loss: 0.144907 IoU: 0.6639 \n",
      "40EP(250822_112158): T_Loss: 0.101673 V_Loss: 0.156362 IoU: 0.6426 \n",
      "41EP(250822_112201): T_Loss: 0.103309 V_Loss: 0.146870 IoU: 0.6605 \n",
      "42EP(250822_112203): T_Loss: 0.102218 V_Loss: 0.152062 IoU: 0.6514 \n",
      "43EP(250822_112206): T_Loss: 0.105709 V_Loss: 0.155907 IoU: 0.6457 \n",
      "44EP(250822_112208): T_Loss: 0.106045 V_Loss: 0.154563 IoU: 0.6459 \n",
      "45EP(250822_112211): T_Loss: 0.097950 V_Loss: 0.144931 IoU: 0.6636 \n",
      "46EP(250822_112213): T_Loss: 0.096277 V_Loss: 0.162099 IoU: 0.6360 \n",
      "47EP(250822_112215): T_Loss: 0.097741 V_Loss: 0.155060 IoU: 0.6461 \n",
      "48EP(250822_112218): T_Loss: 0.096745 V_Loss: 0.146494 IoU: 0.6627 \n",
      "49EP(250822_112220): T_Loss: 0.089022 V_Loss: 0.146519 IoU: 0.6617 \n",
      "50EP(250822_112223): T_Loss: 0.092184 V_Loss: 0.150338 IoU: 0.6556 \n",
      "51EP(250822_112225): T_Loss: 0.088065 V_Loss: 0.158731 IoU: 0.6395 \n",
      "52EP(250822_112227): T_Loss: 0.091213 V_Loss: 0.148858 IoU: 0.6575 \n",
      "53EP(250822_112230): T_Loss: 0.088922 V_Loss: 0.146769 IoU: 0.6611 \n",
      "54EP(250822_112232): T_Loss: 0.083897 V_Loss: 0.144174 IoU: 0.6661 \n",
      "55EP(250822_112235): T_Loss: 0.085345 V_Loss: 0.150461 IoU: 0.6544 \n",
      "56EP(250822_112237): T_Loss: 0.083741 V_Loss: 0.152914 IoU: 0.6517 \n",
      "57EP(250822_112240): T_Loss: 0.079448 V_Loss: 0.146201 IoU: 0.6636 \n",
      "58EP(250822_112242): T_Loss: 0.080270 V_Loss: 0.145528 IoU: 0.6626 \n",
      "59EP(250822_112244): T_Loss: 0.075047 V_Loss: 0.147526 IoU: 0.6588 \n",
      "60EP(250822_112247): T_Loss: 0.075746 V_Loss: 0.147051 IoU: 0.6607 \n",
      "61EP(250822_112249): T_Loss: 0.076594 V_Loss: 0.148751 IoU: 0.6571 \n",
      "62EP(250822_112252): T_Loss: 0.080107 V_Loss: 0.145796 IoU: 0.6635 \n",
      "63EP(250822_112254): T_Loss: 0.076967 V_Loss: 0.152321 IoU: 0.6531 \n",
      "64EP(250822_112256): T_Loss: 0.075777 V_Loss: 0.143944 IoU: 0.6672 Best Epoch: 64 Loss: 0.143944\n",
      "65EP(250822_112259): T_Loss: 0.076495 V_Loss: 0.147196 IoU: 0.6597 \n",
      "66EP(250822_112301): T_Loss: 0.075210 V_Loss: 0.145682 IoU: 0.6632 \n",
      "67EP(250822_112304): T_Loss: 0.071900 V_Loss: 0.148114 IoU: 0.6601 \n",
      "68EP(250822_112306): T_Loss: 0.072698 V_Loss: 0.144061 IoU: 0.6658 \n",
      "69EP(250822_112309): T_Loss: 0.072153 V_Loss: 0.152405 IoU: 0.6520 \n",
      "70EP(250822_112311): T_Loss: 0.072452 V_Loss: 0.143419 IoU: 0.6673 Best Epoch: 70 Loss: 0.143419\n",
      "71EP(250822_112313): T_Loss: 0.069997 V_Loss: 0.144394 IoU: 0.6667 \n",
      "72EP(250822_112316): T_Loss: 0.070253 V_Loss: 0.147940 IoU: 0.6600 \n",
      "73EP(250822_112318): T_Loss: 0.067998 V_Loss: 0.146233 IoU: 0.6622 \n",
      "74EP(250822_112321): T_Loss: 0.069957 V_Loss: 0.148884 IoU: 0.6569 \n",
      "75EP(250822_112323): T_Loss: 0.065396 V_Loss: 0.146998 IoU: 0.6618 \n",
      "76EP(250822_112325): T_Loss: 0.070237 V_Loss: 0.147303 IoU: 0.6613 \n",
      "77EP(250822_112328): T_Loss: 0.068132 V_Loss: 0.148028 IoU: 0.6603 \n",
      "78EP(250822_112330): T_Loss: 0.065724 V_Loss: 0.143404 IoU: 0.6670 Best Epoch: 78 Loss: 0.143404\n",
      "79EP(250822_112333): T_Loss: 0.067648 V_Loss: 0.150048 IoU: 0.6567 \n",
      "80EP(250822_112335): T_Loss: 0.066636 V_Loss: 0.146923 IoU: 0.6604 \n",
      "81EP(250822_112338): T_Loss: 0.061255 V_Loss: 0.146460 IoU: 0.6618 \n",
      "82EP(250822_112340): T_Loss: 0.065162 V_Loss: 0.147192 IoU: 0.6610 \n",
      "83EP(250822_112342): T_Loss: 0.062550 V_Loss: 0.148110 IoU: 0.6586 \n",
      "84EP(250822_112345): T_Loss: 0.059426 V_Loss: 0.150406 IoU: 0.6556 \n",
      "85EP(250822_112347): T_Loss: 0.062876 V_Loss: 0.149253 IoU: 0.6573 \n",
      "86EP(250822_112350): T_Loss: 0.062434 V_Loss: 0.149494 IoU: 0.6563 \n",
      "87EP(250822_112352): T_Loss: 0.062709 V_Loss: 0.149742 IoU: 0.6563 \n",
      "88EP(250822_112354): T_Loss: 0.061555 V_Loss: 0.148950 IoU: 0.6578 \n",
      "89EP(250822_112357): T_Loss: 0.063350 V_Loss: 0.148596 IoU: 0.6580 \n",
      "90EP(250822_112359): T_Loss: 0.059347 V_Loss: 0.149525 IoU: 0.6567 \n",
      "91EP(250822_112402): T_Loss: 0.062333 V_Loss: 0.150132 IoU: 0.6557 \n",
      "92EP(250822_112404): T_Loss: 0.061111 V_Loss: 0.150755 IoU: 0.6546 \n",
      "93EP(250822_112407): T_Loss: 0.060178 V_Loss: 0.150048 IoU: 0.6560 \n",
      "94EP(250822_112409): T_Loss: 0.058135 V_Loss: 0.150558 IoU: 0.6552 \n",
      "95EP(250822_112411): T_Loss: 0.057903 V_Loss: 0.150400 IoU: 0.6554 \n",
      "96EP(250822_112414): T_Loss: 0.057448 V_Loss: 0.150325 IoU: 0.6556 \n",
      "97EP(250822_112416): T_Loss: 0.059748 V_Loss: 0.150217 IoU: 0.6557 \n",
      "98EP(250822_112419): T_Loss: 0.060750 V_Loss: 0.150102 IoU: 0.6563 \n",
      "99EP(250822_112421): T_Loss: 0.059176 V_Loss: 0.150171 IoU: 0.6561 \n",
      "100EP(250822_112423): T_Loss: 0.060796 V_Loss: 0.150151 IoU: 0.6560 \n",
      "Test Start Time: 250822_112423\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12b9d5e43dce46cabc4bd0c6ed6b010e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test:   0%|          | 0/12 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Epoch: 78\n",
      "Test(250822_112424): Loss: 0.151291 IoU: 0.6547 Dice: 0.7716 Precision: 0.7820 Recall: 0.8018\n",
      "End 250822_112424\n",
      "Dataset: CS07. DeepCrack537 (5/5)\n",
      "Preprocessing: P(384,640)\n",
      "Augmentation: FLIP(HV)=>ROT([90,180,270])\n",
      "Evaluation: None\n",
      "train/val/test: 316/105/106\n",
      "UNet_MS_IG (1/1) (Iter 5) Dataset: CS07. DeepCrack537 (Shape: (384, 640, 3)) (5/5) Data Split: 60:20:20\n",
      "  -> Ablation: use_ms=True_use_gate=True\n",
      "Training Start Time: 250822_112424\n",
      "1EP(250822_112442): T_Loss: 0.275703 V_Loss: 0.168500 IoU: 0.6151 Best Epoch: 1 Loss: 0.168500\n",
      "2EP(250822_112500): T_Loss: 0.150052 V_Loss: 0.151211 IoU: 0.6289 Best Epoch: 2 Loss: 0.151211\n",
      "3EP(250822_112517): T_Loss: 0.134207 V_Loss: 0.139016 IoU: 0.6538 Best Epoch: 3 Loss: 0.139016\n",
      "4EP(250822_112535): T_Loss: 0.130790 V_Loss: 0.129956 IoU: 0.6679 Best Epoch: 4 Loss: 0.129956\n",
      "5EP(250822_112553): T_Loss: 0.122152 V_Loss: 0.124727 IoU: 0.6786 Best Epoch: 5 Loss: 0.124727\n",
      "6EP(250822_112610): T_Loss: 0.116383 V_Loss: 0.121200 IoU: 0.6834 Best Epoch: 6 Loss: 0.121200\n",
      "7EP(250822_112628): T_Loss: 0.116082 V_Loss: 0.122568 IoU: 0.6808 \n",
      "8EP(250822_112645): T_Loss: 0.111061 V_Loss: 0.125565 IoU: 0.6768 \n",
      "9EP(250822_112703): T_Loss: 0.109472 V_Loss: 0.118477 IoU: 0.6906 Best Epoch: 9 Loss: 0.118477\n",
      "10EP(250822_112721): T_Loss: 0.108005 V_Loss: 0.117607 IoU: 0.6961 Best Epoch: 10 Loss: 0.117607\n",
      "11EP(250822_112738): T_Loss: 0.108971 V_Loss: 0.116718 IoU: 0.6928 Best Epoch: 11 Loss: 0.116718\n",
      "12EP(250822_112756): T_Loss: 0.106293 V_Loss: 0.112053 IoU: 0.6998 Best Epoch: 12 Loss: 0.112053\n",
      "13EP(250822_112814): T_Loss: 0.103467 V_Loss: 0.108269 IoU: 0.7076 Best Epoch: 13 Loss: 0.108269\n",
      "14EP(250822_112831): T_Loss: 0.103711 V_Loss: 0.115372 IoU: 0.6949 \n",
      "15EP(250822_112849): T_Loss: 0.103938 V_Loss: 0.107282 IoU: 0.7109 Best Epoch: 15 Loss: 0.107282\n",
      "16EP(250822_112907): T_Loss: 0.098230 V_Loss: 0.110141 IoU: 0.7042 \n",
      "17EP(250822_112924): T_Loss: 0.099429 V_Loss: 0.104960 IoU: 0.7136 Best Epoch: 17 Loss: 0.104960\n",
      "18EP(250822_112942): T_Loss: 0.096225 V_Loss: 0.097167 IoU: 0.7320 Best Epoch: 18 Loss: 0.097167\n",
      "19EP(250822_113000): T_Loss: 0.094874 V_Loss: 0.104924 IoU: 0.7155 \n",
      "20EP(250822_113017): T_Loss: 0.097017 V_Loss: 0.098009 IoU: 0.7302 \n",
      "21EP(250822_113035): T_Loss: 0.095157 V_Loss: 0.113541 IoU: 0.7006 \n",
      "22EP(250822_113053): T_Loss: 0.099139 V_Loss: 0.107041 IoU: 0.7106 \n",
      "23EP(250822_113110): T_Loss: 0.093218 V_Loss: 0.099889 IoU: 0.7230 \n",
      "24EP(250822_113128): T_Loss: 0.091586 V_Loss: 0.102908 IoU: 0.7175 \n",
      "25EP(250822_113146): T_Loss: 0.088728 V_Loss: 0.093485 IoU: 0.7417 Best Epoch: 25 Loss: 0.093485\n",
      "26EP(250822_113203): T_Loss: 0.089572 V_Loss: 0.097084 IoU: 0.7300 \n",
      "27EP(250822_113221): T_Loss: 0.088990 V_Loss: 0.094008 IoU: 0.7379 \n",
      "28EP(250822_113239): T_Loss: 0.091748 V_Loss: 0.098160 IoU: 0.7279 \n",
      "29EP(250822_113256): T_Loss: 0.088755 V_Loss: 0.108244 IoU: 0.7074 \n",
      "30EP(250822_113314): T_Loss: 0.089134 V_Loss: 0.095592 IoU: 0.7362 \n",
      "31EP(250822_113331): T_Loss: 0.085427 V_Loss: 0.092498 IoU: 0.7398 Best Epoch: 31 Loss: 0.092498\n",
      "32EP(250822_113349): T_Loss: 0.083110 V_Loss: 0.092999 IoU: 0.7409 \n",
      "33EP(250822_113407): T_Loss: 0.081737 V_Loss: 0.092722 IoU: 0.7408 \n",
      "34EP(250822_113424): T_Loss: 0.080664 V_Loss: 0.092785 IoU: 0.7430 \n",
      "35EP(250822_113442): T_Loss: 0.081091 V_Loss: 0.093184 IoU: 0.7386 \n",
      "36EP(250822_113500): T_Loss: 0.081164 V_Loss: 0.095301 IoU: 0.7359 \n",
      "37EP(250822_113517): T_Loss: 0.080726 V_Loss: 0.097031 IoU: 0.7320 \n",
      "38EP(250822_113535): T_Loss: 0.084718 V_Loss: 0.095960 IoU: 0.7347 \n",
      "39EP(250822_113553): T_Loss: 0.091969 V_Loss: 0.097940 IoU: 0.7293 \n",
      "40EP(250822_113610): T_Loss: 0.086239 V_Loss: 0.093355 IoU: 0.7392 \n",
      "41EP(250822_113628): T_Loss: 0.081198 V_Loss: 0.093098 IoU: 0.7421 \n",
      "42EP(250822_113646): T_Loss: 0.078716 V_Loss: 0.097410 IoU: 0.7330 \n",
      "43EP(250822_113703): T_Loss: 0.078068 V_Loss: 0.090759 IoU: 0.7446 Best Epoch: 43 Loss: 0.090759\n",
      "44EP(250822_113721): T_Loss: 0.077291 V_Loss: 0.094408 IoU: 0.7370 \n",
      "45EP(250822_113739): T_Loss: 0.078443 V_Loss: 0.092182 IoU: 0.7431 \n",
      "46EP(250822_113756): T_Loss: 0.078527 V_Loss: 0.097507 IoU: 0.7321 \n",
      "47EP(250822_113814): T_Loss: 0.078496 V_Loss: 0.091177 IoU: 0.7447 \n",
      "48EP(250822_113832): T_Loss: 0.076694 V_Loss: 0.094879 IoU: 0.7362 \n",
      "49EP(250822_113849): T_Loss: 0.077893 V_Loss: 0.091058 IoU: 0.7458 \n",
      "50EP(250822_113907): T_Loss: 0.075658 V_Loss: 0.090461 IoU: 0.7476 Best Epoch: 50 Loss: 0.090461\n",
      "51EP(250822_113924): T_Loss: 0.073897 V_Loss: 0.091143 IoU: 0.7434 \n",
      "52EP(250822_113942): T_Loss: 0.074248 V_Loss: 0.089824 IoU: 0.7468 Best Epoch: 52 Loss: 0.089824\n",
      "53EP(250822_114000): T_Loss: 0.073071 V_Loss: 0.089650 IoU: 0.7492 Best Epoch: 53 Loss: 0.089650\n",
      "54EP(250822_114018): T_Loss: 0.073132 V_Loss: 0.093756 IoU: 0.7368 \n",
      "55EP(250822_114035): T_Loss: 0.074085 V_Loss: 0.088540 IoU: 0.7518 Best Epoch: 55 Loss: 0.088540\n",
      "56EP(250822_114053): T_Loss: 0.072992 V_Loss: 0.089734 IoU: 0.7491 \n",
      "57EP(250822_114110): T_Loss: 0.071083 V_Loss: 0.088001 IoU: 0.7525 Best Epoch: 57 Loss: 0.088001\n",
      "58EP(250822_114128): T_Loss: 0.069407 V_Loss: 0.089377 IoU: 0.7508 \n",
      "59EP(250822_114146): T_Loss: 0.070447 V_Loss: 0.088930 IoU: 0.7513 \n",
      "60EP(250822_114203): T_Loss: 0.070430 V_Loss: 0.088699 IoU: 0.7510 \n",
      "61EP(250822_114221): T_Loss: 0.068418 V_Loss: 0.089395 IoU: 0.7494 \n",
      "62EP(250822_114239): T_Loss: 0.069506 V_Loss: 0.091901 IoU: 0.7446 \n",
      "63EP(250822_114256): T_Loss: 0.068162 V_Loss: 0.089086 IoU: 0.7515 \n",
      "64EP(250822_114314): T_Loss: 0.069117 V_Loss: 0.088661 IoU: 0.7520 \n",
      "65EP(250822_114332): T_Loss: 0.069203 V_Loss: 0.089040 IoU: 0.7511 \n",
      "66EP(250822_114349): T_Loss: 0.066319 V_Loss: 0.089261 IoU: 0.7515 \n",
      "67EP(250822_114407): T_Loss: 0.067854 V_Loss: 0.088710 IoU: 0.7520 \n",
      "68EP(250822_114425): T_Loss: 0.067022 V_Loss: 0.087942 IoU: 0.7537 Best Epoch: 68 Loss: 0.087942\n",
      "69EP(250822_114442): T_Loss: 0.066709 V_Loss: 0.088036 IoU: 0.7538 \n",
      "70EP(250822_114500): T_Loss: 0.065899 V_Loss: 0.087968 IoU: 0.7537 \n",
      "71EP(250822_114517): T_Loss: 0.065547 V_Loss: 0.088411 IoU: 0.7525 \n",
      "72EP(250822_114535): T_Loss: 0.065728 V_Loss: 0.087762 IoU: 0.7545 Best Epoch: 72 Loss: 0.087762\n",
      "73EP(250822_114553): T_Loss: 0.065135 V_Loss: 0.086607 IoU: 0.7560 Best Epoch: 73 Loss: 0.086607\n",
      "74EP(250822_114610): T_Loss: 0.064356 V_Loss: 0.086643 IoU: 0.7568 \n",
      "75EP(250822_114628): T_Loss: 0.064086 V_Loss: 0.086162 IoU: 0.7578 Best Epoch: 75 Loss: 0.086162\n",
      "76EP(250822_114646): T_Loss: 0.064392 V_Loss: 0.086723 IoU: 0.7569 \n",
      "77EP(250822_114704): T_Loss: 0.063666 V_Loss: 0.088046 IoU: 0.7540 \n",
      "78EP(250822_114721): T_Loss: 0.064087 V_Loss: 0.087116 IoU: 0.7562 \n",
      "79EP(250822_114739): T_Loss: 0.063066 V_Loss: 0.087328 IoU: 0.7553 \n",
      "80EP(250822_114757): T_Loss: 0.062087 V_Loss: 0.086936 IoU: 0.7560 \n",
      "81EP(250822_114814): T_Loss: 0.061919 V_Loss: 0.086692 IoU: 0.7560 \n",
      "82EP(250822_114832): T_Loss: 0.062537 V_Loss: 0.086687 IoU: 0.7564 \n",
      "83EP(250822_114850): T_Loss: 0.060851 V_Loss: 0.087115 IoU: 0.7554 \n",
      "84EP(250822_114907): T_Loss: 0.061048 V_Loss: 0.087682 IoU: 0.7544 \n",
      "85EP(250822_114925): T_Loss: 0.060979 V_Loss: 0.087553 IoU: 0.7542 \n",
      "86EP(250822_114943): T_Loss: 0.061046 V_Loss: 0.087294 IoU: 0.7549 \n",
      "87EP(250822_115000): T_Loss: 0.060699 V_Loss: 0.087873 IoU: 0.7537 \n",
      "88EP(250822_115018): T_Loss: 0.060076 V_Loss: 0.087438 IoU: 0.7548 \n",
      "89EP(250822_115035): T_Loss: 0.060187 V_Loss: 0.087229 IoU: 0.7554 \n",
      "90EP(250822_115053): T_Loss: 0.060641 V_Loss: 0.087412 IoU: 0.7551 \n",
      "91EP(250822_115111): T_Loss: 0.060788 V_Loss: 0.087280 IoU: 0.7554 \n",
      "92EP(250822_115128): T_Loss: 0.059111 V_Loss: 0.087390 IoU: 0.7554 \n",
      "93EP(250822_115146): T_Loss: 0.060910 V_Loss: 0.087214 IoU: 0.7557 \n",
      "94EP(250822_115204): T_Loss: 0.059991 V_Loss: 0.087290 IoU: 0.7557 \n",
      "95EP(250822_115221): T_Loss: 0.059719 V_Loss: 0.087172 IoU: 0.7555 \n",
      "96EP(250822_115239): T_Loss: 0.059442 V_Loss: 0.087180 IoU: 0.7557 \n",
      "97EP(250822_115257): T_Loss: 0.059259 V_Loss: 0.087265 IoU: 0.7555 \n",
      "98EP(250822_115314): T_Loss: 0.060096 V_Loss: 0.087276 IoU: 0.7554 \n",
      "99EP(250822_115332): T_Loss: 0.059459 V_Loss: 0.087240 IoU: 0.7554 \n",
      "100EP(250822_115350): T_Loss: 0.060195 V_Loss: 0.087242 IoU: 0.7555 \n",
      "Test Start Time: 250822_115350\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af748375a79b4c588e3483305e17a166",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test:   0%|          | 0/27 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "now = datetime.now()\n",
    "Experiments_Time=now.strftime(\"%y%m%d_%H%M%S\")\n",
    "print('Experiment Start Time:',Experiments_Time)\n",
    "Metrics=['Experiment Time','Train Time', 'Iteration', 'Dataset Name', 'Data Split', 'Model Name', 'Val Loss', 'Test Loss', 'IoU', 'Dice',  'Precision', 'Recall', 'Total Params','Train-Predction Time','Best Epoch','Time per Epoch', 'Loss Function', 'LR', 'Batch size', '#Epochs', 'Preprocessing','Augmentation','Sample Size','DIR']\n",
    "# [ABLATION] 'Model Name' 바로 뒤에 ablation 변수 컬럼 삽입\n",
    "_insert_idx = Metrics.index('Model Name') + 1\n",
    "Metrics = Metrics[:_insert_idx] + ablation_keys + Metrics[_insert_idx:]\n",
    "df = pd.DataFrame(index=None, columns=Metrics)\n",
    "output_root = f'{output_root}/output_{Experiments_Time}'\n",
    "os.makedirs(output_root, exist_ok = True)\n",
    "for ratio_combination in ratio_combinations:\n",
    "    data_split_csv_list = [Dataset_root+'/'+f'(DataSplit)_Data_splits_{int(ratio_combination[0]*100)}_{int(ratio_combination[1]*100)}_{int(ratio_combination[2]*100)}_30'+'/'+f+f'_splits_{int(ratio_combination[0]*100)}_{int(ratio_combination[1]*100)}_{int(ratio_combination[2]*100)}_30.csv' for f in Dataset_Name_list]\n",
    "    for iteration in range(iterations[0], iterations[1]+1):\n",
    "    # for iteration in iterations:\n",
    "        print(f'(Iter {iteration})')\n",
    "        seed = iteration\n",
    "        for augmentation_str in augmentation_strs:\n",
    "            for j, Dataset_Name in enumerate(Dataset_Name_list):\n",
    "                # if j+1<=19:\n",
    "                #     continue\n",
    "                print(f'Dataset: {Dataset_Name} ({j+1}/{len(Dataset_Name_list)})')\n",
    "                control_random_seed(seed)\n",
    "\n",
    "                Dataset_dir = Dataset_root+'/'+Dataset_Name\n",
    "                data_split_csv = data_split_csv_list[j]\n",
    "\n",
    "                Height, Width = parse_dimensions(df_Dataset_informs[df_Dataset_informs['Dataset Name']==Dataset_Name]['Image Shape (H,W)'].item())\n",
    "                in_channels = int(df_Dataset_informs[df_Dataset_informs['Dataset Name']==Dataset_Name]['Inchannels'].item())\n",
    "                number_of_classes = int(df_Dataset_informs[df_Dataset_informs['Dataset Name']==Dataset_Name]['Number of Classes'].item())\n",
    "                BINARY_SEG = True if number_of_classes==2 else False\n",
    "                exclude_background = EXCLUDE_BACKGROUND\n",
    "\n",
    "                out_channels = 1 if BINARY_SEG else number_of_classes\n",
    "                transform_str = df_Dataset_informs[df_Dataset_informs['Dataset Name']==Dataset_Name]['Transform'].item()\n",
    "                transform_str = None if isinstance(transform_str, float) and np.isnan(transform_str) else transform_str\n",
    "                print(f'Preprocessing: {transform_str}')\n",
    "                # augmentation_str = df_Dataset_informs[df_Dataset_informs['Dataset Name']==Dataset_Name]['Augmentation'].item()\n",
    "                # augmentation_str = None if isinstance(augmentation_str, float) and np.isnan(augmentation_str) else augmentation_str\n",
    "                print(f'Augmentation: {augmentation_str}')\n",
    "\n",
    "                evaluation_transform_str = df_Dataset_informs[df_Dataset_informs['Dataset Name']==Dataset_Name]['Evaluation'].item()\n",
    "                evaluation_transform_str = None if isinstance(evaluation_transform_str, float) and np.isnan(evaluation_transform_str) else evaluation_transform_str\n",
    "                print(f'Evaluation: {evaluation_transform_str}')\n",
    "\n",
    "                (train_image_path_list, train_target_path_list,\n",
    "                 validation_image_path_list, validation_target_path_list,\n",
    "                 test_image_path_list, test_target_path_list) = create_dataset_lists(Dataset_dir, iteration, data_split_csv)\n",
    "\n",
    "                print(f'train/val/test: {len(train_image_path_list)}/{len(validation_image_path_list)}/{len(test_image_path_list)}')\n",
    "\n",
    "                if evaluation_transform_str and re.match(r'RC\\((\\d+),(\\d+)\\)', evaluation_transform_str):\n",
    "                    validation_image_path_list = [path.replace('Originals','Originals_cropped') for path in validation_image_path_list for crop_count in range(1, 5)]\n",
    "                    validation_target_path_list = [path.replace('Masks','Masks_cropped') for path in validation_target_path_list for crop_count in range(1, 5)]\n",
    "                    test_image_path_list = [path.replace('Originals','Originals_cropped') for path in test_image_path_list for crop_count in range(1, 5)]\n",
    "                    test_target_path_list = [path.replace('Masks','Masks_cropped') for path in test_target_path_list for crop_count in range(1, 5)]\n",
    "\n",
    "                    # The lists now contain paths to the windowed images\n",
    "                    print(f'train/cropped val/cropped test: {len(train_image_path_list)}/{len(validation_image_path_list)}/{len(test_image_path_list)}')\n",
    "\n",
    "                # train_image_path_list = natsort.natsorted(train_image_path_list[:batch_size+1])\n",
    "                # train_target_path_list = natsort.natsorted(train_target_path_list[:batch_size+1])\n",
    "                # validation_image_path_list  = natsort.natsorted(validation_image_path_list[:batch_size+1])\n",
    "                # validation_target_path_list = natsort.natsorted(validation_target_path_list[:batch_size+1])\n",
    "                # test_image_path_list = natsort.natsorted(test_image_path_list[:batch_size+1])\n",
    "                # test_target_path_list= natsort.natsorted(test_target_path_list[:batch_size+1])\n",
    "\n",
    "                train_dataset = ImagesDataset(train_image_path_list, train_target_path_list, Height=Height, Width=Width, augmentation_str=augmentation_str, transform_str=transform_str)\n",
    "                validation_dataset = ImagesDataset(validation_image_path_list, validation_target_path_list, Height=Height, Width=Width, augmentation_str=None, transform_str=transform_str)\n",
    "                test_dataset = ImagesDataset(test_image_path_list, test_target_path_list, Height=Height, Width=Width, augmentation_str=None, transform_str=transform_str)\n",
    "                train_loader = SegDataLoader(\n",
    "                train_dataset, batch_size=batch_size,\n",
    "                num_workers=4, pin_memory=True, shuffle=True, drop_last=drop_last, fill_last_batch=fill_last_batch,\n",
    "                )\n",
    "                validation_loader = SegDataLoader(\n",
    "                    validation_dataset, batch_size=batch_size, \n",
    "                    num_workers=4, pin_memory=True,\n",
    "                )\n",
    "                test_loader = SegDataLoader(\n",
    "                    test_dataset, batch_size=batch_size, \n",
    "                    num_workers=4, pin_memory=True,\n",
    "                )\n",
    "\n",
    "                for k, model_name in enumerate(model_names):\n",
    "                    if train_mode=='inference' and ((iteration, Dataset_Name, model_name) not in iteration_dataset_model_tuples):\n",
    "                        continue\n",
    "                    if train_mode=='train_again' and ((iteration, Dataset_Name, model_name) not in iteration_dataset_model_tuples):\n",
    "                        continue\n",
    "                \n",
    "                    print(f'{model_name} ({k+1}/{len(model_names)}) (Iter {iteration})', end=' ')\n",
    "                    height, width, _ = map(int, df_Dataset_informs[df_Dataset_informs['Dataset Name']==Dataset_Name]['Input Shape'].item().strip('()').split(', '))\n",
    "                    print(f'Dataset: {Dataset_Name} (Shape: ({height}, {width}, {in_channels})) ({j+1}/{len(Dataset_Name_list)})', end=' ')\n",
    "                    print(f'Data Split: {int(ratio_combination[0]*100)}:{int(ratio_combination[1]*100)}:{int(ratio_combination[2]*100)}')\n",
    "                \n",
    "                    # [ABLATION] 여기서 조합별로 반복\n",
    "                    for ab_cfg in ablation_grid:\n",
    "                        ab_tag = ablation_to_tag(ab_cfg)\n",
    "                        print(f\"  -> Ablation: {ab_tag}\")\n",
    "                \n",
    "                        # 출력 디렉토리에 ablation 태그 포함 (충돌 방지)\n",
    "                        output_dir = (\n",
    "                            output_root + f'/{model_name}_{Dataset_Name}'\n",
    "                            + f'_Split_{int(ratio_combination[0]*100)}_{int(ratio_combination[1]*100)}_{int(ratio_combination[2]*100)}'\n",
    "                            + f'_Iter_{iteration}_ABL_{ab_tag}'\n",
    "                        )\n",
    "                        copy_sourcefile(output_dir, src_dir='src')\n",
    "                        control_random_seed(seed)\n",
    "                \n",
    "                        # 모델 생성: ablation 인자를 **우선** 전달, 실패 시 기존 방식 fallback\n",
    "                        try:\n",
    "                            if model_name in ['CSTF','DECSNet','CSTF_v2','SwinUNet','UTE_CrackNet']:\n",
    "                                model = str_to_class(model_name)(in_channels, out_channels, resolution = (height, width), **ab_cfg)\n",
    "                            else:\n",
    "                                model = str_to_class(model_name)(in_channels, out_channels, **ab_cfg)\n",
    "                        except TypeError:\n",
    "                            # 기존 모델 시그니처가 ablation 인자를 받지 않는 경우\n",
    "                            if model_name in ['CSTF','DECSNet','CSTF_v2','SwinUNet','UTE_CrackNet']:\n",
    "                                model = str_to_class(model_name)(in_channels, out_channels, resolution = (height, width))\n",
    "                            else:\n",
    "                                model = str_to_class(model_name)(in_channels, out_channels)\n",
    "                \n",
    "                        device = torch.device(\"cuda:\"+str(devices[0]))\n",
    "                        if len(devices)>1 and (model_name not in ['Crackmer','DECSNet']):\n",
    "                            model = torch.nn.DataParallel(model, device_ids = devices ).to(device)\n",
    "                        else:\n",
    "                            model = model.to(device)\n",
    "                \n",
    "                        # [ABLATION] Do_Experiment 전후로 df 길이 측정 → 새로 추가된 행에 ablation 값 채우기\n",
    "                        _len_before = len(df)\n",
    "                        df = Do_Experiment(\n",
    "                            seed, model_name, model,\n",
    "                            train_loader, validation_loader, test_loader,\n",
    "                            optimizer, lr,  number_of_classes, epochs,\n",
    "                            Metrics, df, device, None, train_mode\n",
    "                        )\n",
    "                        _len_after = len(df)\n",
    "                \n",
    "                        if _len_after > _len_before:\n",
    "                            # 방금 추가된 구간(1개 이상일 수도 있음)에 대해 ablation 컬럼 값 채우기\n",
    "                            for key in ablation_keys:\n",
    "                                df.loc[_len_before:_len_after-1, key] = ab_cfg[key]\n",
    "                \n",
    "                        # CSV 저장\n",
    "                        try:\n",
    "                            df.to_csv(output_root+'/'+f'{Project_Name}_'+Experiments_Time+'.csv', index=False, header=True, encoding=\"cp949\")\n",
    "                        except:\n",
    "                            now = datetime.now()\n",
    "                            tmp_date=now.strftime(\"%y%m%d_%H%M%S\")\n",
    "                            df.to_csv(output_root+'/'+f'{Project_Name}_'+Experiments_Time+'_'+tmp_date+'_tmp'+'.csv', index=False, header=True, encoding=\"cp949\")\n",
    "\n",
    "import os\n",
    "print('End')\n",
    "os._exit(00) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0eeb27-77d0-40d9-8b97-6a607391a58a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f12ea3-a3b0-4b50-af25-1cea035fc2ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79aef025-f76d-4f92-8dfc-7a2138f10caa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440252a4-f9e4-4006-9e1d-affa6aa2b090",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10534c23-7bc7-49bc-9ad3-404f48d29707",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72981d7-c12e-4594-bf99-0d803906f46f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LSH_Torch_2.4",
   "language": "python",
   "name": "lsh_torch_2.4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
